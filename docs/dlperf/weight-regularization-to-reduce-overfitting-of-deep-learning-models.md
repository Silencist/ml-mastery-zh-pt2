# 使用权重正则化减少深度学习模型的过拟合

> 原文：<https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/>

最后更新于 2019 年 8 月 6 日

神经网络学习一组最佳地将输入映射到输出的权重。

具有大网络权重的网络可能是不稳定网络的标志，其中输入的小变化可能导致输出的大变化。这可能是网络过度训练训练数据集的一个迹象，并且在对新数据进行预测时可能表现不佳。

解决这个问题的一种方法是更新学习算法，以鼓励网络保持较小的权重。这被称为权重正则化，它可以作为一种通用技术来减少训练数据集的过拟合，并提高模型的泛化能力。

在这篇文章中，您将发现权重正则化是一种减少神经网络过拟合的方法。

看完这篇文章，你会知道:

*   神经网络中的大权重是一个更复杂的网络过度训练数据的标志。
*   在训练期间根据网络权重的大小来惩罚网络可以减少过拟合。
*   可以将 L1 或 L2 向量范数惩罚添加到网络优化中，以鼓励更小的权重。

**用我的新书[更好的深度学习](https://machinelearningmastery.com/better-deep-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![A Gentle Introduction to Weight Regularization to Reduce Overfitting for Deep Learning Models](img/a05cb1514f57f0130b73755568325d63.png)

为深度学习模型减少过拟合的权重调整的温和介绍 [jojo nicdao](https://www.flickr.com/photos/jonicdao/8992722482/) 摄，保留部分权利。

## 大权重问题

在拟合神经网络模型时，我们必须使用随机梯度下降和训练数据集来学习网络的权重(即模型参数)。

我们训练网络的时间越长，训练数据的权重将变得越专业，过拟合训练数据。权重的大小将会增加，以便处理训练数据中看到的示例的细节。

大权重使网络不稳定。尽管权重将专门用于训练数据集，但预期输入上的微小变化或统计噪声将导致输出的巨大差异。

> 大的权重往往会导致节点函数的急剧转变，从而导致输入的小变化导致输出的大变化。

—第 269 页[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2PBsezv)，1999。

一般来说，我们称这个模型具有大的方差和小的偏差。也就是说，该模型对训练数据集中的特定示例(统计噪声)很敏感。

权重大的模型比权重小的模型更复杂。这是一个网络的迹象，可能过于专门用于训练数据。实际上，我们更喜欢选择更简单的模型来解决问题(例如奥卡姆剃刀)。我们更喜欢重量较小的型号。

> ……给定一些训练数据和网络架构，多组权重值(多个模型)可以解释这些数据。简单的模型比复杂的模型更不容易过拟合。在这种情况下，一个简单的模型是参数值的分布具有较小熵的模型

—第 107 页，[Python 深度学习](https://amzn.to/2wVqZDq)，2017。

另一个可能的问题是，可能有许多输入变量，每个变量与输出变量的相关程度不同。有时我们可以使用方法来帮助选择输入变量，但是变量之间的相互关系往往并不明显。

对网络中不太相关或不相关的输入使用小权重甚至零权重将允许模型专注于学习。这也将导致一个更简单的模型。

## 鼓励小重量

学习算法可以更新，以鼓励网络使用小权重。

一种方法是改变网络优化中使用的损耗计算，同时考虑权重的大小。

记住，当我们训练神经网络时，我们[最小化损失函数](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)，例如分类中的对数损失或回归中的均方误差。在批量计算预测值和期望值之间的损失时，我们可以将网络中所有权重的当前大小相加，或者在计算中添加一个层。这被称为惩罚，因为我们惩罚的模型与模型中权重的大小成正比。

> 许多正则化方法都是基于限制模型的容量，例如神经网络、线性回归或逻辑回归，方法是在目标函数中添加[…]惩罚。

—第 230 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

较大的权重导致较大的惩罚，表现为较大的损失分数。然后，优化算法将推动模型具有更小的权重，即权重不大于在训练数据集上表现良好所需的权重。

较小的权重被认为更规则或不太专业，因此，我们称这种惩罚为权重正则化。

当这种惩罚模型系数的方法用于其他机器学习模型，如线性回归或逻辑回归时，它可以被称为收缩，因为惩罚会促使系数在优化过程中收缩。

> 收缩。这种方法包括拟合一个包含所有 p 预测因子的模型。然而，估计的系数向零收缩……这种收缩(也称为正则化)具有减少方差的效果

—第 204 页，[统计学习导论:在 R](https://amzn.to/2MXGK7I) 中的应用，2013。

向神经网络添加权重大小惩罚或权重正则化具有减少泛化误差和允许模型较少关注不太相关的输入变量的效果。

> 1)它通过选择解决学习问题的最小向量来抑制权重向量的任何无关分量。2)如果尺寸选择正确，权重衰减可以抑制静态噪声对目标的一些影响。

——[简单的权重衰减可以提高泛化](https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization)，1992。

## 如何处罚大重量

根据权重的大小对模型进行处罚有两个部分。

第一个是权重大小的计算，第二个是优化过程应该对惩罚的关注程度。

### 计算重量大小

神经网络的权重是实值，可以是正的，也可以是负的，因此，简单地增加权重是不够的。有两种主要方法用于计算权重的大小，它们是:

*   计算权重绝对值之和，称为 L1。
*   计算权重的平方值之和，称为 L2。

如果可能的话，L1 鼓励权重为 0.0，从而导致权重更稀疏(权重具有更多的 0.0 值)。L2 提供了更多的细微差别，既更严厉地惩罚较大的权重，但导致权重不那么稀疏。在线性和逻辑回归中使用 L2 通常被称为岭回归。当试图发展一种对惩罚的直觉或使用惩罚的例子时，知道这一点是很有用的。

> 在其他学术界，L2 正则化也被称为岭回归或 Tikhonov 正则化。

—第 231 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

权重可以被认为是一个向量，向量的大小被称为它的范数，来自线性代数。因此，基于权重的大小来惩罚模型也被称为权重或参数范数惩罚。

有可能将 L1 和 L2 计算权重大小的方法都包括在内。这类似于线性和逻辑回归的弹性网络算法中使用的惩罚。

L2 方法可能是最常用的，在神经网络领域传统上被称为“权重衰减”。它在统计学中被称为“*收缩*，这个名字鼓励你在学习过程中思考惩罚对模型权重的影响。

> 正则化器的这种特殊选择在机器学习文献中被称为权重衰减，因为在顺序学习算法中，它鼓励权重值衰减到零，除非有数据支持。在统计学中，它提供了一个参数收缩方法的例子，因为它将参数值收缩到零。

—第 144-145 页，[模式识别与机器学习](https://amzn.to/2Q2rEeP)，2006。

回想一下，每个节点都有输入权重和偏置权重。因为“*输入*”是恒定的，所以偏差权重一般不包含在处罚中。

### 处罚的控制影响

当训练网络时，权重的计算大小被添加到损失目标函数中。

可以使用一个新的超参数(称为α(a)或有时称为λ)对它们进行加权，而不是将每个权重直接加到惩罚上。这控制了学习过程应该对惩罚的关注程度。或者换句话说，根据权重的大小来惩罚模型的数量。

alpha 超参数的值介于 0.0(无惩罚)和 1.0(完全惩罚)之间。该超参数控制模型中的偏差量，从 0.0 或低偏差(高方差)到 1.0 或高偏差(低方差)。

如果惩罚力度太大，模型会低估权重，并低估问题。如果惩罚太弱，模型将被允许过度训练数据。

权重的[向量范数](https://machinelearningmastery.com/vector-norms-machine-learning/)通常是每层计算的，而不是在整个网络中计算的。这允许更灵活地选择所使用的正则化类型(例如，L1 用于输入，L2 用于其他地方)以及阿尔法值的灵活性，尽管默认情况下在每个层上使用相同的阿尔法值是很常见的。

> 在神经网络的环境中，有时希望对网络的每一层使用具有不同 a 系数的单独惩罚。因为搜索多个超参数的正确值可能很昂贵，所以在所有层使用相同的权重衰减只是为了减小搜索空间的大小，这仍然是合理的。

—第 230 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

## 使用权重正则化的技巧

本节提供了一些在神经网络中使用权重正则化的技巧。

### 适用于所有网络类型

权重正则化是一种通用方法。

它可以用于大多数，也许是所有类型的神经网络模型，尤其是最常见的多层感知器、卷积神经网络和长短期记忆递归神经网络。

在 LSTMs 的情况下，可能需要对输入和循环连接使用不同的惩罚或惩罚配置。

### 标准化输入数据

更新输入变量以具有相同的比例通常是一种好的做法。

当输入变量具有不同的标度时，网络权重的标度将相应地变化。这在使用权重正则化时引入了一个问题，因为权重的绝对值或平方值必须相加才能用于惩罚。

这个问题可以通过规范化或标准化输入变量来解决。

### 使用更大的网络

对于更大的网络(更多的层或更多的节点)来说，更容易过度填充训练数据是很常见的。

当使用权重正则化时，可以使用更大的网络，过拟合的风险更小。一个好的配置策略可能是从更大的网络开始，并使用权重衰减。

### 网格搜索参数

正则化超参数通常使用较小的值来控制每个权重对惩罚的贡献。

也许可以从测试对数标度的值开始，例如 0.1、0.001 和 0.0001。然后按照最有希望的数量级进行网格搜索。

### 一起使用 L1 + L2

与其试图在 L1 和 L2 的点球之间做出选择，不如两者都用。

现代有效的线性回归方法，如弹性网，同时使用 L1 和 L2 惩罚，这可能是一个有用的尝试方法。这给了你 L2 的细微差别和 L1 鼓励的稀疏性。

### 在训练有素的网络上使用

权重正则化的使用可以允许更精细的训练方案。

例如，模型可以在没有任何正则化的情况下首先适合训练数据，然后使用权重惩罚来减少已经表现良好的模型的权重的大小，随后进行更新。

**你有什么使用权重正则化的小技巧吗？**
在下面的评论里告诉我。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 书

*   第 7.1 节参数定额处罚，[深度学习](https://amzn.to/2NJW3gE)，2016。
*   第 5.5 节神经网络中的正则化，[模式识别和机器学习](https://amzn.to/2Q2rEeP)，2006。
*   第 16.5 节权重衰减，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2PBsezv)，1999。
*   第 4.4.2 节加入权重正则化，[Python 深度学习](https://amzn.to/2wVqZDq)，2017。
*   第 6.2 节收缩方法，[统计学习导论:在 R](https://amzn.to/2MXGK7I) 中的应用，2013。

### 报纸

*   [一个简单的权重衰减可以提高泛化](https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization)，1992。
*   [关于非线性学习系统中的泛化、正则化和体系结构选择的注记](https://ieeexplore.ieee.org/abstract/document/239541/)，1991。

### 文章

*   [正则化(数学)，维基百科。](https://en.wikipedia.org/wiki/Regularization_(mathematics))
*   [神经网络中的权重衰减。](https://metacademy.org/graphs/concepts/weight_decay_neural_networks)
*   [为什么神经网络禁止大权重？](https://datascience.stackexchange.com/questions/23287/why-large-weights-are-prohibited-in-neural-networks)

## 摘要

在这篇文章中，您发现了权重正则化作为一种减少神经网络过拟合的方法。

具体来说，您了解到:

*   神经网络中的大权重是一个更复杂的网络过度训练数据的标志。
*   在训练期间根据网络权重的大小来惩罚网络可以减少过拟合。
*   可以将 L1 或 L2 向量范数惩罚添加到网络优化中，以鼓励更小的权重。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。