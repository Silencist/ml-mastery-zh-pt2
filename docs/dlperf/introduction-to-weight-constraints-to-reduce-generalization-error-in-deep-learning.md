# 深度学习中权重限制的温和介绍

> 原文：<https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/>

最后更新于 2019 年 8 月 6 日

当训练神经网络以鼓励网络使用小权重时，权重正则化方法(如权重衰减)会对损失函数引入惩罚。

神经网络中较小的权重可以导致模型更加稳定，并且不太可能[过拟合训练数据集](https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/)，从而在对新数据进行预测时具有更好的表现。

与权重正则化不同，权重约束是一个触发器，它检查权重的大小或大小，并对它们进行缩放，使它们都低于预定义的阈值。该约束迫使权重变小，可以代替权重衰减，并与更积极的网络配置结合使用，例如[非常大的学习率](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/)。

在这篇文章中，您将发现使用权重约束正则化作为权重惩罚的替代，以减少深度神经网络中的过拟合。

看完这篇文章，你会知道:

*   权重惩罚鼓励但不要求神经网络具有小的权重。
*   权重约束，例如 L2 范数和最大范数，可以用来迫使神经网络在训练期间具有小的权重。
*   当与其他正则化方法(如 dropout)结合使用时，权重约束可以提高泛化能力。

**用我的新书[更好的深度学习](https://machinelearningmastery.com/better-deep-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![A Gentle Introduction to Weight Constraints to Reduce Generalization Error in Deep Learning](img/b4975e59484f7386511fbe18994218a2.png)

轻度介绍权重约束以减少深度学习中的泛化错误
图片由[黎明·埃尔纳](https://www.flickr.com/photos/naturesdawn/2835320842/)提供，保留部分权利。

## 概观

*   大重量处罚的替代方案
*   强制小重量
*   如何使用权重约束
*   权重约束的使用示例
*   使用重量限制的提示

## 大重量处罚的替代方案

神经网络中的大权重是过拟合的标志。

具有大权重的网络很可能已经学会了训练数据中的统计噪声。这导致模型不稳定，并且对输入变量的变化非常敏感。反过来，当对新的未知数据进行预测时，overfit 网络的表现很差。

解决该问题的一种流行且有效的技术是更新在训练期间优化的损失函数，以考虑权重的大小。

这被称为惩罚，因为网络的权重越大，网络受到的惩罚就越多，从而导致更大的损失，进而导致更大的更新。其效果是，惩罚鼓励重量变小，或不超过训练过程中所需的重量，从而减少过度训练。

使用惩罚的一个问题是，尽管它确实鼓励网络朝着更小的权重发展，但它并没有强制更小的权重。

用权重正则化惩罚训练的神经网络可能仍然允许大的权重，在某些情况下是非常大的权重。

## 强制小重量

使用网络权重大小惩罚的另一种解决方案是使用权重约束。

权重约束是对检查权重大小的网络的更新，如果大小超过预定义的限制，权重将被重新缩放，以使其大小低于限制或介于范围之间。

您可以将权重约束看作是一个 if-then 规则，它在网络被训练时检查权重的大小，并且只在需要时生效并使权重变小。注意，为了提高效率，它不一定要作为一个“如果-那么”规则来实现，而且通常不是。

与在损失函数中增加惩罚不同，权重约束确保网络的权重很小，而不是鼓励它们很小。

它可以用于解决这些问题，或者用于抵制其他正则化方法(如权重惩罚)的网络。

当您将网络配置为使用替代正则化方法对正则化进行加权，但仍然希望网络具有较小的权重以减少过拟合时，权重约束证明特别有用。一个经常被引用的例子是权重约束正则化与[缺失正则化](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)的使用。

> 虽然仅丢弃就能带来显著的改善，但使用丢弃和[权重约束]正则化，[……]比仅使用丢弃提供了显著的提升。

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

## 如何使用权重约束

对层中的每个节点强制实现约束。

该层中的所有节点使用相同的约束，并且同一网络中的多个隐藏层通常会使用相同的约束。

回想一下，当我们一般讨论[向量范数](https://machinelearningmastery.com/vector-norms-machine-learning/)时，这是节点中权重向量的大小，默认情况下计算为 L2 范数，例如向量中平方值之和的平方根。

可以使用的一些约束示例包括:

*   强制向量范数为 1.0(例如单位范数)。
*   限制向量范数的最大大小(例如最大范数)。
*   限制向量范数的最小和最大大小(例如，min_max 范数)。

最大范数，也称为 max-norm 或 maxnorm，是一种流行的约束，因为它比其他规范(如单位范数)不那么激进，只需设置一个上限。

> 最大范数正则化以前已经被使用过……它典型地改善了深度神经网络的随机梯度下降训练的表现…

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

使用极限或范围时，必须指定超参数。假设权重很小，超参数通常也是一个小的整数值，例如 1 到 4 之间的值。

> …我们可以使用最大范数正则化。这将每个隐藏单元的输入权重向量的范数约束为常数 c。c 的典型值范围为 3 到 4。

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

如果范数超过指定的范围或限制，权重将被重新缩放或归一化，以使其大小低于指定的参数或在指定的范围内。

> 如果权重更新违反了这个约束，我们通过除法来重整隐藏单元的权重。使用约束而不是惩罚可以防止权重变得非常大，不管建议的权重更新有多大。

——[通过防止特征检测器的共同适应来改进神经网络](https://arxiv.org/abs/1207.0580)，2012。

该约束可以在每次更新权重之后应用，例如在每个小批量的末尾。

## 权重约束的使用示例

本节提供了一些从最近的研究论文中精选的例子，其中使用了重量约束。

Geoffrey Hinton 等人在他们 2012 年发表的题为“[通过防止特征检测器](https://arxiv.org/abs/1207.0580)的共同适应来改进神经网络”的论文中，对应用于 MNIST 手写数字分类任务和 ImageNet 照片分类任务的 CNN 模型使用了 maxnorm 约束。

> 所有层对每个隐藏单元的输入权重都有 L2 权重约束。

Nitish Srivastava 等人在 2014 年发表的题为“T0 丢弃:防止神经网络过拟合的简单方法”的论文中，在 MNIST 手写数字分类任务中使用了带有 MLP 的 maxnorm 约束，在街景门牌号数据集上使用了 CNNs，该数据集的参数是通过保持验证集配置的。

> 最大范数正则化用于卷积层和全连接层的权重。

Jan Chorowski 等人在 2015 年发表的题为“基于注意力的语音识别模型”的论文中使用 LSTM 和注意力模型进行语音识别，最大范数约束设置为 1。

> 我们首先用最大范数为 1 的列范数约束训练我们的模型…

## 使用重量限制的提示

本节提供了一些在神经网络中使用权重约束的技巧。

### 适用于所有网络类型

权重约束是一种通用方法。

它们可以用于大多数，也许是所有类型的神经网络模型，尤其是最常见的多层感知器、卷积神经网络和长短期记忆递归神经网络。

在 LSTMs 的情况下，可能希望对输入和循环连接使用不同的约束或约束配置。

### 标准化输入数据

将输入变量重新缩放到相同的比例是一种很好的通用做法。

当输入变量具有不同的标度时，网络权重的标度将相应地变化。这在使用权重约束时引入了一个问题，因为较大的权重将导致约束更频繁地触发。

这个问题可以通过输入变量的规范化或标准化来解决。

### 使用更高的学习率

使用重量限制可以让你在网络训练中更加积极。

具体来说，可以使用更大的学习率，从而允许网络在每次更新时对权重进行更大的更新。

这被认为是使用权重限制的一个重要好处。例如使用约束结合退出:

> 使用约束而不是惩罚可以防止权重变得非常大，不管建议的权重更新有多大。这使得以非常大的学习率开始成为可能，该学习率在学习期间衰减，因此允许比以小权重开始并使用小学习率的方法更彻底地搜索权重空间。

——[通过防止特征检测器的共同适应来改进神经网络](https://arxiv.org/abs/1207.0580)，2012。

### 尝试其他约束

探索其他权重约束的使用，例如最小和最大范围、非负权重等。

您也可以选择在某些权重上使用约束，而不在其他权重上使用约束，例如在 MLP 中不对偏置权重使用约束，或者在 LSTM 中不对循环连接使用约束。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 书

*   7.2 规范惩罚作为约束优化，[深度学习](https://amzn.to/2NJW3gE)，2016。

### 报纸

*   [丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。
*   [秩、迹范数和最大范数](https://link.springer.com/chapter/10.1007/11503415_37)，2005。
*   [通过防止特征检测器的共同适应来改进神经网络](https://arxiv.org/abs/1207.0580)，2012。

### 文章

*   [Norm(数学)，Wikipedia](https://en.wikipedia.org/wiki/Norm_(mathematics)) 。
*   [正则化，神经网络第 2 部分:设置数据和损失，用于视觉识别的卷积神经网络](https://cs231n.github.io/neural-networks-2/#reg)。

## 摘要

在这篇文章中，您发现了使用权重约束正则化作为权重惩罚的替代方法来减少深度神经网络中的过拟合。

具体来说，您了解到:

*   权重惩罚鼓励但不要求神经网络具有小的权重。
*   诸如 L2 范数和最大范数之类的权重约束可以用于在训练期间强制神经网络具有小的权重。
*   当与其他正则化方法(如 dropout)结合使用时，权重约束可以提高泛化能力。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。