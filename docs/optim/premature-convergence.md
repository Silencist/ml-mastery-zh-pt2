# 过早收敛的温和介绍

> 原文：<https://machinelearningmastery.com/premature-convergence/>

最后更新于 2021 年 10 月 12 日

收敛指的是过程的极限，在评估优化算法的预期表现时，它可能是一个有用的分析工具。

当探索优化算法的学习动态以及使用优化算法训练的机器学习算法时，例如深度学习神经网络，它也可以是有用的经验工具。这激发了对学习曲线和技巧的研究，比如提前停止。

如果优化是一个产生候选解的过程，那么收敛就代表了过程结束时的一个稳定点，此时没有进一步的变化或改进。**过早收敛**指的是优化算法的失败模式，其中过程停止在不代表全局最优解的稳定点。

在本教程中，您将发现机器学习中过早收敛的温和介绍。

完成本教程后，您将知道:

*   收敛是指通过迭代优化算法在一系列解的末尾找到的稳定点。
*   过早收敛是指过早地发现一个稳定点，也许接近搜索的起点，并且带有比预期更差的评估。
*   优化算法的贪婪性提供了对算法收敛速度的控制。

**用我的新书[机器学习优化](https://machinelearningmastery.com/optimization-for-machine-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

Let’s get started.![A Gentle Introduction to Premature Convergence](img/350d59d82e64d68f82209ba07e1acf24.png)

提前收敛的温和介绍[唐·格雷厄姆](https://www.flickr.com/photos/23155134@N06/27803767318/)摄，版权所有。

## 教程概述

本教程分为三个部分；它们是:

1.  机器学习中的收敛性
2.  过早收敛
3.  解决早熟收敛问题

## 机器学习中的收敛性

[收敛](https://en.wikipedia.org/wiki/Limit_of_a_sequence)一般是指一个过程的值随着时间的推移而有行为趋势。

当使用优化算法时，这是一个有用的想法。

优化指的是一类需要从目标函数中找到一组导致最大值或最小值的输入的问题。优化是一个迭代过程，产生一系列候选解，直到在过程结束时最终得到最终解。

达到稳定点最终解的优化算法的这种行为或动态被称为收敛，例如优化算法的收敛。这样，收敛定义了优化算法的终止。

> 局部下降涉及迭代选择下降方向，然后在该方向上迈出一步，并重复该过程，直到满足收敛或某些终止条件。

—第 13 页，[优化算法](https://amzn.to/3bqJvJz)，2019。

*   **收敛**:稳定点所在的优化算法的停止条件，算法的进一步迭代不太可能导致进一步的改进。

我们可以根据经验测量和探索优化算法的收敛性，例如使用[学习曲线](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)。此外，我们还可以分析地探索优化算法的收敛性，例如收敛性证明或平均情况下的计算复杂度。

> 强大的选择压力导致快速但可能过早的收敛。减弱选择压力会减慢搜索过程…

—第 78 页，[进化计算:一种统一的方法](https://amzn.to/2LjWceK)，2002 年。

对于那些通过迭代优化算法在训练数据集上拟合(学习)的算法，例如逻辑回归和人工神经网络，优化和优化算法的收敛是机器学习中的一个重要概念。

因此，我们可以选择比其他算法导致更好的收敛行为的优化算法，或者花费大量时间通过优化的超参数(例如，学习率)来调整优化算法的收敛动态(学习动态)。

收敛行为可以与在收敛时找到的稳定点的目标函数评估以及这些关注点的组合进行比较，通常是根据收敛之前所需的算法迭代次数。

## 过早收敛

过早收敛指的是过早发生的过程的收敛。

在优化中，它指的是算法收敛到一个表现比预期差的稳定点。

过早收敛通常会影响复杂的优化任务，其中目标函数是非凸的，这意味着响应面包含许多不同的好解(稳定点)，可能有一个(或几个)最佳解。

如果我们将优化下的目标函数的响应面视为几何景观，并且我们正在寻求函数的最小值，那么过早优化指的是找到接近搜索起点的谷，该谷的深度小于问题域中最深的谷。

> 对于呈现高度多模态(崎岖)适应性景观或随时间变化的景观的问题，过多的开发通常会导致过早收敛到空间中的次优峰值。

—第 60 页，[进化计算:一种统一的方法](https://amzn.to/2LjWceK)，2002 年。

以这种方式，过早收敛被描述为寻找局部最优解而不是优化算法的全局最优解。这是一个优化算法的具体失败案例。

*   **过早收敛**:优化算法收敛到一个比最优稳定点更差的点，该点可能接近起始点。

换句话说，收敛意味着搜索过程的结束，例如，找到了一个稳定点，并且算法的进一步迭代不可能改进解。过早收敛是指在不太理想的稳定点达到优化算法的停止条件。

## 解决早熟收敛问题

在任何具有合理挑战性的优化任务中，过早收敛都可能是一个相关的问题。

例如，进化计算和遗传算法领域的大多数研究涉及识别和克服算法在优化任务上的过早收敛。

> 如果选择集中在最适合的个体上，由于新种群的多样性降低，选择压力可能导致早熟收敛。

—第 139 页，[计算智能:导论](https://amzn.to/3nDFGn6)，2007 年第 2 版。

基于种群的优化算法，像进化算法和群体智能，经常根据选择压力和收敛之间的相互作用来描述它们的动态。例如，强大的选择压力会导致更快的收敛和可能的过早收敛。较弱的选择压力可能会导致较慢的收敛(较大的计算成本)，尽管可能会找到更好甚至全局的最优解。

> 具有高选择压力的算子比具有低选择压力的算子更快地减少种群的多样性，这可能导致过早收敛到次优解。高选择压力限制了人们的探索能力。

—第 135 页，[计算智能:导论](https://amzn.to/3nDFGn6)，2007 年第 2 版。

这种选择性压力的想法更有助于理解优化算法的学习动态。例如，被配置为过于贪婪的优化(例如，通过诸如步长或学习率的超参数)可能由于过早收敛而失败，而被配置为不太贪婪的相同算法可能克服过早收敛并发现更好的或全局最优解。

当使用随机梯度下降来训练神经网络模型时，可能会遇到过早收敛，表现为学习曲线快速指数下降，然后停止改善。

> 达到收敛所需的更新数量通常随着训练集的大小而增加。然而，随着 m 趋近于无穷大，在 SGD 对训练集中的每个示例进行采样之前，模型最终将收敛到其最佳可能的测试误差。

—第 153 页，[深度学习](https://amzn.to/3oxOwUA)，2016。

拟合神经网络容易过早收敛的事实促使人们使用学习曲线等方法来监控和诊断训练数据集中模型的收敛问题，并使用正则化方法，如[提前停止](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/)，在找到稳定点之前停止优化算法，但代价是保持数据集的表现较差。

因此，对深度学习神经网络的许多研究最终都是为了克服过早收敛。

> 根据经验，我们经常发现“tanh”激活函数比逻辑函数能更快地收敛训练算法。

—第 127 页，[用于模式识别的神经网络](https://amzn.to/3nFrjyF)，1995。

这包括权重初始化等技术，这一点至关重要，因为神经网络的初始权重定义了优化过程的起点，而不良的初始化会导致过早收敛。

> 初始点可以决定算法是否收敛，一些初始点非常不稳定，以至于算法遇到数值困难并完全失败。

—第 301 页，[深度学习](https://amzn.to/3oxOwUA)，2016。

这也包括随机梯度下降优化算法的大量变化和扩展，例如增加动量以使算法不会超过最优值(稳定点)，以及 Adam 为每个正在优化的参数增加一个自动调整的步长超参数([学习率](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/))，显著加快收敛速度。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 教程

*   [如何利用学习曲线诊断机器学习模型表现](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)

### 书

*   [优化算法](https://amzn.to/3bqJvJz)，2019。
*   [遗传算法导论](https://amzn.to/2LEfVpn)，1998。
*   [计算智能:导论](https://amzn.to/3nDFGn6)，第二版，2007。
*   [深度学习](https://amzn.to/3oxOwUA)，2016 年。
*   [进化计算:统一方法](https://amzn.to/2LjWceK)，2002。
*   [用于模式识别的神经网络](https://amzn.to/3nFrjyF)，1995。
*   [概率机器学习:导论](https://probml.github.io/pml-book/book1.html) 2020。

### 文章

*   [一个序列的极限，维基百科](https://en.wikipedia.org/wiki/Limit_of_a_sequence)。
*   [随机变量的收敛，维基百科](https://en.wikipedia.org/wiki/Convergence_of_random_variables)。
*   [过早收敛，维基百科](https://en.wikipedia.org/wiki/Premature_convergence)。

## 摘要

在本教程中，您发现了机器学习中过早收敛的温和介绍。

具体来说，您了解到:

*   收敛是指通过迭代优化算法在一系列解的末尾找到的稳定点。
*   过早收敛是指过早地发现一个稳定点，也许接近搜索的起点，并且带有比预期更差的评估。
*   优化算法的贪婪性提供了对算法收敛速度的控制。

**你有什么问题吗？**
在下面的评论中提问，我会尽力回答。