# 深度学习神经网络的集成学习方法

> 原文：<https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/>

最后更新于 2019 年 8 月 6 日

#### 如何通过组合多个模型的预测来提高表现。

深度学习神经网络是非线性方法。

它们提供了更高的灵活性，并且可以根据可用的培训数据量按比例扩展。这种灵活性的缺点是，他们通过随机训练算法学习，这意味着他们对训练数据的细节很敏感，每次训练时可能会发现不同的权重集，这反过来会产生不同的预测。

一般来说，这被称为具有高方差的神经网络，当试图开发用于进行预测的最终模型时，这可能会令人沮丧。

降低神经网络模型方差的一种成功方法是训练多个模型而不是单个模型，并组合来自这些模型的预测。这被称为集成学习，不仅可以减少预测的方差，还可以产生比任何单一模型都好的预测。

在这篇文章中，您将发现深度学习神经网络的方法，以减少方差并提高预测表现。

看完这篇文章，你会知道:

*   神经网络模型是非线性的，具有很高的方差，这在准备最终模型进行预测时可能会令人沮丧。
*   集成学习将来自多个神经网络模型的预测组合在一起，以减少预测的方差和泛化误差。
*   用于集成学习的技术可以根据不同的元素进行分组，例如训练数据、模型以及如何组合预测。

**用我的新书[更好的深度学习](https://machinelearningmastery.com/better-deep-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![Ensemble Methods to Reduce Variance and Improve Performance of Deep Learning Neural Networks](img/903d818928e302476f753c49855983ce.png)

减少差异和提高深度学习神经网络表现的集成方法
图片由[旧金山表演艺术大学](https://www.flickr.com/photos/usfperformingarts/8769648469/)提供，保留部分权利。

## 概观

本教程分为四个部分；它们是:

1.  神经网络模型的高方差
2.  使用模型集合减少方差
3.  如何集成神经网络模型
4.  集合技术综述

## 神经网络模型的高方差

训练深度神经网络在计算上非常昂贵。

基于数百万个例子训练的非常深的网络可能需要几天、几周甚至几个月的时间来训练。

> 谷歌的基线模型[…]是一个深度卷积神经网络[…]已经在大量核心上使用异步随机梯度下降进行了大约六个月的训练。

——[在神经网络中提取知识](https://arxiv.org/abs/1503.02531)，2015。

投入这么多时间和资源后，不能保证最终的模型泛化误差低，在训练时没有看到的例子上表现良好。

> …训练许多不同的候选网络，然后选择最好的，[……]并丢弃其余的。这种方法有两个缺点。首先，所有训练剩余网络的努力都白费了。第二，[……]在验证集上表现最好的网络可能不是在新测试数据上表现最好的网络。

—第 364-365 页，[用于模式识别的神经网络](https://amzn.to/2I9gNMP)，1995。

神经网络模型是一种非线性方法。这意味着他们可以学习数据中复杂的非线性关系。这种灵活性的缺点是它们对初始条件敏感，无论是初始随机权重还是训练数据集中的统计噪声。

学习算法的这种随机性质意味着，每次训练神经网络模型时，它可能从输入到输出学习到稍微(或显著)不同版本的映射函数，这反过来将在训练和保持数据集上具有不同的表现。

因此，我们可以认为神经网络是一种具有低偏差和高方差的方法。即使在大型数据集上进行训练以满足高方差，在最终模型中使用任何方差来进行预测也会令人沮丧。

## 使用模型集合减少方差

神经网络高方差的解决方案是训练多个模型并组合它们的预测。

这个想法是将来自多个好的但不同的模型的预测结合起来。

一个好的模型有技巧，这意味着它的预测比随机的机会更好。重要的是，模型在不同的方面必须是好的；他们必须做出不同的预测误差。

> 模型平均有效的原因是不同的模型通常不会在测试集上产生相同的错误。

—第 256 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

组合来自多个神经网络的预测增加了偏差，这反过来又抵消了单个训练的神经网络模型的方差。结果是对训练数据的细节、训练方案的选择和单次训练的意外发现不太敏感的预测。

除了减少预测中的方差之外，集合还可以产生比任何单一最佳模型更好的预测。

> …一个委员会的表现可能优于孤立使用的最佳单个网络的表现。

—第 365 页，[用于模式识别的神经网络](https://amzn.to/2I9gNMP)，1995。

这种方法属于一类称为“集合学习”的一般方法，描述了试图最好地利用为同一问题准备的多个模型的预测的方法。

一般来说，集成学习包括在同一数据集上训练多个网络，然后在以某种方式组合预测以做出最终结果或预测之前，使用每个训练的模型来做出预测。

事实上，模型的集成是应用机器学习的标准方法，以确保做出最稳定和最好的预测。

例如，Alex Krizhevsky 等人在他们 2012 年的著名论文《使用深度卷积神经网络的 [Imagenet 分类](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)》中介绍了用于照片分类的非常深度的卷积神经网络(即 AlexNet)，该论文使用多个表现良好的 CNN 模型之间的模型平均来获得当时最先进的结果。将一个模型的表现与两个、五个和七个不同模型的总体平均预测进行比较。

> 对五种类似氯化萘的预测进行平均，误差率为 16.4%。[……]用上述五种氯化萘对预先训练过的两种氯化萘进行平均预测，误差率为 15.3%。

集合也是机器学习竞赛获胜者使用的方法。

> 另一种在任务中获得最佳结果的强大技术是模型组装。[……]如果你看看机器学习竞赛，尤其是在 Kaggle 上，你会看到获胜者使用非常大的模型集合，这些模型集合不可避免地会击败任何一个模型，不管它有多好。

—第 264 页，[用 Python 深度学习](https://amzn.to/2NJq1pf)，2017。

## 如何集成神经网络模型

也许最古老也是最常用的神经网络集成方法被称为网络委员会

在同一数据集上训练具有相同配置和不同初始随机权重的网络集合。然后使用每个模型进行预测，并将实际预测计算为预测的平均值。

由于训练模型的计算开销以及增加更多集成成员带来的表现收益递减，集成中模型的数量通常保持较少。集合可能小到三个、五个或十个训练好的模型。

集成学习领域研究得很好，在这个简单的主题上有许多变化。

考虑改变集成方法的三个主要要素可能会有所帮助；例如:

*   **训练数据**:改变用于训练集合中每个模型的数据选择。
*   **集成模型**:改变集成中所用模型的选择。
*   **组合**:改变组合成员结果的方式选择。

让我们依次仔细看看每个元素。

### 变化的训练数据

用于训练全体成员的数据可以变化。

最简单的方法是使用 k 倍交叉验证来估计所选模型配置的泛化误差。在这个过程中，在 k 个不同的训练数据子集上训练 k 个不同的模型。然后，这些 k 个模型可以被保存并用作一个集合的成员。

另一种流行的方法包括用替换对训练数据集进行重采样，然后使用重采样的数据集训练网络。重采样过程意味着每个训练数据集的组成是不同的，重复示例的可能性允许在数据集上训练的模型对样本密度有稍微不同的预期，进而有不同的泛化误差。

这种方法被称为引导聚合，简称 bagging，设计用于具有高方差和低偏差的未运行决策树。通常使用大量决策树，例如数百或数千棵，因为它们准备起来很快。

> …降低方差并因此提高统计学习方法的预测准确率的一种自然方法是从总体中提取许多训练集，使用每个训练集构建单独的预测模型，并对结果预测进行平均。[……]当然，这并不实际，因为我们通常无法访问多个训练集。相反，我们可以通过从(单个)训练数据集中获取重复样本来引导。

—第 216-317 页，[R](https://amzn.to/2zxHR5E)中应用的统计学习介绍，2013。

一种等效的方法可能是使用较小的训练数据集子集而不进行正则化，以允许更快的训练和一些过拟合。

对稍微优化不足的模型的期望更普遍地适用于整体成员的选择。

> ……委员会成员不应被单独挑选，以在偏差和方差之间进行最佳权衡，而应具有相对较小的偏差，因为额外的方差可以通过平均来消除。

—第 366 页，[用于模式识别的神经网络](https://amzn.to/2I9gNMP)，1995。

其他方法可以包括选择输入空间的随机子空间来分配给每个模型，例如输入空间中超体积的子集或输入特征的子集。

#### 集成教程

有关不同训练数据的深度学习集成的示例，请参见:

*   [如何开发用于深度学习的随机拆分、交叉验证和装袋集成](https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/)

### 可变模型

给定问题的难度和学习算法的随机性质，在具有不同初始条件的相同数据上训练相同的欠约束模型将导致不同的模型。

这是因为网络试图解决的优化问题是如此具有挑战性，以至于有许多“*好的*”和“*不同的*”解决方案来将输入映射到输出。

> 大多数神经网络算法达到次优表现，特别是由于存在大量次优局部极小值。如果我们取一组已经收敛到局部极小值的神经网络，并应用平均，我们可以构造一个改进的估计。理解这一事实的一种方式是考虑到，一般来说，落入不同局部最小值的网络在特征空间的不同区域中表现不佳，因此它们的误差项不会强相关。

——[当网络不一致时:混合神经网络的集成方法](https://www.worldscientific.com/doi/abs/10.1142/9789812795885_0025)，1995。

这可能导致方差减小，但可能不会显著改善泛化误差。模型产生的误差可能仍然高度相关，因为模型都学习了相似的映射函数。

另一种方法可能是改变每个集成模型的配置，例如使用具有不同容量(例如，层数或节点数)的网络或在不同条件下训练的模型(例如，学习率或正则化)。

结果可能是模型的集合，这些模型已经学习了映射函数的更异质的集合，并且反过来在它们的预测和预测误差中具有更低的相关性。

> 随机初始化的差异、小匹配的随机选择、超参数的差异或神经网络的非确定性实现的不同结果通常足以导致集成的不同成员产生部分独立的错误。

—第 257-258 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

这种不同配置模型的集合可以通过开发网络和调整其超参数的正常过程来实现。在这个过程中，可以保存每个模型，并选择更好的模型的子集来组成集合。

> 稍微训练不足的网络是大多数调优算法的免费副产品；希望使用这样的额外副本，即使它们的表现明显差于找到的最佳表现。通过使用最佳可用参数和在可用数据库的不同子集上训练不同的副本，仔细规划集合分类，可以获得更好的表现。

——[神经网络集成](https://ieeexplore.ieee.org/abstract/document/58871/)，1990。

在单个模型可能需要几周或几个月来训练的情况下，另一种选择可能是在训练过程中定期保存最佳模型，称为快照或检查点模型，然后在保存的模型中选择集成成员。这提供了在相同数据上训练多个模型的好处，尽管是在单次训练中收集的。

> 快照集合从单个训练过程中产生精确和多样模型的集合。快照集成的核心是一个优化过程，在收敛到最终解决方案之前，会访问几个局部最小值。我们在这些不同的最小值拍摄模型快照，并在测试时对它们的预测进行平均。

——[快照合集:1 号列车，免费获得 M](https://arxiv.org/abs/1704.00109)，2017。

快照集成的一个变化是保存一系列时期的模型，可能是通过在训练期间查看模型在训练和验证数据集上的学习曲线来识别的。来自这种连续模型序列的集成被称为水平集成。

> 首先，选择为相对稳定的历元范围训练的网络。每个标签概率的预测由标准分类器在选定的时期内产生，然后进行平均。

——[深度表示分类的水平和垂直集合](https://arxiv.org/abs/1306.2759)，2013。

快照集成的进一步增强是在训练期间系统地改变优化过程，以强制不同的解决方案(即权重集)，其中最好的可以保存到检查点。这可能涉及在训练时期注入振荡量的噪声，或者在训练时期振荡学习率。这种方法的一个变种叫随机梯度下降与热重启(SGDR)证明了更快的学习和最先进的标准照片分类任务的结果。

> 我们的 SGDR 通过安排学习率来模拟热重启，以获得快 2 到 4 倍的竞争结果。我们还与 SGDR 达成了新的最先进的成果，主要是通过使用更广泛的(模型)和 SGDR 轨迹快照的集合。

——[SGDR:温重启随机梯度下降](https://arxiv.org/abs/1608.03983)，2016。

深度神经网络的一个好处是，中间隐藏层提供了低分辨率输入数据的学习表示。隐藏层可以直接输出它们的内部表示，来自一个非常深的网络的一个或多个隐藏层的输出可以用作新分类模型的输入。当使用自动编码器模型训练深度模型时，这可能是最有效的。这种类型的集成称为垂直集成。

> 该方法集成了一系列输入是中间层表示的分类器。由于这些特性看起来多种多样，因此错误率预计会更低。

——[深度表示分类的水平和垂直集合](https://arxiv.org/abs/1306.2759)，2013。

#### 集成教程

有关不同模型的深度学习集成的示例，请参见:

*   [如何开发深度学习的快照集成](https://machinelearningmastery.com/snapshot-ensemble-deep-learning-neural-network/)
*   [如何开发深度学习的水平投票集成](https://machinelearningmastery.com/horizontal-voting-ensemble/)

### 不同的组合

组合预测的最简单方法是计算来自集成成员的预测平均值。

通过对每个模型的预测进行加权，可以稍微改善这一点，其中权重使用保持验证数据集进行优化。这提供了一种加权平均集成，有时称为模型混合。

> ……我们可能会预计，委员会的一些成员通常会比其他成员做出更好的预测。因此，如果我们给予一些委员会成员比其他成员更大的权重，我们希望能够进一步减少错误。因此，我们考虑由成员预测的加权组合给出的广义委员会预测…

—第 367 页，[用于模式识别的神经网络](https://amzn.to/2I9gNMP)，1995。

复杂性的另一步涉及使用新模型来学习如何最好地组合来自每个集成成员的预测。

该模型可以是简单的线性模型(例如，非常像加权平均)，但也可以是复杂的非线性方法，除了每个成员提供的预测之外，还考虑特定的输入样本。这种学习新模型的一般方法被称为模型[堆叠，或堆叠概括](https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/)。

> 堆叠泛化的工作原理是通过推导泛化器相对于所提供的学习集的偏差。这种推导是通过在第二个空间中进行归纳来进行的，当用学习集的一部分进行教学时，第二个空间的输入是(例如)原始归纳者的猜测，并试图猜测其余部分，第二个空间的输出是(例如)正确的猜测。[……]当与单个概化器一起使用时，堆叠概化是一种用于估计(然后校正)概化器的误差的方案，该概化器已经在特定的学习集上被训练，然后被问到特定的问题。

——[堆叠概括](https://www.sciencedirect.com/science/article/pii/S0893608005800231)，1992。

有更复杂的方法来堆叠模型，例如增强，每次添加一个集合成员，以纠正先前模型的错误。复杂性的增加意味着这种方法不太常用于大型神经网络模型。

另一个有点不同的组合是将多个结构相同的神经网络的权重进行组合。可以对多个网络的权重进行平均，希望产生一个新的单一模型，该模型的整体表现优于任何原始模型。这种方法被称为模型加权平均。

> …表明有希望在权重空间中对这些点进行平均，并使用具有这些平均权重的网络，而不是通过对模型空间中网络的输出进行平均来形成集合

——[平均权重带来更宽的最优值和更好的泛化](https://arxiv.org/abs/1803.05407)，2018。

#### 集成教程

有关不同组合的深度学习集成的示例，请参见:

*   [如何开发深度学习的模型平均集成](https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/)
*   [如何开发用于深度学习的加权平均集成](https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/)
*   [如何开发深度学习的堆叠集成](https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/)
*   [如何为深度学习创建 Polyak-Ruppert 集成](https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/)

## 集合技术综述

总之，我们可以列出一些更常见和更有趣的神经网络集成方法，这些方法由可以变化的方法的每个元素组成，如下所示:

*   **变化的训练数据**
    *   [k 倍交叉验证集合](https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/)
    *   [自举聚合(装袋)集成](https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/)
    *   [随机训练子集集成](https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/)
*   **不同型号**
    *   多重训练跑集成
    *   超参数调谐集成
    *   快照集
    *   [横向时代集成](https://machinelearningmastery.com/horizontal-voting-ensemble/)
    *   垂直表征集合
*   **不同组合**
    *   [模型平均集合](https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/)
    *   [加权平均集合](https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/)
    *   [叠加综合(叠加)集合](https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/)
    *   提升集成
    *   [模型加权平均集成](https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/)

没有单一的最佳集成方法；也许可以尝试一些方法，或者让项目的约束来指导你。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 书

*   第 9.6 节网络委员会，[模式识别神经网络](https://amzn.to/2I9gNMP)，1995。
*   第 7.11 节装袋等集成方法，[深度学习](https://amzn.to/2NJW3gE)，2016。
*   第 7.3.3 节模型组装，[Python 深度学习](https://amzn.to/2NJq1pf)，2017。
*   第 8.2 节装袋，随机森林，提升，[R](https://amzn.to/2zxHR5E)中应用的统计学习介绍，2013。

### 报纸

*   [神经网络集成](https://ieeexplore.ieee.org/abstract/document/58871/)，1990。
*   [神经网络集成、交叉验证和主动学习](https://dl.acm.org/citation.cfm?id=2998716)，1994。
*   [当网络不一致时:混合神经网络的集成方法](https://www.worldscientific.com/doi/abs/10.1142/9789812795885_0025)，1995。
*   [快照合集:1 号列车，免费获得 M](https://arxiv.org/abs/1704.00109)，2017。
*   [SGDR:温重启随机梯度下降](https://arxiv.org/abs/1608.03983)，2016。
*   [深度表征分类的水平和垂直集合](https://arxiv.org/abs/1306.2759)，2013。
*   [堆叠概括](https://www.sciencedirect.com/science/article/pii/S0893608005800231)，1992。
*   [平均权重导致更宽的最优值和更好的泛化](https://arxiv.org/abs/1803.05407)，2018。

### 文章

*   一起学习，维基百科。
*   [引导聚合，维基百科](https://en.wikipedia.org/wiki/Bootstrap_aggregating)。
*   [Boosting(机器学习)，维基百科](https://en.wikipedia.org/wiki/Boosting_(machine_learning))。

## 摘要

在这篇文章中，您发现了深度学习神经网络的集成方法，以减少方差并提高预测表现。

具体来说，您了解到:

*   神经网络模型是非线性的，具有很高的方差，这在准备最终模型进行预测时可能会令人沮丧。
*   集成学习将来自多个神经网络模型的预测组合在一起，以减少预测的方差和泛化误差。
*   用于集成学习的技术可以根据不同的元素进行分组，例如训练数据、模型以及如何组合预测。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。