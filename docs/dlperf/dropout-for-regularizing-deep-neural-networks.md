# 用于深度神经网络正则化的丢弃法的温和介绍

> 原文：<https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/>

最后更新于 2019 年 8 月 6 日

深度学习神经网络很可能用很少的例子来快速过度训练训练数据集。

已知具有不同模型配置的神经网络的集成减少了过拟合，但是需要训练和维护多个模型的额外计算费用。

通过在训练期间随机丢弃节点，可以使用单个模型来模拟拥有大量不同的网络架构。这被称为丢失，并提供了一种计算量非常小且非常有效的正则化方法[来减少过拟合并改善各种深度神经网络中的泛化误差](https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/)。

在这篇文章中，你将会发现使用丢失正则化来减少过拟合和提高深度神经网络的泛化能力。

看完这篇文章，你会知道:

*   神经网络中的大权重是一个更复杂的网络过度训练数据的标志。
*   概率性地丢弃网络中的节点是一种简单有效的正则化方法。
*   使用 drop 时，建议使用具有更多训练和使用权重约束的大型网络。

**用我的新书[更好的深度学习](https://machinelearningmastery.com/better-deep-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![A Gentle Introduction to Dropout for Regularizing Deep Neural Networks](img/da8b34269c62c7cb690f070b6ed44b76.png)

《深度神经网络规范化的丢弃生入门》摄影:乔斯林·金霍恩，版权所有。

## 概观

本教程分为五个部分；它们是:

1.  过拟合的问题
2.  随机删除节点
3.  如何丢弃
4.  使用丢弃的例子
5.  使用流失正则化的技巧

## 过拟合的问题

在相对较小的数据集上训练的大型神经网络会过度训练训练数据。

这具有模型学习训练数据中的统计噪声的效果，当在新数据(例如测试数据集)上评估模型时，这导致较差的表现。泛化误差由于过拟合而增加。

减少过拟合的一种方法是在同一数据集上拟合所有可能的不同神经网络，并对每个模型的预测进行平均。这在实践中是不可行的，并且可以使用不同模型的小集合来近似，称为集合。

> 通过无限制的计算，对固定大小的模型进行“规则化”的最佳方式是对参数的所有可能设置的预测进行平均，在给定训练数据的情况下，通过其后验概率对每个设置进行加权。

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

即使是集成近似也存在一个问题，即需要拟合和存储多个模型，如果模型很大，需要几天或几周的时间来训练和调整，这可能是一个挑战。

## 随机删除节点

Dropout 是一种近似并行训练大量不同结构神经网络的正则化方法。

在训练过程中，一些层输出被随机忽略或“*掉出*”这样做的效果是使该层看起来像一个具有不同节点数和与前一层连接性的层，并被当作一个层来对待。实际上，在训练期间对层的每次更新都是使用已配置层的不同“*视图*”来执行的。

> 放弃一个单元，我们指的是暂时将它从网络中移除，以及它的所有传入和传出连接

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

退出会使训练过程变得嘈杂，迫使层内的节点在概率上承担或多或少的输入责任。

这种概念化表明，可能是丢弃打破了网络层共同适应纠正前几层的错误的情况，反过来使模型更加健壮。

> …单位可能会改变，以弥补其他单位的错误。这可能会导致复杂的共同适应。这反过来会导致过拟合，因为这些协同适应不会推广到看不见的数据。[…]

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

Dropout 模拟来自给定层的稀疏激活，有趣的是，这反过来鼓励网络实际学习稀疏表示作为副作用。因此，它可以用作活动正则化的替代，以鼓励自动编码器模型中的稀疏表示。

> 我们发现，作为放弃的副作用，隐藏单元的激活变得稀疏，即使不存在稀疏诱导正则化。

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

因为丢失层的输出是随机二次采样的，所以它在训练期间具有减少容量或细化网络的效果。因此，当使用丢弃时，可能需要更宽的网络，例如更多的节点。

## 如何丢弃

在神经网络中，每一层都实现了丢包。

它可以用于大多数类型的层，如密集的全连接层、卷积层和递归层，如长短期内存网络层。

丢弃可以在网络中的任何或所有隐藏层以及可见或输入层上实现。它不用于输出层。

> 术语“丢失”是指在神经网络中丢失单元(隐藏和可见)。

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

引入了一个新的超参数，它指定了层的输出被丢弃的概率，或者相反，指定了层的输出被保留的概率。解释是一个实现细节，可能会因纸张和代码库而异。

公共值是 0.5 的概率，用于保留隐藏层中每个节点的输出，以及接近 1.0 的值，例如 0.8，用于保留可见层的输入。

> 在最简单的情况下，每个单元都以独立于其他单元的固定概率 p 保留，其中 p 可以使用验证集来选择，也可以简单地设置为 0.5，这对于广泛的网络和任务来说似乎接近最佳。然而，对于输入单元，最佳保留概率通常更接近 1 而不是 0.5。

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

当使用拟合网络进行预测时，在训练之后不使用丢弃。

由于丢失，网络的权重将比正常情况下更大。因此，在最终确定网络之前，首先根据所选的丢弃率来调整权重。然后，网络可以正常使用来进行预测。

> 如果一个单元在训练期间以概率 p 被保留，则该单元的输出权重在测试时乘以 p

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

相反，权重的重新调整可以在训练时执行，在小批量结束时的每次权重更新之后。这有时被称为“T0”反向下降，在训练过程中不需要对重量进行任何修改。Keras 和 PyTorch 深度学习库都以这种方式实现丢弃。

> 在测试时，我们按丢弃率缩小输出。[……]请注意，这个过程可以通过在训练时执行两个操作，并在测试时保持输出不变来实现，这通常是它在实践中的实现方式

—第 109 页，[用 Python 深度学习](https://amzn.to/2wVqZDq)，2017。

Dropout 在实践中运行良好，或许可以取代权重正则化(例如权重衰减)和[活动正则化](https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras/)(例如表示稀疏性)的需要。

> ……与其他标准的计算成本低廉的正则化器(如权重衰减、滤波器范数约束和稀疏活动正则化)相比，dropout 更有效。丢弃也可以结合其他形式的正规化，以产生进一步的改善。

—第 265 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

## 使用 drop 的示例

本节总结了一些在最近的研究论文中使用丢弃的例子，为如何和在哪里使用丢弃提供建议。

Geoffrey Hinton 等人在 2012 年的论文中首次介绍了名为“[通过防止特征检测器的共同适应来改进神经网络”](https://arxiv.org/abs/1207.0580)的 drop，他们将该方法应用于不同问题类型的一系列不同神经网络，取得了改进的结果，包括手写数字识别( [MNIST](https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/) )、照片分类(CIFAR-10)和语音识别(TIMIT)。

> …我们使用相同的丢弃率–所有隐藏单元的丢弃率为 50%，可见单元的丢弃率为 20%

Nitish Srivastava 等人在 2014 年的期刊论文中介绍了名为“[from:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)”的 from，该论文在广泛的计算机视觉、语音识别和文本分类任务中使用了 drop，并发现它在每个问题上的表现都得到了一致的提高。

> 我们为不同领域的数据集上的分类问题训练了脱落神经网络。我们发现，与不使用丢弃的神经网络相比，丢弃提高了所有数据集的泛化表现。

在计算机视觉问题上，不同的丢弃率与最大范数权重约束一起被用于整个网络层。

> 丢弃被应用于网络的所有层，对于网络的不同层(从输入层到卷积层到完全连接层)，保留该单元的概率为 p = (0.9，0.75，0.75，0.5，0.5，0.5)。此外，c = 4 的最大范数约束用于所有权重。[…]

文本分类任务使用了更简单的配置。

> 我们在输入层使用保留概率 p = 0.8，在隐藏层使用 0.5。所有层都使用了 c = 4 的最大范数约束。

Alex Krizhevsky 等人在他们 2012 年的著名论文《使用深度卷积神经网络的图像网分类》》中(当时)在使用深度卷积神经网络和缺失正则化的图像网数据集上获得了照片分类的最新结果。

> 我们在[模型的]前两个完全连接的层中使用了 drop。没有丢弃，我们的网络表现出大量的过度匹配。退出大约是收敛所需迭代次数的两倍。

乔治·达尔(George Dahl)等人在 2013 年发表的题为“使用整流线性单元和脱落改进 LVCSR 的深度神经网络”的论文中使用了具有整流线性激活函数和脱落的深度神经网络，以在标准语音识别任务中获得(当时)最先进的结果。他们使用贝叶斯优化程序来配置激活函数的选择和退出量。

> ……贝叶斯优化程序了解到，对于我们训练的这种规模的 sigmoid 网来说，丢弃是没有帮助的。总的来说，ReLUs 和丢弃似乎合作得很好。

## 使用流失正则化的技巧

本节提供了一些在神经网络中使用脱落正则化的技巧。

### 适用于所有网络类型

丢弃正规化是一种通用的方法。

它可以用于大多数，也许是所有类型的神经网络模型，尤其是最常见的多层感知器、卷积神经网络和长短期记忆递归神经网络。

在 LSTMs 的情况下，可能需要对输入和循环连接使用不同的丢弃率。

### 丢弃率

丢失超参数的默认解释是训练层中给定节点的概率，其中 1.0 表示没有丢失，0.0 表示该层没有输出。

隐藏层中脱落的良好值介于 0.5 和 0.8 之间。输入层使用较大的丢弃率，例如 0.8。

### 使用更大的网络

对于更大的网络(更多的层或更多的节点)来说，更容易过度填充训练数据是很常见的。

当使用脱落正则化时，可以使用更大的网络，具有更小的过拟合风险。事实上，可能需要一个大型网络(每层更多的节点)，因为丢失可能会降低网络的容量。

一个很好的经验法则是将丢弃之前的层中的节点数除以建议的丢弃率，并将其用作使用丢弃的新网络中的节点数。例如，一个网络有 100 个节点，建议的丢弃率为 0.5，当使用 drop 时，将需要 200 个节点(100 / 0.5)。

> 如果 n 是任何层中隐藏单元的数量，p 是保留一个单元的概率……,那么一个好的丢失网应该至少有 n/p 个单元

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

### 网格搜索参数

与其猜测适合你的网络的丢弃率，不如系统地测试不同的丢弃率。

例如，测试值在 1.0 和 0.1 之间，增量为 0.1。

这将帮助您发现什么最适合您的特定模型和数据集，以及模型对丢弃率有多敏感。一个更敏感的模型可能是不稳定的，可能会受益于尺寸的增加。

### 使用权重约束

响应于层激活的概率移除，网络权重的大小将增加。

重量大可能是网络不稳定的标志。

为了抵消这种影响，可以施加权重约束，以强制层中所有权重的范数(大小)低于指定值。例如，建议最大范数约束的值在 3-4 之间。

> [……]我们可以使用最大范数正则化。这将每个隐藏单元的输入权重向量的范数约束为常数 c。c 的典型值范围为 3 到 4。

——[丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。

这确实引入了额外的超参数，可能需要对模型进行调整。

### 用于较小的数据集

像其他正则化方法一样，对于训练数据量有限且模型可能会过度训练数据的问题，dropout 更有效。

有大量训练数据的问题可能会从使用 from 中看到较少的好处。

> 对于非常大的数据集，正则化几乎不会减少泛化误差。在这些情况下，使用脱落和更大模型的计算成本可能超过正则化的好处。

—第 265 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 书

*   第 7.12 节丢弃，[深度学习](https://amzn.to/2NJW3gE)，2016。
*   第 4.4.3 节添加丢弃生，[Python 深度学习](https://amzn.to/2wVqZDq)，2017。

### 报纸

*   [通过防止特征检测器的共同适应来改进神经网络](https://arxiv.org/abs/1207.0580)，2012。
*   [丢弃:防止神经网络过拟合的简单方法](http://jmlr.org/papers/v15/srivastava14a.html)，2014。
*   [利用整流线性单元和压差改善 LVCSR 的深度神经网络](https://ieeexplore.ieee.org/document/6639346/)，2013。
*   [作为自适应正则化的丢弃训练](https://arxiv.org/abs/1307.1493)，2013。

### 邮件

*   [基于 Keras 的深度学习模型中的丢弃正则化](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)
*   [如何利用 LSTM 网络的丢包进行时间序列预测](https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/)

### 文章

*   [丢弃(神经网络)，维基百科](https://en.wikipedia.org/wiki/Dropout_(neural_networks))。
*   [用于视觉识别的正则化卷积神经网络](https://cs231n.github.io/neural-networks-2/#reg)
*   [“丢弃生”是如何构思的？有没有“啊哈”的时刻？](https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/d64yyas)

## 摘要

在这篇文章中，您发现了使用丢失正则化来减少过拟合和提高深度神经网络的泛化能力。

具体来说，您了解到:

*   神经网络中的大权重是一个更复杂的网络过度训练数据的标志。
*   概率性地丢弃网络中的节点是一种简单有效的正则化方法。
*   使用 drop 时，建议使用具有更多训练和使用权重约束的大型网络。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。