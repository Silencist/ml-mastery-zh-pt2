# 机器学习提升集成的本质

> 原文：<https://machinelearningmastery.com/essence-of-boosting-ensembles-for-machine-learning/>

Boosting 是一类强大而流行的集成学习技术。

从历史上看，增强算法的实现具有挑战性，直到 AdaBoost 演示了如何实现增强，该技术才得以有效使用。AdaBoost 和现代梯度提升通过顺序添加模型来校正模型的残余预测误差。因此，众所周知，增强方法是有效的，但构建模型可能会很慢，尤其是对于大型数据集。

最近，为提高计算效率而设计的扩展使得这些方法足够快，可以被更广泛地采用。开源实现，比如 XGBoost 和 LightGBM，意味着增强算法已经成为机器学习竞赛中对表格数据进行分类和回归的首选方法，并且通常表现最好。

在本教程中，您将发现增强机器学习集成的本质。

完成本教程后，您将知道:

*   用于机器学习的提升集成方法递增地添加在训练数据集的加权版本上训练的弱学习器。
*   所有增强算法的基本思想和每个增强算法中使用的关键方法。
*   如何在新的预测建模项目中探索提升背后的基本思想。

**用我的新书[Python 集成学习算法](https://machinelearningmastery.com/ensemble-learning-algorithms-with-python/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![Essence of Boosting Ensembles for Machine Learning](img/cebd94cd13f693b494cb1f56d0f19b57.png)

为机器学习提升集成的本质
阿明·科瓦尔斯基摄，版权所有。

## 教程概述

本教程分为四个部分；它们是:

1.  提升集成
2.  提升集成的本质
3.  提升集成算法家族
    1.  adaboost 在一起
    2.  经典梯度提升集成
    3.  现代梯度提升集成
4.  定制提升集成

## 提升集成

Boosting 是一种强大的集成学习技术。

因此，增强是流行的，可能是写作时最广泛使用的集成技术。

> 提升是过去二十年中引入的最强有力的学习理念之一。

—第 337 页，[统计学习的要素](https://amzn.to/31VPyRW)，2016。

作为一种集成技术，它可以比兄弟方法读起来更复杂，比如自举聚合([装袋](https://machinelearningmastery.com/bagging-ensemble-with-python/))和堆叠泛化([堆叠](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/))。实际上，实现可能非常复杂，但是提升集成背后的想法非常简单。

提振可以通过与装袋的对比来理解。

在 bagging 中，通过对同一训练数据集制作多个不同的样本，并在每个样本上拟合一个决策树，来创建一个集成。给定训练数据集的每个样本是不同的，每个决策树是不同的，从而产生稍微不同的预测和预测误差。对所有创建的决策树的预测被组合，导致比拟合单个树更低的误差。

提升的运作方式类似。多个树适合不同版本的训练数据集，并且使用简单的分类投票或回归平均来组合来自树的预测，以产生比适合单个决策树更好的预测。

> …boosting【…】使用简单多数投票结合了一组弱分类器…

—第 13 页，[集成机器学习](https://amzn.to/2C7syo5)，2012。

有一些重要的区别；它们是:

*   训练集中的实例根据难度分配权重。
*   学习算法必须注意实例权重。
*   集成成员按顺序添加。

第一个区别是使用相同的训练数据集来训练每个决策树。不执行训练数据集的采样。相反，训练数据集中的每个示例(每行数据)都根据集合发现该示例预测的难易程度来分配权重。

> 这个算法背后的主要思想是把更多的焦点放在更难分类的模式上。焦点的数量由分配给训练集中每个模式的权重来量化。

—第 28-29 页，[使用集成方法的模式分类](https://amzn.to/2zxc0F7)，2010。

这意味着使用集合容易预测的行权重较小，而难以正确预测的行权重较大。

> 增强的工作方式类似，只是树是按顺序生长的:每棵树都是利用以前生长的树的信息生长的。升压不涉及自举采样；相反，每棵树都适合原始数据集的修改版本。

—第 322 页，[R](https://amzn.to/2CjflZa)中应用的统计学习介绍，2014。

装袋的第二个区别是基础学习算法，例如决策树，必须注意训练数据集的权重。反过来，这意味着 boosting 被特别设计成使用决策树作为基础学习器，或者在构建模型时支持行加权的其他算法。

模型的构建必须更多地关注与其分配权重成比例的训练样本。这意味着集成成员是以一种有偏见的方式构建的，以便在权重很大的例子上做出(或努力做出)正确的预测。

最后，缓慢地构建增强集成。集成成员按顺序添加，一个接一个，依此类推，直到集成拥有所需数量的成员。

重要的是，在添加每个集成成员之后，基于整个集成的能力更新训练数据集的权重。这确保了随后添加的每个成员努力纠正整个模型在训练数据集上产生的错误。

> 然而，在增强过程中，每个后续分类器的训练数据集越来越关注由先前生成的分类器错误分类的实例。

—第 13 页，[集成机器学习](https://amzn.to/2C7syo5)，2012。

每个模型对最终预测的贡献是每个模型表现的加权和，例如加权平均或加权投票。

这种增加集成成员来纠正训练数据集中的错误听起来像是最终会过度填充训练数据集。在实践中，提升集成可以过拟合训练数据集，但通常，效果是微妙的，过拟合不是主要问题。

> 与装袋和随机森林不同，如果(树木数量)太大，提升会过拟合，尽管这种过拟合往往发生得很慢。

—第 323 页，[R](https://amzn.to/2CjflZa)中应用的统计学习介绍，2014。

这是 boosting 集成方法的高级总结，非常类似于 AdaBoost 算法，但是我们可以概括该方法并提取基本元素。

## 提升集成的本质

提升的本质听起来可能是关于修正预测。

这就是所有现代提升算法的实现方式，这是一个有趣而重要的想法。然而，校正预测误差可能被认为是实现增强的实现细节(一个大而重要的细节)，而不是提升集成方法的本质。

提升的本质是多个弱学习器组合成一个强学习器。

> 分类器的提升和集成策略是学习许多弱分类器并以某种方式组合它们，而不是试图学习单个强分类器。

—第 35 页，[集成机器学习](https://amzn.to/2C7syo5)，2012。

弱学习器是一种技能非常一般的模型，通常意味着其表现略高于用于二进制分类或预测回归平均值的随机分类器。传统上，这意味着一个决策树桩，它是一个考虑一个变量的一个值并进行预测的决策树。

> 弱学习器(WL)是一种学习算法，它能够产生错误概率严格(但仅略微)小于随机猜测概率的分类器

—第 35 页，[集成机器学习](https://amzn.to/2C7syo5)，2012。

弱学习器可以与在预测建模问题上表现良好的强学习器形成对比。一般来说，我们寻求一个强有力的学习器来解决分类或回归问题。

> ……一个强有力的学习器(SL)能够(给定足够的训练数据)以任意小的错误概率产生分类器。

—第 35 页，[集成机器学习](https://amzn.to/2C7syo5)，2012。

尽管我们寻求一个强有力的学习器来解决一个给定的预测建模问题，但是训练他们是有挑战性的。而学习能力弱的人训练起来很快也很容易。

Boosting 认识到了这一区别，并建议从多个弱学习器中明确构建一个强学习器。

> Boosting 是一类机器学习方法，基于简单分类器的组合(由弱学习器获得)可以比单独的任何简单分类器表现更好的思想。

—第 35 页，[集成机器学习](https://amzn.to/2C7syo5)，2012。

已经探索了许多促进发展的方法，但只有一种是真正成功的。这是上一节描述的方法，其中弱学习器被顺序地添加到集合中，以专门解决或校正回归树的残差或分类的类标签预测误差。结果是学习能力很强。

让我们仔细看看集成方法，它可能被认为是提升家族的一部分。

## 提升集成算法家族

有大量的提升集成学习算法，尽管它们的工作方式基本相同。

也就是说，它们包括顺序添加简单的基本学习器模型，这些模型在训练数据集的(重新)加权版本上进行训练。

> 术语增强指的是能够将弱学习器转化为强学习器的一系列算法。

—第 23 页，[集合方法](https://amzn.to/2XZzrjG)，2012。

我们可以考虑三大类提升方法；它们是:AdaBoost、经典梯度提升和现代梯度提升。

这种划分有些随意，因为有些技术可能跨越所有组或实现，可以配置成实现每个组的一个示例，甚至基于 bagging 的方法。

### adaboost 在一起

最初，朴素增强方法探索在训练数据集的单独样本上训练弱分类器并组合预测。

与装袋相比，这些方法并不成功。

自适应升压，简称 AdaBoost，是升压的第一个成功实现。

> 研究人员努力了一段时间来寻找提升理论的有效实现，直到弗伦德和沙皮雷合作产生了 AdaBoost 算法。

—第 204 页，[应用预测建模](https://amzn.to/2AHaFfu)，2013 年。

这不仅仅是提升原则的成功实现；这是一种有效的分类算法。

> 增强，尤其是以 AdaBoost 算法的形式，被证明是一个强大的预测工具，通常优于任何单个模型。它的成功引起了建模社区的注意，它的使用变得广泛…

—第 204 页，[应用预测建模](https://amzn.to/2AHaFfu)，2013 年。

虽然 AdaBoost 最初是为二进制分类而开发的，但后来被扩展为多类分类、回归以及无数其他扩展和专用版本。

他们首先维护相同的训练数据集，并引入训练示例的权重和训练模型的顺序添加，以校正集合的预测误差。

### 经典梯度提升集成

AdaBoost 成功后，很多注意力都放在了 boosting 方法上。

梯度提升是 AdaBoost 技术组的推广，它允许使用任意损失函数来实现每个后续基础学习的训练。

> 与其为每个不同的损失函数推导新版本的升压，不如推导一个通用版本，称为梯度升压。

—第 560 页，[机器学习:概率视角](https://amzn.to/3e9kL6D)，2012 年。

梯度提升中的“*梯度*”指的是来自所选损失函数的预测误差，通过添加基础学习器使其最小化。

> 梯度提升的基本原理如下:给定损失函数(例如，回归的平方误差)和弱学习器(例如，回归树)，算法寻求找到最小化损失函数的加性模型。

—第 204 页，[应用预测建模](https://amzn.to/2AHaFfu)，2013 年。

在最初将 AdaBoost 重新定义为梯度提升和使用交替损失函数之后，有了许多进一步的创新，例如多元自适应回归树(MART)、树增强和梯度提升机(GBM)。

> 如果我们将梯度提升算法与(浅)回归树相结合，在将回归树拟合到残差(负梯度)后，我们得到了一个称为 MART【…】的模型，我们重新估计了树叶处的参数，以最小化损失…

—第 562 页，[机器学习:概率视角](https://amzn.to/3e9kL6D)，2012 年。

该技术被扩展到包括正则化，试图进一步减缓每个决策树的行和列的学习和采样，以便基于装袋的思想(称为随机梯度提升)为集成成员增加一些独立性。

### 现代梯度提升集成

该方法的梯度提升和方差被证明是非常有效的，但通常训练速度很慢，尤其是对于大型训练数据集。

这主要是由于集成成员的顺序训练，不能并行化。这是不幸的，因为并行训练集成成员和它提供的计算速度是使用集成的一个经常被描述的理想特性。

因此，在提高该方法的计算效率方面付出了很多努力。

这导致了梯度提升的高度优化的开源实现，引入了创新技术，既加速了模型的训练，又提供了进一步改进的预测表现。

值得注意的例子包括极限梯度提升(XGBoost)和光梯度提升机(LightGBM)项目。这两种方法都非常有效，以至于在处理表格数据时，它们成为了机器学习竞赛中实际使用的方法。

## 定制提升集成

我们简要回顾了提升算法的典型类型。

像 XGBoost 和 LightGBM 这样的算法的现代实现提供了足够的配置超参数来实现许多不同类型的增强算法。

虽然与像 bagging 这样更简单的实现方法相比，boosting 最初很难实现，但基本的想法可能有助于在您自己的预测建模项目中探索或扩展集成方法。

从弱学习器中培养一个强学习器的基本思想可以通过许多不同的方式来实现。例如，用决策树桩或标准机器学习算法的其他类似弱学习器配置装袋可以被认为是这种方法的实现。

*   给弱势学习器装袋。

它还提供了与其他集合类型的对比，例如堆叠，试图将多个强学习器组合成一个稍微强的学习器。即便如此，通过堆叠不同的弱学习器，也许在一个项目上可以获得交替的成功。

*   堆叠弱学习器。

有效提升的途径包括加权训练示例的具体实现细节、能够实现权重的模型，以及适合某种损失最小化方法的模型的顺序添加。

然而，这些原则可以更普遍地用于集合模型。

例如，也许成员可以被顺序地添加到装袋或堆叠集合中，并且只有当它们导致技能的有用提升、预测误差的下降或模型所做的预测分布的改变时才被保留。

*   顺序装袋或堆叠。

从某种意义上说，叠加提供了一种实现修正其他模型预测的想法。元模型(1 级学习器)试图有效地组合基础模型(0 级学习器)的预测。从某种意义上说，它正试图修正这些模型的预测。

可以添加堆叠模型的级别来满足特定要求，例如最小化一些或所有预测误差。

*   模型的深度堆叠。

这些也许是几个显而易见的例子，说明如何探索提升方法的本质，希望能激发更多的想法。我鼓励你集思广益，如何让这些方法适应你自己的特定项目。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 相关教程

*   [如何在 Python 中开发梯度提升机集成](https://machinelearningmastery.com/gradient-boosting-machine-ensemble-in-python/)
*   [使用 Sklearn、XGBoost、LightGBM 和 CatBoost 进行梯度提升](https://machinelearningmastery.com/gradient-boosting-with-Sklearn-xgboost-lightgbm-and-catboost/)
*   [机器学习梯度提升算法简介](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)

### 书

*   [使用集成方法的模式分类](https://amzn.to/2zxc0F7)，2010。
*   [集成方法](https://amzn.to/2XZzrjG)，2012。
*   [集成机器学习](https://amzn.to/2C7syo5)，2012。
*   [统计学习的要素](https://amzn.to/31VPyRW)，2016。
*   [R](https://amzn.to/2CjflZa)中应用的统计学习导论，2014。
*   [机器学习:概率视角](https://amzn.to/3e9kL6D)，2012。
*   [应用预测建模](https://amzn.to/2AHaFfu)，2013。

### 文章

*   [Boosting(机器学习)，维基百科](https://en.wikipedia.org/wiki/Boosting_(machine_learning))。

## 摘要

在本教程中，您发现了增强机器学习集成的本质。

具体来说，您了解到:

*   用于机器学习的提升集成方法递增地添加在训练数据集的加权版本上训练的弱学习器。
*   所有增强算法的基本思想和每个增强算法中使用的关键方法。
*   如何在新的预测建模项目中探索提升背后的基本思想。

**你有什么问题吗？**
在下面的评论中提问，我会尽力回答。