# 用于训练深度学习神经网络的损失和损失函数

> 原文：<https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/>

最后更新于 2019 年 10 月 23 日

神经网络使用随机梯度下降进行训练，并要求您在设计和配置模型时选择损失函数。

有许多损失函数可供选择，要知道选择什么，甚至什么是损失函数以及它在训练神经网络时所扮演的角色可能是一项挑战。

在这篇文章中，你将发现损失和损失函数在训练深度学习神经网络中的作用，以及如何为你的预测建模问题选择合适的损失函数。

看完这篇文章，你会知道:

*   使用需要损失函数来计算模型误差的优化过程来训练神经网络。
*   最大似然提供了一个框架，用于在训练神经网络和机器学习模型时选择损失函数。
*   交叉熵和均方误差是训练神经网络模型时使用的两种主要损失函数。

**用我的新书[更好的深度学习](https://machinelearningmastery.com/better-deep-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![Loss and Loss Functions for Training Deep Learning Neural Networks](img/23e306de16b49726a70866c363c91073.png)

训练深度学习神经网络的损失和损失函数
图片由[瑞安·阿尔布雷](https://www.flickr.com/photos/ryan-albrey/12904646275/)提供，保留部分权利。

## 概观

本教程分为七个部分；它们是:

1.  作为优化的神经网络学习
2.  什么是损失函数和损失？
3.  最大概似法
4.  最大似然和交叉熵
5.  使用什么损失函数？
6.  如何实现损失函数
7.  损失函数和报告的模型表现

我们将集中讨论损失函数背后的理论。

有关选择和实现不同损失函数的帮助，请参见文章:

*   [训练深度学习神经网络时如何选择损失函数](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)

## 作为优化的神经网络学习

深度学习神经网络从训练数据中学习将一组输入映射到一组输出。

我们无法计算神经网络的完美权重；有太多的未知。相反，学习问题被视为搜索或优化问题，并且使用算法来导航模型可能使用的权重集的空间，以便做出好的或足够好的预测。

典型地，使用随机梯度下降优化算法训练神经网络模型，并且使用误差反向传播算法更新权重。

梯度下降中的“*梯度*”是指误差梯度。使用具有给定权重集的模型进行预测，并计算这些预测的误差。

梯度下降算法试图改变权重，以便下一次评估减少误差，这意味着优化算法正在沿着误差的梯度(或斜率)向下导航。

既然我们知道训练神经网络解决了一个优化问题，我们可以看看如何计算给定权重集的误差。

## 什么是损失函数和损失？

在优化算法的上下文中，用于评估候选解(即一组权重)的函数被称为目标函数。

我们可能会寻求最大化或最小化目标函数，这意味着我们正在搜索分别具有最高或最低分数的候选解。

典型地，对于神经网络，我们寻求最小化误差。因此，目标函数通常被称为成本函数或损失函数，由损失函数计算的值被简称为“*损失*”

> 我们希望最小化或最大化的函数称为目标函数或准则。当我们最小化它时，我们也可以称它为成本函数、损失函数或误差函数。

—第 82 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

成本或损失函数有一项重要的工作，它必须忠实地将模型的所有方面浓缩成一个单一的数字，使得该数字的改进是一个更好模型的标志。

> 成本函数将一个可能复杂的系统的所有好的和坏的方面减少到一个单一的数字，一个标量值，这允许对候选解决方案进行排序和比较。

—第 155 页，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2S8qRdI)，1999。

在优化过程中计算模型误差时，必须选择损失函数。

这可能是一个具有挑战性的问题，因为该功能必须捕获问题的属性，并由对项目和利益相关者很重要的关注点来驱动。

> 因此，重要的是该功能忠实地代表我们的设计目标。如果我们选择了一个较差的误差函数，并获得了不令人满意的结果，那么错误在于我们没有很好地指定搜索的目标。

—第 155 页，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2S8qRdI)，1999。

现在我们已经熟悉了损失函数和损失，我们需要知道使用什么函数。

## 最大概似法

有许多函数可以用来估计神经网络中一组权重的误差。

我们更喜欢这样一种函数，其中候选解的空间映射到平滑(但高维)的景观上，优化算法可以通过迭代更新模型权重来合理地导航。

[最大似然估计](https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/)，或 MLE，是一个推理框架，用于从历史训练数据中找到参数的最佳统计估计:这正是我们试图用神经网络做的。

> 最大似然通过最大化从训练数据中得到的似然函数来寻找参数的最佳值。

—第 39 页，[用于模式识别的神经网络](https://amzn.to/2S8qdwt)，1995。

我们有一个包含一个或多个输入变量的训练数据集，并且我们需要一个模型来估计模型权重参数，该参数最好地将输入示例映射到输出或目标变量。

给定输入，模型试图做出与目标变量的数据分布相匹配的预测。在最大似然下，损失函数估计模型预测的分布与训练数据中目标变量的分布有多接近。

> 解释最大似然估计的一种方法是将其视为最小化由训练集定义的经验分布[…]和模型分布之间的相异度，两者之间的相异度由 KL 散度来衡量。[……]最小化这种 KL 散度正好对应于最小化分布之间的交叉熵。

—第 132 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

使用最大似然作为框架来估计神经网络和一般机器学习的模型参数(权重)的好处是，随着训练数据集中的示例数量增加，模型参数的估计得到改善。这就是所谓的*一致性*的属性。”

> 在适当的条件下，极大似然估计量具有一致性的性质，即随着训练样本数趋近于无穷大，一个参数的极大似然估计收敛于该参数的真值。

—第 134 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

现在我们已经熟悉了最大似然的一般方法，我们可以看看误差函数。

## 最大似然和交叉熵

在最大似然框架下，使用[交叉熵](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)测量两个概率分布之间的误差。

当我们对将输入变量映射到类标签感兴趣的分类问题建模时，我们可以将问题建模为预测一个例子属于每个类的概率。在二分类问题中，会有两个类，所以我们可以预测这个例子属于第一类的概率。在多类分类的情况下，我们可以预测属于每个类的例子的概率。

在训练数据集中，一个示例属于给定类的概率将是 1 或 0，因为训练数据集中的每个样本都是该域的已知示例。我们知道答案。

因此，在最大似然估计下，我们将寻求一组模型权重，最小化给定数据集的模型预测概率分布和训练数据集中概率分布之间的差异。这叫做交叉熵。

> 在大多数情况下，我们的参数模型定义了一个分布……,我们简单地使用最大似然原理。这意味着我们使用训练数据和模型预测之间的交叉熵作为成本函数。

—第 178 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

从技术上讲，交叉熵来自信息论领域，以“比特”为单位。它用于估计估计概率分布和预测概率分布之间的差异。

在预测数量的回归问题中，通常使用均方误差损失函数。

> 一些基本功能非常常用。均方误差在函数逼近(回归)问题中很流行[……]当输出被解释为指示类中的成员概率时，交叉熵误差函数通常用于分类问题。

—第 155-156 页，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2S8qRdI)，1999。

然而，在最大似然估计的框架下，假设目标变量为高斯分布，均方误差可以被认为是模型预测分布和目标变量分布之间的交叉熵。

> 许多作者使用“交叉熵”这个术语来明确表示伯努利分布或软最大值分布的负对数似然性，但这是一个用词不当的说法。由负对数似然构成的任何损失都是训练集定义的经验分布和模型定义的概率分布之间的交叉熵。例如，均方误差是经验分布和高斯模型之间的交叉熵。

—第 132 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

因此，当使用最大似然估计的框架时，我们将实现交叉熵损失函数，这在实践中通常意味着分类问题的交叉熵损失函数和回归问题的均方误差损失函数。

几乎普遍地，深度学习神经网络是在最大似然框架下使用交叉熵作为损失函数进行训练的。

> 大多数现代神经网络是使用最大似然法训练的。这意味着成本函数【……】被描述为训练数据和模型分布之间的交叉熵。

—第 178-179 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

事实上，采用这一框架可能被认为是深度学习的一个里程碑，因为在完全形式化之前，神经网络分类有时通常使用均方误差损失函数。

> 这些算法的变化之一是用损失函数的交叉熵族代替了均方误差。均方误差在 20 世纪 80 年代和 90 年代很流行，但随着统计界和机器学习界之间思想的传播，它逐渐被交叉熵损失和最大似然原理所取代。

—第 226 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

最大似然法被广泛采用，不仅仅是因为它的理论框架，更主要的是因为它产生的结果。具体而言，在输出层使用 sigmoid 或 softmax 激活函数的分类神经网络使用交叉熵损失函数学习得更快、更鲁棒。

> 交叉熵损失的使用极大地改善了具有 sigmoid 和 softmax 输出的模型的表现，这些模型以前在使用均方误差损失时会遭受饱和和学习缓慢的问题。

—第 226 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

## 使用什么损失函数？

我们可以总结前面的部分，并直接建议您应该在最大似然框架下使用的损失函数。

重要的是，损失函数的选择与神经网络输出层使用的激活函数直接相关。这两个设计元素是相互联系的。

将输出层的配置视为关于预测问题框架的选择，将损失函数的选择视为计算问题给定框架的误差的方式。

> 成本函数的选择与输出单位的选择紧密相连。大多数情况下，我们简单地使用数据分布和模型分布之间的交叉熵。如何表示输出的选择决定了交叉熵函数的形式。

—第 181 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

我们将回顾关于输出层和损失函数的每个问题类型的最佳实践或默认值。

### 回归问题

预测实际值的问题。

*   **输出层配置**:一个带有线性激活单元的节点。
*   **损失函数**:均方误差。

### 二分类问题

将一个例子归类为属于两个类别之一的问题。

这个问题被框定为预测一个例子属于第一类的可能性，例如你分配给整数值 1 的类，而另一个类被分配给数值 0。

*   **输出层配置**:一个带乙状线激活单元的节点。
*   **损失函数**:交叉熵，也称对数损失。

### 多类分类问题

将一个例子归类为属于两个以上类别之一的问题。

这个问题的框架是预测一个例子属于每一类的可能性。

*   **输出层配置**:使用 softmax 激活功能，每个类一个节点。
*   **损失函数**:交叉熵，也称对数损失。

## 如何实现损失函数

为了使损失函数具体化，本节解释了损失函数的每种主要类型是如何工作的，以及如何在 Python 中计算分数。

### 均方误差损失

均方误差损失，简称均方误差，是预测值和实际值之间的平方差的平均值。

无论预测值和实际值的符号如何，结果总是正的，理想值为 0.0。损失值被最小化，尽管它可以通过使得分为负而用于最大化优化过程。

下面的 Python 函数提供了一个类似伪代码的函数工作实现，用于计算一系列实际值和一系列预测实值量的均方误差。

```py
# calculate mean squared error
def mean_squared_error(actual, predicted):
	sum_square_error = 0.0
	for i in range(len(actual)):
		sum_square_error += (actual[i] - predicted[i])**2.0
	mean_square_error = 1.0 / len(actual) * sum_square_error
	return mean_square_error
```

为了高效实现，我鼓励您使用 Sklearn [均方误差()函数](http://Sklearn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)。

### 交叉熵损失(或对数损失)

交叉熵损失通常简称为“*交叉熵*”、“*对数损失*、“*逻辑损失*”或简称为“*对数损失*”。

将每个预测的概率与实际的类输出值(0 或 1)进行比较，并根据与期望值的距离计算惩罚概率的分数。惩罚是对数的，为小的差异(0.1 或 0.2)提供小的分数，为大的差异(0.9 或 1.0)提供大的分数。

交叉熵损失被最小化，其中较小的值比较大的值代表更好的模型。预测完美概率的模型的交叉熵或对数损失为 0.0。

二元或两类预测问题的交叉熵实际上是作为所有例子的平均交叉熵来计算的。

下面的 Python 函数提供了一个类似伪代码的函数工作实现，用于计算实际 0 和 1 值列表与类 1 预测概率的交叉熵。

```py
from math import log

# calculate binary cross entropy
def binary_cross_entropy(actual, predicted):
	sum_score = 0.0
	for i in range(len(actual)):
		sum_score += actual[i] * log(1e-15 + predicted[i])
	mean_sum_score = 1.0 / len(actual) * sum_score
	return -mean_sum_score
```

注意，我们将一个非常小的值(在本例中为 1E-15)添加到预测概率中，以避免计算 0.0 的对数。这意味着在实践中，最好的可能损失将是非常接近于零的值，但不完全为零。

可以为多类分类计算交叉熵。这些类已经被热编码，这意味着每个类值都有一个二进制特征，并且预测必须具有每个类的预测概率。交叉熵随后在每个二进制特征之间求和，并在数据集中的所有示例之间取平均值。

下面的 Python 函数提供了一个类似伪代码的函数工作实现，用于计算一系列实际的热编码值与每个类的预测概率的交叉熵。

```py
from math import log

# calculate categorical cross entropy
def categorical_cross_entropy(actual, predicted):
	sum_score = 0.0
	for i in range(len(actual)):
		for j in range(len(actual[i])):
			sum_score += actual[i][j] * log(1e-15 + predicted[i][j])
	mean_sum_score = 1.0 / len(actual) * sum_score
	return -mean_sum_score
```

为了高效实现，我鼓励您使用 Sklearn [log_loss()函数](http://Sklearn.org/stable/modules/generated/sklearn.metrics.log_loss.html)。

## 损失函数和报告的模型表现

给定最大似然的框架，我们知道我们想要在随机梯度下降下使用交叉熵或均方误差损失函数。

然而，我们可能希望也可能不希望使用损失函数来报告模型的表现。

例如，对数损失很难解释，尤其是非机器学习从业者利益相关者。均方差也是如此。相反，报告分别用于分类和回归的模型的准确率和均方根误差可能更重要。

可能还需要根据这些指标而不是损失来选择模型。这是一个重要的考虑因素，因为具有最小损失的模型可能不是具有对项目涉众很重要的最佳度量的模型。

需要考虑的一个好的划分是使用损失来评估和诊断模型的学习情况。这包括优化过程的所有考虑，如过拟合、欠拟合和收敛。然后可以选择一个对项目涉众来说有意义的替代度量来评估模型表现和执行模型选择。

*   **损失**:仅用于评估诊断模型优化。
*   **度量**:用于在项目上下文中评估和选择模型。

相同的度量可以用于两个关注点，但是优化过程的关注点更有可能与项目的目标不同，并且需要不同的分数。然而，通常情况下，改善损失会改善，或者在最坏的情况下，对利率指标没有影响。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 书

*   [深度学习](https://amzn.to/2NJW3gE)，2016 年。
*   [神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2S8qRdI)，1999。
*   [用于模式识别的神经网络](https://amzn.to/2S8qdwt)，1995。

### 文章

*   [最大似然估计，维基百科](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)。
*   [kul lback–leiber 分歧，维基百科](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)。
*   [交叉熵，维基百科](https://en.wikipedia.org/wiki/Cross_entropy)。
*   [均方误差，维基百科](https://en.wikipedia.org/wiki/Mean_squared_error)。
*   [日志丢失，快泰维基](http://wiki.fast.ai/index.php/Log_Loss)。

## 摘要

在这篇文章中，你发现了损失和损失函数在训练深度学习神经网络中的作用，以及如何为你的预测建模问题选择合适的损失函数。

具体来说，您了解到:

*   使用需要损失函数来计算模型误差的优化过程来训练神经网络。
*   最大似然提供了一个框架，用于在训练神经网络和机器学习模型时选择损失函数。
*   交叉熵和均方误差是训练神经网络模型时使用的两种主要损失函数。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。