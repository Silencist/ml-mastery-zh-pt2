# 避免过度训练神经网络的提前停止的温和介绍

> 原文：<https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/>

最后更新于 2019 年 8 月 6 日

训练神经网络的一个主要挑战是训练它们需要多长时间。

太少的训练将意味着模型将会使火车和测试设备下不来。过多的训练将意味着模型会过度训练训练数据集，并且在测试集上表现不佳。

一种折中的方法是在训练数据集上进行训练，但在验证数据集的表现开始下降时停止训练。这种简单、有效且广泛使用的神经网络训练方法被称为提前停止。

在这篇文章中，你会发现在神经网络对训练数据集进行过拟合之前，尽早停止神经网络的训练可以[减少过拟合](https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/)，提高深度神经网络的泛化能力。

阅读这篇文章后，你会知道:

*   训练一个神经网络足够长的时间来学习映射，但又不能长到超过训练数据，这是一个挑战。
*   可以在训练期间监控保持验证数据集上的模型表现，并且当泛化误差开始增加时停止训练。
*   使用提前停止需要选择要监控的表现度量、停止训练的触发器以及要使用的模型权重。

**用我的新书[更好的深度学习](https://machinelearningmastery.com/better-deep-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![A Gentle Introduction to Early Stopping for Avoiding Overtraining Neural Network Models](img/8dabc7ca1c07b61b178457acf488cb30.png)

提前停止避免过度训练神经网络模型简介
图片由[本森夸](https://www.flickr.com/photos/bensonkua/3153328844)提供，版权所有。

## 概观

本教程分为五个部分；它们是:

1.  训练刚刚好的问题
2.  当泛化误差增加时停止训练
3.  如何尽早停止训练
4.  提前停止的例子
5.  提前停止的提示

## 培训是否足够的问题

训练神经网络具有挑战性。

当训练一个大网络时，在训练过程中会有一个点，此时模型将停止泛化，并开始学习训练数据集中的统计噪声。

训练数据集的这种过拟合将导致泛化误差的增加，使得该模型在对新数据进行预测时不太有用。

挑战是训练网络足够长的时间，使其能够学习从输入到输出的映射，但不要训练模型太长，以致于过度训练训练数据。

> 然而，所有标准的神经网络体系结构，如全连接多层感知器，都容易过拟合[10]:虽然网络似乎变得越来越好，即训练集上的误差减小，但在训练过程中的某个时刻，它实际上又开始变得更差，即看不见的例子上的误差增加。

——[提前停止–但是什么时候？](https://link.springer.com/chapter/10.1007/3-540-49430-8_3)，2002 年。

解决这一问题的一种方法是将[个训练时期](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)视为一个超参数，用不同的值多次训练模型，然后选择在训练或保持测试数据集上获得最佳表现的时期数。

这种方法的缺点是需要训练和丢弃多个模型。这可能会导致计算效率低下且耗时，尤其是对于在大型数据集上经过数天或数周训练的大型模型。

## 当泛化误差增加时停止训练

另一种方法是针对大量训练时期对模型进行一次训练。

在训练期间，在每个时期之后，在保持验证数据集上评估模型。如果模型在验证数据集上的表现开始下降(例如，损失开始增加或准确率开始降低)，则训练过程停止。

> …相对于独立数据(通常称为验证集)而言，测量的误差通常一开始会减小，随后随着网络开始过拟合而增大。因此，可以在相对于验证数据集的最小误差点停止训练

—第 259 页，[模式识别与机器学习](https://amzn.to/2Q2rEeP)，2006。

然后使用停止训练时的模型，并且已知该模型具有良好的泛化表现。

这一过程被称为“提前停止”并且可能是最古老和最广泛使用的神经网络正则化形式之一。

> 这种策略被称为提前停止。这可能是深度学习中最常用的正则化形式。它的流行是因为它的有效性和简单性。

—第 247 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

如果像权重衰减这样更新损失函数以鼓励不太复杂的模型的正则化方法被认为是“*显式*”正则化，那么提前停止可以被认为是一种“*隐式*”正则化，很像使用容量较小的较小网络。

> 正则化也可能是隐含的，如提前停止的情况。

——[理解深度学习需要重新思考概括](https://arxiv.org/abs/1611.03530)，2017。

## 如何尽早停止训练

提前停止要求您将网络配置为受限，这意味着它的容量超过了问题所需的容量。

当训练网络时，使用比通常可能需要的更多的训练时期，以给网络足够的机会来适应，然后开始过度训练训练数据集。

使用提前停止有三个要素；它们是:

*   监控模型表现。
*   触发停止训练。
*   要使用的型号的选择。

### 监控表现

培训期间必须监控模型的表现。

这需要选择用于评估模型的数据集和用于评估模型的指标。

通常会分割训练数据集，并使用一个子集(如 30%)作为验证数据集，用于在训练期间监控模型的表现。该验证集不用于训练模型。使用验证数据集中的损失作为监控指标也很常见，尽管您也可以在回归的情况下使用预测误差，或者在分类的情况下使用准确性。

训练数据集中模型的丢失也将作为训练过程的一部分，并且还可以在训练数据集中计算和监控额外的度量。

在每个时期结束时，在验证集上评估模型的表现，这在训练期间增加了额外的计算成本。这可以通过减少评估模型的频率来减少，例如每 2、5 或 10 个训练阶段。

### 提前停止触发器

一旦选择了评估模型的方案，就必须选择停止训练过程的触发器。

触发器将使用监控的表现指标来决定何时停止训练。这通常是模型在保持数据集上的表现，例如损失。

在最简单的情况下，一旦验证数据集上的表现与先前训练时期的验证数据集上的表现相比下降(例如损失增加)，就停止训练。

实践中可能需要更复杂的触发器。这是因为神经网络的训练是随机的，可能有噪声。绘制在图表上，模型在验证数据集上的表现可能会上下波动多次。这意味着过度训练的第一个迹象可能不是停止训练的好地方。

> ……在验证误差开始增加之后，它仍然可以进一步下降………………………………………………………………………………………………………………………………………………………………………………………

——[提前停止–但是什么时候？](https://link.springer.com/chapter/10.1007/3-540-49430-8_3)，2002 年。

一些更复杂的触发器可能包括:

*   在给定的时期数内，度量没有变化。
*   度量的绝对变化。
*   在一定时期内观察到的表现下降。
*   给定时期数内指标的平均变化。

停止时的一些延迟或“耐心”几乎总是一个好主意。

> ……结果表明，与“更快”的标准相比，“更慢”的标准比其他标准停止得更晚，平均而言会导致泛化能力的提高。然而，平均而言，这种改进需要花费的培训时间相当长，而且当使用慢速标准时，培训时间也会发生巨大变化。

——[提前停止–但是什么时候？](https://link.springer.com/chapter/10.1007/3-540-49430-8_3)，2002 年。

### 车型选择

当停止训练时，已知该模型比先前时期的模型具有稍差的泛化误差。

因此，可能需要考虑到底保存哪个模型。具体来说，就是将模型中的权重保存到文件中的训练时期。

这将取决于选择停止训练过程的触发器。例如，如果触发因素是从一个时期到下一个时期表现的简单下降，则前一时期模型的权重将是优选的。

如果需要触发器来观察在固定数量的时期内表现的下降，那么在触发器周期开始时的模型将是优选的。

也许一个简单的方法是，如果模型在保持数据集上的表现比前一个时期更好，则总是保存模型权重。这样，您将始终拥有在保持集上具有最佳表现的模型。

> 每次验证集的错误有所改善，我们都会存储一份模型参数的副本。当训练算法终止时，我们返回这些参数，而不是最新的参数。

—第 246 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

## 提前停止的例子

本节总结了一些使用提前停止的例子。

尹金在 2014 年的论文《卷积神经网络在情感分析中的开创性应用》中使用了提前停止，将 10%的训练数据集用作验证开始。

> 除了在开发集上提前停止之外，我们不会执行任何特定于数据集的调优。对于没有标准开发集的数据集，我们随机选择 10%的训练数据作为开发集。

麻省理工学院(MIT)、伯克利分校(Berkeley)和谷歌(Google)的张等人在 2017 年发表的题为《[理解深度学习需要重新思考泛化](https://arxiv.org/abs/1611.03530)》的论文中强调，在用于照片分类的非常深度的卷积神经网络上，有丰富的数据集，早期停止可能并不总是带来好处，因为模型不太可能过度填充如此大的数据集。

> [关于 ImageNet 上的训练和测试准确性[结果建议]早期停止的潜在表现增益参考。但是，在 CIFAR10 数据集上，我们没有观察到提前停止的任何潜在好处。

剑桥大学的亚林·加尔和邹斌·盖拉马尼在他们 2015 年发表的论文《递归神经网络中丢弃生的理论基础应用》中“将提前停止作为 LSTM 模型在一系列语言建模问题上的不规则基线”*。*

 *> RNN 模型缺乏规律性，使得处理小数据变得困难，为了避免过拟合，研究人员经常使用提前停止，或者小的和指定不足的模型

Alex Graves 等人在他们 2013 年的著名论文《使用深度递归神经网络的语音识别》》中，利用 LSTMs 实现了语音识别的最新成果，同时利用了早期停止。

> 规范化对于无线网络的良好表现至关重要，因为它们的灵活性使它们容易过度适应。本文使用了两个正则项:提前停止和加权噪声

## 提前停止的提示

本节提供了一些在神经网络中使用提前停止正则化的技巧。

### 何时使用提前停止

提前停止是如此容易使用，例如用最简单的触发器，以至于在训练神经网络时几乎没有理由不使用它。

使用提前停止可能是深度神经网络现代训练的主要内容。

> 早停应该几乎普遍使用。

—第 425 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

### 绘制学习曲线以选择触发器

在使用提前停止之前，拟合一个欠约束模型，并在一个训练和验证数据集上监控模型的表现，可能会很有趣。

实时或在长期运行结束时绘制模型的表现将显示特定模型和数据集的训练过程有多嘈杂。

这可能有助于选择提前停止的触发器。

### 监控重要指标

损失是一个很容易在训练中监控并触发提前停止的指标。

问题是损失并不总是抓住模型对你和你的项目最重要的地方。

选择一个表现指标来监控可能更好，该指标最好地定义了模型在您打算使用它的方式方面的表现。这可能是您打算用来报告模型表现的指标。

### 建议的培训时期

提前停止的一个问题是模型没有利用所有可用的训练数据。

可能需要避免过拟合，并对所有可能的数据进行训练，尤其是在训练数据量非常有限的问题上。

一种推荐的方法是将训练时期的数量视为一个超参数，并网格搜索一系列不同的值，可能使用 k 倍交叉验证。这将允许您固定训练时期的数量，并在所有可用数据上拟合最终模型。

可以使用提前停止来代替。提前停止程序可以重复多次。可以记录停止训练的时期号。然后，当在所有可用的训练数据上拟合最终模型时，可以使用所有早期停止重复的历元数的平均值。

每次运行提前停止时，可以使用将训练集分成训练和验证步骤的不同方式来执行该过程。

另一种方法可能是使用验证数据集的早期停止，然后通过对保留的验证集进行进一步的训练来更新最终模型。

### 通过交叉验证提前停止

早期停止可以与 k 倍交叉验证一起使用，尽管不建议这样做。

k-fold 交叉验证程序旨在通过在数据集的不同子集上反复修改和评估来估计模型的泛化误差。

早期停止旨在监控一个模型的泛化误差，并在泛化误差开始下降时停止训练。

它们之所以不一致，是因为交叉验证假设你不知道泛化误差，而提前停止是为了给你基于泛化误差知识的最佳模型。

可能希望使用交叉验证来估计具有不同超参数值的模型的表现，例如学习率或网络结构，同时也使用提前停止。

在这种情况下，如果您有资源来重复评估模型的表现，那么也许训练时期的数量也可以被视为一个要优化的超参数，而不是使用提前停止。

当评估模型的不同超参数值(例如，不同的学习率)时，可以直接使用早期停止，而不需要重复评估，而不是使用具有早期停止的交叉验证。

一个可能的混淆点是，提前停止有时被称为“交叉验证训练”此外，对比较触发器的早期停止的研究可以使用交叉验证来比较不同触发器的影响。

### 超额验证

多次重复早期停止过程可能会导致模型过拟合验证数据集。

这就像过拟合训练数据集一样容易发生。

一种方法是，一旦模型的所有其他超参数都被选择，就只使用提前停止。

另一种策略可以是在每次使用提前停止时，将训练数据集分成不同的训练集和验证集。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 书

*   第 7.8 节早停，[深度学习](https://amzn.to/2NJW3gE)，2016。
*   第 5.5.2 节提前停止，[模式识别与机器学习](https://amzn.to/2Q2rEeP)，2006。
*   第 16.1 节提前停止，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2poqOxc)，1999。

### 报纸

*   [提前停止–但是什么时候？](https://link.springer.com/chapter/10.1007/3-540-49430-8_3)，2002 年。
*   [用非收敛方法改进模型选择](https://www.sciencedirect.com/science/article/pii/S0893608005801224)，1993。
*   [使用交叉验证自动提前停止:量化标准](https://www.sciencedirect.com/science/article/pii/S0893608098000100)，1997。
*   [理解深度学习需要重新思考概括](https://arxiv.org/abs/1611.03530)，2017。

### 邮件

*   [通过在 Python 中使用 XGBoost 提前停止来避免过拟合](https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/)

### 文章

*   [早停，维基百科](https://en.wikipedia.org/wiki/Early_stopping)。

## 摘要

在这篇文章中，你发现在神经网络对训练数据集进行过拟合之前，尽早停止神经网络的训练，可以减少过拟合，提高深度神经网络的泛化能力。

具体来说，您了解到:

*   训练一个神经网络足够长的时间来学习映射，但又不能长到超过训练数据，这是一个挑战。
*   可以在训练期间监控保持验证数据集上的模型表现，并且当泛化误差开始增加时停止训练。
*   使用提前停止需要选择要监控的表现度量、停止训练的触发器以及要使用的模型权重。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。*