# 多模型机器学习入门

> 原文：<https://machinelearningmastery.com/multiple-model-machine-learning/>

最后更新于 2021 年 10 月 22 日

集成学习方法包括组合来自多个贡献模型的预测。

然而，并非所有利用多机器学习模型的技术都是集成学习算法。

将预测问题分成子问题是很常见的。例如，一些问题自然地细分为独立但相关的子问题，并且可以为每个子问题准备一个机器学习模型。不太清楚这些是否代表集成学习的例子，尽管我们可能会将这些方法与集成区分开来，因为有贡献的集成成员无法为整体预测问题提供解决方案(无论多么弱)。

在本教程中，您将发现机器学习的多模型技术及其与集成学习的关系。

完成本教程后，您将知道:

*   多模型机器学习是指以某种方式使用多个模型的技术，类似于集成学习。
*   使用多模型进行多类分类和多输出回归不同于集成，因为没有贡献成员可以解决问题。
*   专家混合可能被认为是一种真正的集成方法，尽管混合机器学习模型可能不是集成学习方法。

**用我的新书[Python 集成学习算法](https://machinelearningmastery.com/ensemble-learning-algorithms-with-python/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![A Gentle Introduction to Multiple-Model Machine Learning](img/af4300cf4153a66eea8c7f3ef4b15d82.png)

多模型机器学习简介
图片由[托拜厄斯·贝格曼](https://www.flickr.com/photos/tobiasbegemann/29545102080/)提供，版权所有。

## 教程概述

本教程分为五个部分；它们是:

1.  多模型技术
2.  多类分类的多模型
3.  多输出回归的多重模型
4.  多专家模型
5.  由多个模型构建的混血儿

## 多模型技术

集成学习涉及结合两个或更多模型的预测的方法。

如果模型具有两个属性，我们可以将它描述为集成学习技术，例如:

*   包括两个或多个模型。
*   预测结合在一起。

我们可能还会建议，集合模型的目标是提高对任何贡献成员的预测。尽管较小的目标可能是提高模型的稳定性，例如减少预测中的方差或预测误差。

然而，有包含集成学习方法元素的模型和模型体系结构，但是不清楚它们是否可以被认为是集成学习。

例如，我们可以将集成学习技术定义为由两个或多个模型组成。问题是，可能有技术有两个以上的模型，但没有结合他们的预测。或者，他们可能会以意想不到的方式组合他们的预测。

> 有一些方法试图利用多个学习器，但从严格意义上来说，它们不能被认为是集成组合方法。

—第 89 页，[集合方法](https://amzn.to/2XZzrjG)，2012。

由于缺乏更好的名称，我们将这些称为“*多模型技术*，以帮助将它们与集成学习方法区分开来。然而，正如我们将看到的，区分这两种机器学习方法的界限并不那么清楚。

*   **多模型技术**:由多个模型组成的机器学习算法，结合了多种技术，但可能不被视为集成学习。

因此，重要的是回顾和探索位于集成学习边界的多模型技术，以便更好地理解集成学习，并利用可能改进我们创建的集成学习模型的相关思想。

存在预测建模问题，其中问题本身的结构可能建议使用多个模型。

通常，这些问题可以自然地分为子问题。这并不意味着将问题分成子问题是给定示例的最佳解决方案；这只是意味着问题自然会分解。

两个例子是多类分类和多输出回归。

## 多类分类的多模型

分类问题包括给输入的例子分配一个类别标签。

二分类任务是那些有两个类的任务。每个示例都有一个决定，要么将其分配给一个类，要么分配给另一个类。如果使用概率建模，则预测该示例属于一类的单一概率，其中逆概率是第二类的概率，称为[二项式概率分布](https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/)。

两个以上的班级会带来挑战。为两个类设计的技术可以扩展到多个类，有时，这很简单。

*   **多类分类**:在给定输入示例的多个类标签中指定一个。

或者，问题可以自然地划分为多个二进制分类任务。

有很多方法可以做到这一点。

例如，这些类可以被分成多个一对一与静态预测问题。然后，模型可以适合每个子问题，通常每个模型使用相同的算法类型。当新示例需要预测时，响应比其他模型更强的模型可以指定预测。这被称为一对多(OvR)或一对多(OvA)方法。

*   **OvR** :将多类分类拆分为每类一个二分类问题的技术。

多类分类问题可以分为多对类，每对类上适合一个模型。同样，新示例的预测可以从响应更强的模型中选择。这被称为一对一(OvO)。

*   **OvR** :将多类分类拆分为每对类一个二分类问题的技术。

有关一对一和一对一分类的更多信息，请参见教程:

*   [如何使用一比一休息和一比一进行多类分类](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)

这种将多类分类问题划分为多个二进制分类问题的方法可以推广。对于任意长度的类，每个类都可以映射到唯一的二进制字符串。然后，一个分类器可以适合预测位串中的每个位，允许使用任意数量的分类器。

然后，该位串可以被映射到具有最接近匹配的类标签。额外的比特起到纠错码的作用，在某些情况下比更简单的 OvR 和 OvO 方法提高了该方法的表现。这种方法被称为纠错输出码，ECOC。

*   **ECOC** :将多类分类拆分成任意数量的二进制分类问题的技术。

有关这种方法的更多信息，请参见教程:

*   [机器学习的纠错输出码(ECOC)](https://machinelearningmastery.com/error-correcting-output-codes-ecoc-for-machine-learning/)

在每种情况下，都使用多个模型，就像一个集合一样。预测也是结合在一起的，就像一个集合方法，尽管是赢家通吃的方法，而不是投票或加权总和。从技术上讲，这是一种组合方法，但与大多数典型的集成学习方法不同。

与集成学习不同，这些技术旨在探索预测问题的自然分解，并利用可能不容易扩展到多个类别的二进制分类问题。

而集成学习不关心开发新的功能，通常只关注于改进贡献模型的预测表现。使用像 OvR、OvR 和 ECOC 这样的技术，根据定义，贡献模型不能用来孤立地解决预测问题。

## 多输出回归的多重模型

回归问题包括预测给定输入示例的数值。

通常，预测单个输出值。然而，存在回归问题，其中必须为每个输入示例预测多个数值。这些问题被称为多输出回归问题。

*   **多输出回归**:给定一个输入，预测两个或多个数值输出。

可以开发模型来一次预测所有目标值，尽管多输出回归问题是可以自然划分为子问题的问题的另一个例子。

与上一节中的二进制分类一样，大多数回归预测建模技术都是为了预测单个值而设计的。预测多个值可能会带来问题，并且需要修改技术。有些技术无法针对多个值进行合理修改。

一种方法是开发单独的回归模型来预测多输出回归问题中的每个目标值。通常，每个模型使用相同的算法类型。例如，具有三个目标值的多输出回归将涉及拟合三个模型，每个目标一个模型。

当需要预测时，相同的输入模式被提供给每个模型，并且每个模型的特定目标被预测并且一起表示该方法的矢量输出。

*   **多输出回归**:多输出回归问题中每个目标使用一个回归模型的技术。

另一个相关的方法是创建回归模型的顺序链。不同之处在于，第一模型的输出预测第一输出目标值，但是该值被用作链中第二模型的输入的一部分，以便预测第二输出目标值，以此类推。

这样，链在回归模型之间引入了线性相关性，允许链中后面模型的输出以链中前面模型的输出为条件。

*   **回归链**:一种技术，其中使用回归模型的顺序链来预测多输出回归问题中的每个目标，链中后面的一个模型使用链中前面模型预测的值。

有关多输出回归的更多信息，请参见教程:

*   [如何用 Python 开发多输出回归模型](https://machinelearningmastery.com/multi-output-regression-models-with-python/)

在每种情况下，都使用多元回归模型，就像一个集合。

与集成的一个可能区别是，每个模型所做的预测没有直接组合。然而，我们可以扩展结合预测的“T0”的定义来涵盖这种方法。例如，在多输出回归模型的情况下，预测是串联的，而在链式回归中是通过条件方法间接连接的。

与集成学习方法的关键区别在于，没有贡献的集成成员可以单独解决预测问题。只有结合所有成员的预测才能找到解决方案。

## 多专家模型

到目前为止，我们已经着眼于根据预测的结构将问题分成子任务。

还有一些问题可以根据输入数据自然地划分为子问题。这可能就像输入特征空间的划分一样简单，也可能是更复杂的事情，比如将图像分为前景和背景，并为每一个都开发一个模型。

神经网络领域对此的一种更普遍的方法被称为专家混合(MoE)。

该方法包括首先将学习任务划分为子任务，为每个子任务开发专家模型，使用选通模型来决定或学习为每个示例使用哪个专家，池化专家的输出，并将模型选通在一起进行最终预测。

*   **MoE** :为每个子任务开发一个专家模型的技术，在对具体的例子进行预测时，学习对每个专家的信任程度。

有关专家混合的更多信息，请参见教程:

*   [混合专家集成的温和介绍](https://machinelearningmastery.com/mixture-of-experts/)

MoE 的两个方面使这种方法独一无二。第一种是输入特征空间的显式划分，第二种是使用门控网络或门控模型，学习在每种情况下信任哪个专家，例如每个输入案例。

与前面的将目标划分为子问题的多类分类和多输出回归的例子不同，混合专家模型中的贡献成员可以解决整个问题，至少是部分或一定程度上。虽然专家可能不适合特定的输入，但它仍然可以用于对其专业领域之外的事情进行预测。

此外，与那些先前审查的方法不同，专家的混合还使用加权和来组合所有贡献成员的预测，尽管是通过门控网络来测量的。

因此，它更类似于更熟悉的集成学习技术，例如[堆叠泛化，称为堆叠](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/)。

## 由多个模型构建的混血儿

另一种类型的机器学习涉及多个模型的使用，并且与集成学习松散相关，这就是混合模型。

混合模型是明确组合两个或更多模型的模型。因此，混合模型的定义可能是模糊的。

*   **混合模型**:以某种方式将两个或多个不同的机器学习模型结合在一起的技术。

例如，学习如何将输入模式压缩到瓶颈层的自动编码器神经网络，其输出然后被馈送到另一个模型，例如支持向量机，将被认为是混合机器学习模型。

这个例子有两个机器学习模型，一个神经网络和一个支持向量机。碰巧的是，模型被一个接一个地线性堆叠成一个管道，管道中的最后一个模型进行预测。

考虑一种集成学习方法，该方法具有不同类型的多个贡献集成成员(例如，逻辑回归和支持向量机)，并使用投票来平均它们的预测。根据更广泛的定义，这种集成也可以被认为是一种混合机器学习模型。

也许集成学习和混合机器学习的关键区别在于混合模型需要使用不同类型的模型。而在集成学习中，集成的贡献成员可以是任何类型。

此外，混合机器学习更有可能将一个或多个模型移植到另一个基础模型上，这与我们在集成学习中所做的拟合单独的模型并组合它们的预测非常不同。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 相关教程

*   [机器学习的离散概率分布](https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/)
*   [如何使用一比一休息和一比一进行多类分类](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)
*   [机器学习的纠错输出码(ECOC)](https://machinelearningmastery.com/error-correcting-output-codes-ecoc-for-machine-learning/)
*   [如何用 Python 开发多输出回归模型](https://machinelearningmastery.com/multi-output-regression-models-with-python/)
*   [混合专家集成的温和介绍](https://machinelearningmastery.com/mixture-of-experts/)
*   [机器学习建模管道的温和介绍](https://machinelearningmastery.com/machine-learning-modeling-pipelines/)

### 书

*   [使用集成方法的模式分类](https://amzn.to/2zxc0F7)，2010。
*   [集成方法](https://amzn.to/2XZzrjG)，2012。
*   [集成机器学习](https://amzn.to/2C7syo5)，2012。

## 摘要

在本教程中，您发现了机器学习的多模型技术及其与集成学习的关系。

具体来说，您了解到:

*   多模型机器学习是指以某种方式使用多个模型的技术，类似于集成学习。
*   使用多模型进行多类分类和多输出回归不同于集成，因为没有贡献成员可以解决问题。
*   专家混合可能被认为是一种真正的集成方法，尽管混合机器学习模型可能不是集成学习方法。

**你有什么问题吗？**
在下面的评论中提问，我会尽力回答。