# 贝叶斯最优分类器的简单介绍

> 原文：<https://machinelearningmastery.com/bayes-optimal-classifier/>

最后更新于 2020 年 8 月 19 日

贝叶斯最优分类器是一个概率模型，它为一个新的例子做出最可能的预测。

它是使用贝叶斯定理描述的，该定理为计算条件概率提供了一种有原则的方法。它还与最大后验概率密切相关:一种被称为 MAP 的概率框架，它为训练数据集找到最可能的假设。

实际上，贝叶斯最优分类器计算量很大，如果不是很难计算的话，相反，像吉布斯算法和朴素贝叶斯这样的简化可以用来近似结果。

在这篇文章中，您将发现贝叶斯最优分类器，用于为新的数据实例做出最准确的预测。

看完这篇文章，你会知道:

*   贝叶斯定理为计算条件概率提供了一种原则性的方法，称为后验概率。
*   最大后验概率是一个概率框架，它找到描述训练数据集的最可能的假设。
*   贝叶斯最优分类器是一种概率模型，它使用训练数据和假设空间来找到最可能的预测，从而对新的数据实例进行预测。

**用我的新书[机器学习概率](https://machinelearningmastery.com/probability-for-machine-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![A Gentle Introduction to the Bayes Optimal Classifier](img/28720b47ab9d64a40382750895cc015a.png)

贝氏最优分类器简介
图片由[内特·洛珀](https://flickr.com/photos/genesisscience/43468319822/)提供，版权所有。

## 概观

本教程分为三个部分；它们是:

1.  贝叶斯定理
2.  最大后验概率
3.  贝叶斯最优分类器

## 贝叶斯定理

回想一下，贝叶斯定理提供了一种计算[条件概率](https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/)的原则性方法。

它包括计算一个结果给定另一个结果的条件概率，使用这种关系的倒数，陈述如下:

*   P(A | B) = (P(B | A) * P(A)) / P(B)

我们正在计算的量通常被称为给定 *B* 的 *A* 的后验概率，而 *P(A)* 被称为 *A* 的先验概率。

可以去掉 *P(B)* 的归一化常数，可以证明后验与给定 A 的 B 乘以前验的概率成正比。

*   P(A | B)与 P(B | A) * P(A)成正比

或者，简单地说:

*   P(A | B) = P(B | A) * P(A)

这是一个有用的简化，因为我们对估计概率不感兴趣，而是对优化一个量感兴趣。一个成比例的数量就足够了。

有关贝叶斯定理主题的更多信息，请参见帖子:

*   [机器学习贝叶斯定理的温和介绍](https://machinelearningmastery.com/bayes-theorem-for-machine-learning)

既然我们已经了解了贝叶斯定理，我们也来看看最大后验概率框架。

## 最大后验概率

机器学习包括找到一个模型([假设](https://machinelearningmastery.com/what-is-a-hypothesis-in-machine-learning/))来最好地解释训练数据。

许多不同的机器学习算法背后都有两个概率框架。

它们是:

*   最大后验概率，一种贝叶斯方法。
*   最大似然估计，一种频率方法。

在机器学习的背景下，这两个框架的目标都是在给定训练数据集的情况下定位最有可能的假设。

具体来说，他们回答了这个问题:

> **给定训练数据，最可能的假设是什么？**

这两种方法都将拟合模型的问题框定为优化问题，并涉及搜索最能描述观测数据的分布和分布参数集。

最大似然法是一种经常使用的方法，而最大似然法提供了一种贝叶斯方法。

> 最大化可能性的一个流行替代方法是最大化参数的贝叶斯后验概率密度。

—第 306 页，[信息论，推理和学习算法](https://amzn.to/2zn1Eny)，2003。

假设将贝叶斯定理简化为一个比例量，我们可以用它来估计比例假设和参数(*θ*)来解释我们的数据集( *X* ，表述如下:

*   P(θ| X)= P(X |θ)* P(θ)

在θ范围内最大化这个量解决了用于估计后验概率的中心趋势(例如，分布的模型)的优化问题。

因此，这种技术被称为“*最大后验估计*，或简称 MAP 估计，有时简称为“*最大后验估计*”

*   最大化 P(X |θ)* P(θ)

关于最大后验概率主题的更多信息，请看帖子:

*   [机器学习最大后验概率的温和介绍](https://machinelearningmastery.com/maximum-a-posteriori-estimation)

现在我们已经熟悉了 MAP 框架，我们可以更仔细地看看贝叶斯最优分类器的相关概念。

## 贝叶斯最优分类器

贝叶斯最优分类器是一种概率模型，在给定训练数据集的情况下，它对一个新的例子做出最可能的预测。

该模型也被称为贝叶斯最优学习器、贝叶斯分类器、贝叶斯最优决策边界或贝叶斯最优判别函数。

*   **贝叶斯分类器**:对新实例进行最可能预测的概率模型。

具体来说，贝叶斯最优分类器回答了这个问题:

> **给定训练数据，新实例最可能的分类是什么？**

这不同于寻求最可能假设(模型)的 MAP 框架。相反，我们有兴趣做一个具体的预测。

> 一般来说，新实例的最可能的分类是通过结合所有假设的预测，用它们的后验概率加权得到的。

—第 175 页，[机器学习](https://amzn.to/2O1R51L)，1997。

下面的等式演示了在给定假设空间( *H* )的情况下，给定训练数据( *D* ，如何计算新实例( *vi* )的条件概率。

*   P(vj | D) =和{h 中的 H} P(vj | hi) * P(hi | D)

其中 *vj* 为待分类的新实例， *H* 为该实例分类的一组假设， *hi* 为给定假设， *P(vj | hi)* 为 *vi* 给定假设 *hi* 的后验概率， *P(hi | D)* 为给定数据*D*的假设 *hi* 的后验概率

选择具有最大概率的结果是贝叶斯最优分类的一个例子。

*   最大总和{h in H} P(vj | hi) * P(hi | D)

任何使用这个方程对例子进行分类的模型都是贝叶斯最优分类器，平均而言，没有其他模型能够胜过这种技术。

> 任何根据[等式]对新实例进行分类的系统都称为贝叶斯最优分类器，或贝叶斯最优学习器。使用相同假设空间和相同先验知识的任何其他分类方法平均来说都不能优于该方法。

—第 175 页，[机器学习](https://amzn.to/2O1R51L)，1997。

我们必须接受这一点。

这是一件大事。

这意味着，平均而言，对相同数据、相同假设集和相同先验概率进行运算的任何其他算法都无法超越这种方法。因此得名“*最优分类器*”

尽管分类器做出了最优预测，但由于训练数据的不确定性以及问题域和假设空间的不完全覆盖，它并不完美。因此，模型会出错。这些误差通常被称为贝叶斯误差。

> 贝叶斯分类器产生最低可能的测试错误率，称为贝叶斯错误率。[……]贝叶斯错误率类似于不可约错误…

—第 38 页，[R](https://amzn.to/2O4gCHv)中应用的统计学习介绍，2017。

因为贝叶斯分类器是最优的，所以贝叶斯误差是可以产生的最小可能误差。

*   **贝叶斯误差**:进行预测时可能出现的最小误差。

此外，该模型通常根据分类来描述，例如贝叶斯分类器。然而，该原理同样适用于回归:即预测建模问题，其中预测数值而不是类别标签。

这是一个理论模型，但它被认为是我们可能希望追求的理想。

> 理论上，我们总是希望使用贝叶斯分类器来预测定性响应。但是对于真实数据，我们不知道给定 X 的 Y 的条件分布，所以计算贝叶斯分类器是不可能的。因此，贝叶斯分类器作为一个无法达到的黄金标准，可以用来与其他方法进行比较。

—第 39 页，[R](https://amzn.to/2O4gCHv)中应用的统计学习介绍，2017。

由于这种最优策略的计算成本，我们可以直接简化这种方法。

两种最常用的简化使用假设的采样算法，例如吉布斯采样，或者使用朴素贝叶斯分类器的简化假设。

*   **吉布斯算法**。基于后验概率的随机采样假设。
*   **朴素贝叶斯**。假设输入数据中的变量是条件独立的。

有关朴素贝叶斯主题的更多信息，请参见帖子:

*   [如何在 Python 中从零开始开发朴素贝叶斯分类器](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm)

然而，许多非线性机器学习算法能够做出的预测实际上是贝叶斯分类器的近似。

> 尽管这是一种非常简单的方法，但 KNN 通常可以生成与最佳贝叶斯分类器惊人接近的分类器。

—第 39 页，[R](https://amzn.to/2O4gCHv)中应用的统计学习介绍，2017。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 邮件

*   [机器学习最大后验概率的温和介绍](https://machinelearningmastery.com/maximum-a-posteriori-estimation)
*   [机器学习贝叶斯定理的温和介绍](https://machinelearningmastery.com/bayes-theorem-for-machine-learning)
*   [如何在 Python 中从零开始开发朴素贝叶斯分类器](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm)

### 书

*   第 6.7 节贝叶斯最优分类器，[机器学习](https://amzn.to/2O1R51L)，1997。
*   第 2.4.2 节贝叶斯误差和噪声，[机器学习基础](https://amzn.to/32Kw53x)，第二版，2018。
*   第 2.2.3 节分类设置，[R](https://amzn.to/2O4gCHv)中应用的统计学习介绍，2017。
*   [信息论、推理和学习算法](https://amzn.to/2zn1Eny)，2003。

### 报纸

*   [多层感知器作为贝叶斯最佳鉴别函数的近似](https://ieeexplore.ieee.org/abstract/document/80266/)，1990。
*   [基于概率分类器链的贝叶斯最优多标签分类](https://www.informatik.uni-marburg.de/~eyke/publications/589.pdf)，2010。
*   [限制性贝叶斯最优分类器](http://new.aaai.org/Papers/AAAI/2000/AAAI00-101.pdf)，2000。
*   [贝叶斯分类器和贝叶斯误差](https://www.cs.helsinki.fi/u/jkivinen/opetus/iml/2013/Bayes.pdf)，2013。

## 摘要

在这篇文章中，您发现了贝叶斯最优分类器，用于对新的数据实例进行最准确的预测。

具体来说，您了解到:

*   贝叶斯定理为计算条件概率提供了一种原则性的方法，称为后验概率。
*   最大后验概率是一个概率框架，它找到描述训练数据集的最可能的假设。
*   贝叶斯最优分类器是一种概率框架，它使用训练数据和假设空间来找到最可能的预测，从而为新的数据实例做出预测。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。