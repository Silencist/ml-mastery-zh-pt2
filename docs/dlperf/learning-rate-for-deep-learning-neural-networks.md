# 训练深度学习神经网络时如何配置学习率

> 原文：<https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/>

最后更新于 2019 年 8 月 6 日

神经网络的权重不能用解析法计算。相反，权重必须通过称为随机梯度下降的经验优化过程来发现。

神经网络的随机梯度下降解决的优化问题是具有挑战性的，并且解的空间(权重集)可以由许多好的解(称为全局最优解)组成，并且容易找到，但是技能低的解(称为局部最优解)。

在这个搜索过程的每一步中，模型的变化量，或者说步长，被称为“*学习率*”，它可能提供了最重要的超参数来调整你的神经网络，以便在你的问题上取得良好的表现。

在本教程中，您将发现在训练深度学习神经网络时使用的学习率超参数。

完成本教程后，您将知道:

*   学习率控制神经网络模型学习问题的速度。
*   如何用合理的默认值配置学习率，诊断行为，并进行敏感性分析。
*   如何通过学习进度计划、动力和自适应学习速度进一步提高绩效。

**用我的新书[更好的深度学习](https://machinelearningmastery.com/better-deep-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![How to Configure the Learning Rate Hyperparameter When Training Deep Learning Neural Networks](img/277dd06a8634b5cd368d90998d9fd797.png)

训练深度学习神经网络时如何配置学习率超参数
图片由[贝恩德·泰勒](https://www.flickr.com/photos/bernd_thaller/30697908097/)提供，保留部分权利。

## 教程概述

本教程分为六个部分；它们是:

1.  学习率是多少？
2.  学习率的影响
3.  如何配置学习率
4.  为学习过程增添动力
5.  使用学习进度计划
6.  适应性学习率

## 学习率是多少？

使用随机梯度下降算法训练深度学习神经网络。

随机梯度下降是一种优化算法，它使用来自训练数据集的示例来估计模型当前状态的误差梯度，然后使用误差反向传播算法来更新模型的权重，简称为[反向传播](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/)。

训练期间更新的权重量被称为步长或“*学习率*”

具体而言，学习率是用于神经网络训练的可配置超参数，其具有小的正值，通常在 0.0 和 1.0 之间的范围内。

> …学习率，决定步长大小的正标量。

—第 86 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

学习率通常用小写希腊字母 eta ( *n* )来表示。

在训练期间，误差的反向传播估计网络中节点权重所负责的误差量。它不是用全量更新权重，而是根据学习率进行缩放。

这意味着 0.1 的学习率(传统上常见的默认值)意味着每次更新权重时，网络中的权重都会更新 0.1 *(估计权重误差)或估计权重误差的 10%。

## 学习率的影响

神经网络学习或逼近一个函数，以最佳地将输入映射到训练数据集中示例的输出。

学习率超参数控制模型学习的速率或速度。具体来说，它控制模型权重每次更新时(例如在每批训练示例结束时)更新的分摊误差量。

给定一个完美配置的学习率，该模型将学习在给定数量的训练时期(通过训练数据)中给定可用资源(层数和每层节点数)的最佳近似函数。

通常，较大的学习率允许模型学习得更快，但代价是达到次优的最终权重集。较小的学习率可以允许模型学习更优的或者甚至全局最优的权重集，但是可能花费更长的时间来训练。

在极端情况下，太大的学习率将导致太大的权重更新，并且模型的表现(例如它在训练数据集上的损失)将随着训练时期而振荡。据说振荡表现是由发散的重量引起的。太小的学习率可能永远不会收敛，或者可能陷入次优解。

> 当学习率过大时，梯度下降会无意中增加而不是减少训练误差。[……]当学习率太小时，训练不仅会更慢，而且可能会永久停留在较高的训练误差上。

—第 429 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

在最坏的情况下，权重更新太大可能会导致权重爆炸(即导致数字溢出)。

> 当使用高学习率时，可能会遇到正反馈回路，其中大的权重导致大的梯度，然后导致权重的大的更新。如果这些更新持续增加权重的大小，那么[权重]会迅速远离原点，直到出现数值溢出。

—第 238 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

因此，我们不应该使用过大或过小的学习率。然而，我们必须以这样一种方式配置模型，即平均来说，找到一组足够好的权重来近似由训练数据集表示的映射问题。

## 如何配置学习率

在训练数据集中为模型找到一个好的学习率值是很重要的。

事实上，学习率可能是为您的模型配置的最重要的超参数。

> 初始学习率【…】这通常是最重要的单个超参数，人们应该始终确保它已经被调整【…】如果只有时间来优化一个超参数，并且使用随机梯度下降，那么这是值得调整的超参数

——[深度架构基于梯度训练的实用建议](https://arxiv.org/abs/1206.5533)，2012。

事实上，如果有资源来调整超参数，那么大部分时间应该用于调整学习率。

> 学习率可能是最重要的超参数。如果你有时间只调整一个超参数，调整学习率。

—第 429 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

不幸的是，我们无法分析计算给定数据集上给定模型的最佳学习率。相反，一个好的(或足够好的)学习率必须通过反复试验来发现。

> ……一般来说，不可能先验地计算出最佳学习率。

—第 72 页，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2S8qRdI)，1999。

学习率要考虑的值范围小于 1.0 且大于 10^-6.

> 具有标准化输入(或映射到(0，1)区间的输入)的神经网络的典型值小于 1 且大于 10^−6

——[深度架构基于梯度训练的实用建议](https://arxiv.org/abs/1206.5533)，2012。

学习率将与优化过程的许多其他方面相互作用，并且这些相互作用可能是非线性的。然而，总的来说，较小的学习率需要更多的训练时期。相反，更高的学习率将需要更少的训练时期。此外，给定误差梯度的噪声估计，较小的[批次尺寸](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)更适合较小的学习率。

学习率的传统默认值是 0.1 或 0.01，这可能是解决您的问题的一个很好的起点。

> 默认值 0.01 通常适用于标准的多层神经网络，但是仅仅依赖这个默认值是愚蠢的

——[深度架构基于梯度训练的实用建议](https://arxiv.org/abs/1206.5533)，2012。

诊断图可用于调查学习率如何影响模型的学习率和学习动态。一个例子是在训练期间创建一个训练时期的损失线图。线图可以显示许多属性，例如:

*   学习速度超过训练时期，如快或慢。
*   模型是学习得太快(急剧上升和平稳)还是学习得太慢(变化很小或没有变化)。
*   通过损失的波动来判断学习率是否过大。

配置学习率既有挑战性又耗时。

> 选择[学习率]的值可能相当关键，因为如果它太小，误差的减少将非常缓慢，而如果它太大，会导致发散振荡。

—第 95 页，[用于模式识别的神经网络](https://amzn.to/2S8qdwt)，1995。

另一种方法是对所选模型的学习率进行敏感性分析，也称为网格搜索。这有助于突出好的学习率所在的数量级，以及描述学习率和表现之间的关系。

从 0.1 到 10^-5 或 10^-6.的对数标度上的网格搜索学习率是很常见的

> 通常情况下，网格搜索包括选取近似对数标度的值，例如，在集合{.1，. 01，103，104，105 }内的学习率

—第 434 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

当绘制时，这种灵敏度分析的结果通常显示为“U”形，其中随着学习率随着固定数量的训练时期而降低，损失降低(表现提高)，直到由于模型未能收敛而损失再次急剧增加的点。

如果你需要帮助来试验你的模型的学习率，请看帖子:

*   [利用深度学习神经网络了解学习率对模型表现的影响](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)

## 为学习过程增添动力

通过将历史添加到权重更新中，可以更容易地训练神经网络。

具体而言，当权重被更新时，可以包括权重的先前更新的指数加权平均值。这种随机梯度下降的变化被称为“动量”，并增加了更新过程的惯性，导致一个方向上的许多过去的更新在未来继续朝着那个方向发展。

> 动量算法累积过去梯度的指数衰减移动平均值，并继续向它们的方向移动。

—第 296 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

动量可以加速那些问题的学习，在这些问题中，优化过程正在导航的高维“*权重空间*”具有误导梯度下降算法的结构，例如平坦区域或陡峭曲率。

> 动量法旨在加速学习，尤其是在面对高曲率、小但一致的梯度或噪声梯度时。

—第 296 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

过去更新的惯性量是通过添加一个新的超参数来控制的，该超参数通常被称为“*动量*或“*速度*，并使用希腊小写字母 alpha ( *a* )的符号。

> ……动量算法引入了一个变量 v，它起着速度的作用——它是参数在参数空间中移动的方向和速度。速度设置为负梯度的指数衰减平均值。

—第 296 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

它具有平滑优化过程的效果，减缓更新以继续先前的方向，而不是卡住或振荡。

> 一个非常简单的处理差异很大的[特征值](https://machinelearningmastery.com/introduction-to-eigendecomposition-eigenvalues-and-eigenvectors/)问题的方法是在梯度下降公式中加入动量项。这有效地通过重量空间增加了运动的惯性，并消除了振荡

—第 267 页，[用于模式识别的神经网络](https://amzn.to/2S8qdwt)，1995。

动量被设置为大于 0.0 且小于 1 的值，其中在实践中使用诸如 0.9 和 0.99 的公共值。

> 实践中常用的[动量]值包括. 5、. 9 和. 99。

—第 298 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

动量并不能使配置学习率变得更容易，因为步长与动量无关。相反，动量可以与步长一起提高优化过程的速度，从而提高在更少的训练时期发现更好的权重集的可能性。

## 使用学习进度计划

使用固定学习率的替代方法是在整个训练过程中改变学习率。

学习率随时间变化的方式(训练时期)称为学习率时间表或学习率衰减。

也许最简单的学习率计划是将学习率从大初始值线性降低到小值。这允许在学习过程的开始有大的权重变化，而在学习过程的结束有小的变化或微调。

> 在实践中，有必要随着时间的推移逐渐降低学习率，因此我们现在表示迭代时的学习率[……]这是因为 SGD 梯度估计器引入了噪声源(m 个训练示例的随机采样)，即使我们达到最小值，该噪声源也不会消失。

—第 294 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

事实上，在训练神经网络时，使用学习率计划可能是最佳实践。配置挑战涉及选择初始学习率和学习率时间表，而不是选择固定的学习率超参数。考虑到学习率计划可能允许的更好的表现，初始学习率的选择可能不如选择固定学习率敏感。

学习率可以衰减到接近零的小值。或者，学习率可以在固定数量的训练时期衰减，然后在剩余的训练时期保持恒定在一个小值，以便于更多的时间微调。

> 在实践中，通常线性衰减学习率，直到迭代[τ]。迭代[τ]后，通常保持[学习率]不变。

—第 295 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

## 适应性学习率

学习算法可以监控模型在训练数据集上的表现，并且可以相应地调整学习率。

这被称为适应性学习率。

也许最简单的实现是一旦模型的表现稳定下来，就使学习率变小，例如通过将学习率降低两倍或一个数量级。

> 优化算法的合理选择是具有衰减学习率的动量 SGD(在不同问题上表现更好或更差的流行衰减方案包括线性衰减，直到达到固定的最小学习率，指数衰减，或者每次验证误差平稳时将学习率降低 2-10 倍)。

—第 425 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

或者，如果在固定数量的训练时期内表现没有提高，学习率可以再次提高。

自适应学习率方法通常优于学习率配置不当的模型。

> 先验选择好的学习率的困难是自适应学习率方法如此有用和流行的原因之一。一个好的自适应算法通常会比简单的反向传播收敛得更快，而反向传播的固定学习率选择不当。

—第 72 页，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2S8qRdI)，1999。

尽管没有一种方法能最好地解决所有问题，但有三种自适应学习率方法已被证明对许多类型的神经网络体系结构和问题类型都是稳健的。

它们是 AdaGrad、RMSProp 和 Adam，并且都为模型中的每个权重保持和调整学习率。

也许最受欢迎的是亚当，因为它建立在 RMSProp 的基础上，增加了动力。

> 此时，一个自然的问题是:应该选择哪种算法？不幸的是，目前在这一点上没有共识。目前，最流行的优化算法包括 SGD、带动量的 SGD、RMSProp、带动量的 RMSProp、AdaDelta 和 Adam。

—第 309 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

一个稳健的策略可能是首先评估具有自适应学习率的现代随机梯度下降模型(如 Adam)的表现，并将结果用作基线。然后，如果时间允许，探索是否可以通过精心选择的学习率或更简单的学习率计划来实现改进。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 邮政

*   [利用深度学习神经网络了解学习率对模型表现的影响](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)

### 报纸

*   [深度架构基于梯度训练的实用建议](https://arxiv.org/abs/1206.5533)，2012。

### 书

*   第八章:深度模型训练优化，[深度学习](https://amzn.to/2NJW3gE)，2016。
*   第 6 章:学习率和动量，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2S8qRdI)，1999。
*   [第 5.7 节:梯度下降，模式识别的神经网络](https://amzn.to/2S8qdwt)，1995。

### 文章

*   [随机梯度下降，维基百科](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)。
*   [反向钻取应该使用什么学习率？，神经网络常见问题](ftp://ftp.sas.com/pub/neural/FAQ2.html#A_learn_rate)。

## 摘要

在本教程中，您发现了在训练深度学习神经网络时使用的学习率超参数。

具体来说，您了解到:

*   学习率控制神经网络模型学习问题的速度。
*   如何用合理的默认值配置学习率，诊断行为，并进行敏感性分析。
*   如何通过学习进度计划、动力和自适应学习速度进一步提高绩效。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。