# 集成学习中强学习器与弱学习器

> 原文：<https://machinelearningmastery.com/strong-learners-vs-weak-learners-for-ensemble-learning/>

常用**弱学习器和强学习器**来描述集成学习技术。

例如，我们可能希望从许多弱学习器的预测中构建一个强学习器。事实上，这是集成学习算法提升类的明确目标。

尽管我们可以将模型描述为弱模型或强模型，但这些术语有一个特定的形式定义，并被用作计算学习理论领域一个重要发现的基础。

在本教程中，您将发现弱学习器和强学习器及其与集成学习的关系。

完成本教程后，您将知道:

*   弱学习器是比随机猜测表现稍好的模型。
*   强学习器是具有任意好的准确性的模型。
*   弱学习器和强学习器是来自计算学习理论的工具，并且为集成方法的提升类的发展提供基础。

**用我的新书[Python 集成学习算法](https://machinelearningmastery.com/ensemble-learning-algorithms-with-python/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![Strong Learners vs. Weak Learners for Ensemble Learning](img/b64659c3dea1862a8d5481b348ee10a9.png)

集体学习的强学习器 vs 弱学习器
图片由 [G .拉马尔](https://www.flickr.com/photos/geewhypics/50009350033/)提供，保留部分权利。

## 教程概述

本教程分为三个部分；它们是:

1.  弱学习器
2.  优秀的学习器
3.  弱与强的学习器和提升

## 弱学习器

弱分类器是一种二进制分类模型，其表现略好于随机猜测。

> 弱学习器产生的分类器只比随机分类稍微精确一点。

—第 21 页，[使用集成方法的模式分类](https://amzn.to/2zxc0F7)，2010。

这意味着模型将做出已知具有某种技能的预测，例如使模型的能力变弱，尽管不会弱到模型没有技能，例如表现比随机更差。

*   **弱分类器**:形式上，达到略高于 50%准确率的分类器。

弱分类器有时被称为“*弱学习器*”或“*基础学习器*，这个概念可以被推广到二分类之外。

虽然弱学习器的概念在二进制分类的背景下被很好地理解，但它可以被通俗地理解为任何比简单预测方法表现稍好的模型。从这个意义上说，它是思考分类器能力和集合组成的有用工具。

*   **弱学习器**:通俗来说，表现比幼稚模型略好的模型。

更正式地说，这个概念已经被推广到多类分类，并且有着不同的含义，超过了 50%的准确率。

> 对于二分类，众所周知，对弱学习器的确切要求是要优于随机猜测。[……]请注意，要求基础学习器优于随机猜测对于多类问题来说太弱了，然而要求优于 50%的准确率太严格了。

—第 46 页，[集合方法](https://amzn.to/2XZzrjG)，2012。

它基于形式计算学习理论，提出了一类具有弱可学习性的学习方法，这意味着它们比随机猜测表现得更好。弱可学习性是作为更理想的强可学习性的简化而提出的，其中可学习性实现了任意好的分类准确率。

> 一个较弱的可学习性模型，称为弱可学习性，放弃了学习器能够达到任意高准确率的要求；弱学习算法只需要输出一个比随机猜测表现稍好的假设(通过逆多项式)。

——[弱可学性的力量](https://link.springer.com/article/10.1007/BF00116037)，1990。

这是一个有用的概念，因为它经常被用来描述集成学习算法的贡献成员的能力。例如，有时引导聚合的成员被称为弱学习器，而不是强学习器，至少在术语的口语意义上是这样。

更具体地说，弱学习器是集成学习算法的增强类的基础。

> 术语增强指的是能够将弱学习器转化为强学习器的一系列算法。

—第 23 页，[集合方法](https://amzn.to/2XZzrjG)，2012。

最常用的弱学习模型是决策树。这是因为在施工过程中，树的弱点可以通过树的深度来控制。

对于二进制分类任务，最弱的决策树由单个节点组成，该节点对一个输入变量做出决策并输出二进制预测。这通常被称为“*决策树桩*”

> 这里弱分类器只是一个“树桩”:一个双端节点分类树。

—第 339 页，[统计学习的要素](https://amzn.to/31mzA31)，2016。

它经常被用作弱学习器，以至于决策树桩和弱学习器实际上是同义词。

*   **决策树桩**:单节点对一个输入变量进行运算的决策树，其输出直接进行预测。

尽管如此，其他模型也可以配置为弱学习器。

> 因为提升需要一个弱的学习器，几乎任何带有调整参数的技术都可以变成弱的学习器。事实证明，树木是提升学习能力的绝佳基础…

—第 205 页，[应用预测建模](https://amzn.to/3eJyPVz)，2013 年。

虽然不被正式称为弱学习器，但我们可以考虑以下作为候选弱学习模型:

*   **k-最近邻**，k=1 操作一个或一个子集的输入变量。
*   **多层感知器**，单个节点对一个或一个子集的输入变量进行操作。
*   **朴素贝叶斯**，对单个输入变量进行运算。

既然我们已经熟悉了一个弱学习器，让我们来仔细看看强学习器。

## 优秀的学习器

强分类器是一种二进制分类模型，其表现优于随机猜测。

> 一类概念是可学习的(或强可学习的)，如果存在多项式时间算法，该算法对该类中的所有概念实现了低误差和高置信度。

——[弱可学性的力量](https://link.springer.com/article/10.1007/BF00116037)，1990。

这有时被解释为训练或保持数据集上的完美技能，尽管更可能指的是“*好的*”或“*有用地熟练的*”模型。

*   **强分类器**:形式上，达到任意好准确率的分类器。

我们寻求预测建模问题的强分类器。建模项目的目标是开发一个强大的分类器，以高置信度做出最正确的预测。

同样，虽然强分类器的概念对于二进制分类来说很好理解，但它可以推广到其他问题类型，我们可以不太正式地将这个概念解释为一个表现良好的模型，也许是接近最优的。

*   **强学习器**:通俗来说，就是一个和幼稚模型相比表现非常好的模型。

当我们在数据集上直接拟合机器学习模型时，我们试图开发一个强模型。例如，我们可以将以下算法视为用于拟合通俗意义上的强模型的技术，其中每种方法的超参数都针对目标问题进行了调整:

*   逻辑回归。
*   支持向量机。
*   k-最近邻居。

以及更多在前面章节中列出的或者你可能熟悉的方法。

强学习是我们所追求的，我们可以将他们的能力与弱学习器进行对比，尽管我们也可以从弱学习器中构建强学习器。

## 弱与强的学习器和提升

我们已经确定**弱学习器**的表现略好于随机，并且**强学习器**是好的甚至接近最优的，并且我们寻求的预测建模项目是后者。

在计算学习理论中，特别是 PAC 学习中，弱可学习性和强可学习性的形式分类被定义为两者是否等价的开放问题。

> 这里提出的证据是建设性的；描述了一种显式方法，用于将弱学习算法直接转换成实现任意准确率的算法。该构造使用过滤来修改示例的分布，从而迫使弱学习算法关注分布中较难学习的部分。

——[弱可学性的力量](https://link.springer.com/article/10.1007/BF00116037)，1990。

后来才发现，它们确实是等价的。更重要的是，一个强有力的学习器可以由许多弱的学习器构成，正式定义。这为集成学习方法的提升类提供了基础。

> 主要结果是证明了强可学习性和弱可学习性可能惊人的等价性。

——[弱可学性的力量](https://link.springer.com/article/10.1007/BF00116037)，1990。

尽管这一理论发现已经有了，但在第一批可行的提升方法被开发出来并实现之前，仍然需要数年的时间。

最值得注意的是，自适应增强(AdaBoost)是第一个成功的增强方法，后来导致了大量的方法，最终导致了今天非常成功的技术，如梯度提升和实现，如极限梯度提升(XGBoost)。

> 弱学习器的集合主要在机器学习社区中研究。在这个线索中，研究人员经常研究弱学习器，并试图设计强大的算法来提升从弱到强的表现。这一系列工作导致了著名的集成方法的诞生，如 AdaBoost、Bagging 等。、以及关于为什么以及如何将弱学习器提升到强学习器的理论理解。

—第 16 页，[集合方法](https://amzn.to/2XZzrjG)，2012。

一般来说，提升集成的目标是为预测性学习问题培养大量弱学习器，然后最好地组合它们，以实现强学习器。这是一个很好的目标，因为弱学习器容易准备但不可取，而强学习器很难准备并且非常可取。

> 由于强学习器是可取的但很难获得，而弱学习器在实际实践中很容易获得，这一结果为通过集成方法产生强学习器开辟了一个有希望的方向。

—第 16-17 页，[集合方法](https://amzn.to/2XZzrjG)，2012。

*   **弱学习器**:容易准备，但由于技能低不可取。
*   **学习能力强**:很难准备，但因为技术高而令人向往。

实现这一点的过程是顺序开发弱学习器，并将他们添加到集合中，其中每个弱学习器都以一种方式进行训练，以更加关注问题域中先前模型出错的部分。尽管所有的增强技术都遵循这个具有特定差异和优化的一般过程，但是弱学习器和强学习器的概念对于机器学习和集成学习来说是一个更普遍的有用概念。

例如，我们已经看到如何描述预测模型的目标是开发一个强模型。通常的做法是根据基线或原始模型评估模型的表现，例如二进制分类的随机预测。一个弱的学习器非常像天真的模型，尽管稍微熟练并且使用来自问题领域的最少信息，而不是完全天真的。

考虑到虽然我们在自举聚合(bagging)中没有从技术上构造弱学习器，这意味着成员不是决策树桩，但是我们确实致力于创建更弱的决策树来组成集成。这通常是通过在采样的数据子集上拟合树，而不是修剪树来实现的，这允许它们稍微过度填充训练数据。

> 对于分类，我们可以根据独立弱学习器的共识来理解 bagging 效应

—第 286 页，[统计学习的要素](https://amzn.to/31mzA31)，2016。

这两种变化都是为了寻找相关性较低的树，但具有训练较弱(但可能不是较弱)的模型来组成集合的效果。

*   **装袋**:明确训练较弱(但不算弱)的学习器。

考虑堆叠泛化([堆叠](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/))，它训练一个模型，以最好地组合来自多个不同模型的预测，以适合同一训练数据集。每个贡献的 0 级模型实际上都是一个强大的学习器，元 1 级模型试图通过组合来自强大模型的预测来构建一个更强大的模型。

*   **叠加**:明确组合强学习器的预测。

专家混合(MoE)以类似的方式运行，训练多个强模型(专家)，通过元模型、门控网络和梳理方法将它们组合成有希望更强的模型。

> 专家混合也可以看作是一种分类器选择算法，其中个体分类器被训练成为特征空间某一部分的专家。在这种情况下，个体分类器确实被训练成为专家，因此通常不是弱分类器

—第 16 页，[集成机器学习](https://amzn.to/2C7syo5)，2012。

这突出表明，尽管弱和强的可学习性和学习器是增强的重要理论发现和基础，但是这些分类器的更一般化的思想是设计和选择集成方法的有用工具。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 报纸

*   [弱可学性的力量](https://link.springer.com/article/10.1007/BF00116037)，1990。

### 书

*   [使用集成方法的模式分类](https://amzn.to/2zxc0F7)，2010。
*   [集成方法](https://amzn.to/2XZzrjG)，2012。
*   [集成机器学习](https://amzn.to/2C7syo5)，2012。
*   [数据挖掘中的集成方法](https://amzn.to/3frGM1A)，2010。
*   [统计学习的要素](https://amzn.to/31mzA31)，2016。
*   [应用预测建模](https://amzn.to/3eJyPVz)，2013。

### 文章

*   一起学习，维基百科。
*   [Boosting(机器学习)，维基百科](https://en.wikipedia.org/wiki/Boosting_(machine_learning))。

## 摘要

在本教程中，您发现了弱学习器和强学习器及其与集成学习的关系。

具体来说，您了解到:

*   弱学习器是比随机猜测表现稍好的模型。
*   强学习器是具有任意好的准确性的模型。
*   弱学习器和强学习器是来自计算学习理论的工具，并且为集成方法的提升类的发展提供基础。

**你有什么问题吗？**
在下面的评论中提问，我会尽力回答。