# 机器学习贝叶斯定理的温和介绍

> 原文：<https://machinelearningmastery.com/bayes-theorem-for-machine-learning/>

最后更新于 2019 年 12 月 4 日

贝叶斯定理为计算条件概率提供了一种有原则的方法。

这是一个看似简单的计算，尽管它可以用来轻松计算直觉经常失败的事件的条件概率。

虽然贝叶斯定理在概率领域是一个强大的工具，但在机器学习领域也有着广泛的应用。包括其在概率框架中的使用，该概率框架用于将模型拟合到训练数据集，简称为最大后验概率或最大后验概率，以及用于开发用于分类预测建模问题的模型，例如贝叶斯最优分类器和朴素贝叶斯。

在这篇文章中，你将发现计算条件概率的贝叶斯定理，以及它是如何在机器学习中使用的。

看完这篇文章，你会知道:

*   什么是贝叶斯定理，如何在真实场景中进行计算。
*   贝叶斯定理计算中的术语意味着什么以及它们背后的直觉。
*   贝叶斯定理如何用于分类器、优化和因果模型的例子。

**用我的新书[机器学习概率](https://machinelearningmastery.com/probability-for-machine-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

*   **2019 年 10 月更新**:加入[黑客新闻](https://news.ycombinator.com/item?id=21151032)关于本教程的讨论。
*   **2019 年 10 月更新**:扩展添加更多贝叶斯定理的例子和用法。

![A Gentle Introduction to Bayes Theorem for Machine Learning](img/dd96c0e846fe25afcc400dfc2bbf2ab5.png)

机器学习贝叶斯定理的温和介绍
图片由[马尔科·韦奇](https://www.flickr.com/photos/160866001@N07/46679370454/)提供，版权所有。

## 概观

本教程分为六个部分；它们是:

1.  条件概率的贝叶斯定理
2.  命名定理中的术语
3.  贝叶斯定理计算实例
    1.  诊断测试场景
    2.  手动计算
    3.  Python 代码计算
    4.  二进制分类术语
4.  假设建模的贝叶斯定理
5.  分类的贝叶斯定理
    1.  朴素贝叶斯分类器
    2.  贝叶斯最优分类器
6.  贝叶斯定理在机器学习中的更多应用
    1.  贝叶斯优化
    2.  贝叶斯信念网络

## 条件概率的贝叶斯定理

在我们深入研究贝叶斯定理之前，让我们回顾一下边缘概率、联合概率和条件概率。

回想一下，边缘概率是一个事件的概率，不考虑其他随机变量。如果随机变量是独立的，那么它直接是事件的概率，否则，如果变量依赖于其他变量，那么边缘概率是事件对因变量的所有结果求和的概率，称为求和规则。

*   **边缘概率**:一个事件的概率，与其他随机变量的结果无关，例如 P(A)。

联合概率是两个(或多个)同时发生的事件的概率，通常用来自两个相关随机变量(如 X 和 y)的事件 A 和 B 来描述。联合概率通常概括为结果，如 A 和 B

*   **联合概率**:两个(或多个)同时发生事件的概率，如 P(A 和 B)或 P(A，B)。

条件概率是给定另一个事件发生的情况下，一个事件发生的概率，通常用来自两个相关随机变量(如 X 和 y)的事件 A 和 B 来描述

*   **条件概率**:一个(或多个)事件给定另一个事件发生的概率，例如 P(A 给定 B)或 P(A | B)。

可以使用条件概率来计算联合概率；例如:

*   P(A，B) = P(A | B) * P(B)

这被称为产品规则。重要的是，联合概率是对称的，这意味着:

*   P(A，B) = P(B，A)

条件概率可以使用联合概率来计算；例如:

*   P(A | B) = P(A，B) / P(B)

条件概率不对称；例如:

*   P(A | B)！= P(B | A)

我们现在已经掌握了边缘概率、联合概率和条件概率。如果您想了解这些基础知识的更多背景知识，请参阅教程:

*   [对联合概率、边缘概率和条件概率的温和介绍](https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/)

### 计算条件概率的另一种方法

现在，有另一种计算条件概率的方法。

具体而言，可以使用另一个条件概率来计算一个条件概率；例如:

*   P(A|B) = P(B|A) * P(A) / P(B)

反之亦然；例如:

*   P(B|A) = P(A|B) * P(B) / P(A)

这种计算条件概率的替代方法在联合概率难以计算时(大多数情况下)或者在反向条件概率可用或易于计算时都是有用的。

这种条件概率的交替计算被称为贝叶斯规则或[贝叶斯定理](https://en.wikipedia.org/wiki/Bayes%27_theorem)，以第一个描述它的[牧师托马斯·贝叶斯](https://en.wikipedia.org/wiki/Thomas_Bayes)的名字命名。从语法上来说，称它为贝叶斯定理(带撇号)是正确的，但为了简单起见，省略撇号是很常见的。

*   **贝叶斯定理**:在没有联合概率的情况下，计算条件概率的原则性方法。

通常情况下，我们无法直接获得分母，例如 P(B)。

我们可以用另一种方法来计算它；例如:

*   P(B)= P(B | A) * P(A)+P(B |不是 A)* P(不是 A)

这给出了贝叶斯定理的一个公式，我们可以使用它来替代 P(B)的计算，如下所述:

*   P(A | B)= P(B | A) * P(A)/P(B | A)* P(A)+P(B |不是 A)* P(不是 A)

或者为了清楚起见，在分母周围加上括号:

*   P(A | B)= P(B | A) * P(A)/(P(B | A)* P(A)+P(B |不是 A)* P(不是 A))

**注**:分母就是我们上面给出的展开式。

因此，如果我们有 P(A)，那么我们可以计算 P(不是 A)作为它的补数；例如:

*   P(非 A)= 1–P(A)

另外，如果我们有 P(不是 B |不是 A)，那么我们可以计算 P(B |不是 A)作为它的补数；例如:

*   P(B |不是 A)= 1–P(不是 B |不是 A)

现在我们已经熟悉了贝叶斯定理的计算，让我们更仔细地看看等式中的术语的含义。

## 命名定理中的术语

贝叶斯定理等式中的术语根据使用该等式的上下文来命名。

从这些不同的角度思考计算是有帮助的，有助于将你的问题映射到等式上。

首先，一般情况下，结果 P(A|B)称为**后验概率**，P(A)称为**先验概率**。

*   P(A|B):后验概率。
*   P(A):先验概率。

有时 P(B|A)被称为**可能性**，P(B)被称为**证据**。

*   P(B|A):可能性。
*   P(B):证据。

这使得贝叶斯定理可以重申为:

*   后验=可能性*先验/证据

我们可以用一个烟与火的案例来说明这一点。

**假设有烟，有火的概率是多少？**

其中 P(火)是先验，P(烟|火)是可能性，P(烟)是证据:

*   P(火|烟)= P(烟|火)* P(火)/ P(烟)

你可以想象雨和云的情况。

现在我们已经熟悉了贝叶斯定理和术语的含义，让我们看看一个我们可以计算它的场景。

## 贝叶斯定理计算实例

贝叶斯定理最好用一个真实的例子来理解，用实数来演示计算。

首先，我们将定义一个场景，然后通过手动计算、Python 中的计算以及使用二进制分类领域中您可能熟悉的术语的计算来工作。

1.  诊断测试场景
2.  手动计算
3.  Python 代码计算
4.  二进制分类术语

我们走吧。

### 诊断测试场景

贝叶斯定理的好处的一个优秀且广泛使用的例子是在医学诊断测试的分析中。

**场景**:考虑一个可能患有或不患有癌症(癌症为真或假)的人群，以及一个返回阳性或阴性以检测癌症的医学测试(测试为阳性或阴性)，例如像检测乳腺癌的乳房 x 光检查。

> **问题**:如果随机抽取一个患者进行检测，结果回来是阳性，那么这个患者得癌症的概率是多少？

### 手动计算

医学诊断测试并不完美；他们有错误。

有时病人会得癌症，但检测不会发现。这种检测癌症的能力被称为**灵敏度**，或真实阳性率。

在这种情况下，我们将为测试设计一个灵敏度值。测试是好的，但不是很好，真实阳性率或灵敏度为 85%。也就是说，在所有患有癌症并接受检测的人中，85%的人将从检测中获得阳性结果。

*   p(测试=阳性|癌症=真)= 0.85

根据这些信息，我们的直觉会认为患者患癌症的概率为 85%。

我们对概率的直觉是错误的。

这种解释概率的错误非常普遍，以至于它有自己的名字；它被称为[基本利率谬误](https://en.wikipedia.org/wiki/Base_rate_fallacy)。

它之所以有这个名字，是因为在估计一个事件的概率时的错误是由忽略基本速率引起的。也就是说，它忽略了随机选择的人患癌症的概率，而不管诊断测试的结果如何。

在这种情况下，我们可以假设乳腺癌的概率较低，并使用人为的 5000 人中有一人的基本比率值，或(0.0002) 0.02%。

*   p(癌症=真)= 0.02%。

利用贝叶斯定理，我们可以正确地计算出一个病人得到阳性检测结果的概率。

让我们将场景映射到等式上:

*   P(A|B) = P(B|A) * P(A) / P(B)
*   P(癌症=真|测试=阳性)= P(测试=阳性|癌症=真)* P(癌症=真)/ P(测试=阳性)

我们知道给定患者患有癌症的情况下测试为阳性的概率为 85%，并且我们知道给定患者患有癌症的基础率或先验概率为 0.02%；我们可以将这些值插入:

*   P(癌症=真|检验=阳性)= 0.85 * 0.0002 / P(检验=阳性)

我们不知道 P(测试=阳性)，它不是直接给出的。

相反，我们可以使用以下方法进行估计:

*   P(B)= P(B | A) * P(A)+P(B |不是 A)* P(不是 A)
*   P(测试=阳性)= P(测试=阳性|癌症=真)* P(癌症=真)+ P(测试=阳性|癌症=假)* P(癌症=假)

首先，我们可以计算 P(癌症=假)作为 P(癌症=真)的补码，我们已经知道了

*   P(癌症=假)= 1–P(癌症=真)
*   = 1 – 0.0002
*   = 0.9998

让我们插入我们所拥有的:

我们可以插入我们已知的值如下:

*   P(检验=阳性)= 0.85 * 0.0002 + P(检验=阳性|癌症=假)* 0.9998

鉴于没有癌症，我们仍然不知道阳性检测结果的概率。

这需要额外的信息。

具体来说，我们需要知道这项测试在正确识别没有癌症的人方面有多好。也就是说，当患者没有癌症(癌症=假)时，检测阴性结果(检测=阴性)，称为真阴性率或**特异性**。

我们将使用 95%的人为特异性值。

*   p(测试=阴性|癌症=假)= 0.95

有了这最后一条信息，我们可以计算假阳性或假警报率，作为真阴性率的补充。

*   P(测试=阳性|癌症=假)= 1–P(测试=阴性|癌症=假)
*   = 1 – 0.95
*   = 0.05

我们可以将这个误报率插入到我们的 P(测试=阳性)计算中，如下所示:

*   p(检验=阳性)= 0.85 * 0.0002 + 0.05 * 0.9998
*   p(测试=阳性)= 0.00017 + 0.04999
*   p(测试=正)= 0.05016

太好了，所以测试返回阳性结果的概率，不管这个人有没有癌症，都在 5%左右。

我们现在有足够的信息来计算贝叶斯定理，并估计随机选择的人如果获得阳性检测结果，患癌症的概率。

*   P(癌症=真|测试=阳性)= P(测试=阳性|癌症=真)* P(癌症=真)/ P(测试=阳性)
*   p(癌症=真|检验=阳性)= 0.85 * 0.0002 / 0.05016
*   p(癌症=真|检验=阳性)= 0.00017 / 0.05016
*   p(癌症=真|测试=阳性)= 0.003389154704944

计算表明，如果患者通过这项测试被告知患有癌症，那么他们患癌症的可能性只有 0.33%。

**这是一个可怕的诊断测试！**

实例还表明，条件概率的计算需要*足够的*信息。

例如，如果我们已经有了贝叶斯定理中使用的值，我们可以直接使用它们。

这种情况很少发生，我们通常需要计算所需的位并将其插入，就像我们在本例中所做的那样。在我们的场景中，我们得到了 3 条信息，即**基础率**、**灵敏度**(或真阳性率)和**特异性**(或真阴性率)。

*   **敏感性** : 85%的癌症患者会得到阳性检测结果。
*   **基础率** : 0.02%的人有癌症。
*   **特异性** : 95%没有癌症的人会得到阴性检测结果。

我们没有 P(测试=阳性)，但是我们根据我们已经有的数据进行了计算。

我们可以想象，贝叶斯定理允许我们对给定的场景更加精确。例如，如果我们有更多关于患者(例如，他们的年龄)和领域(例如，年龄范围内的癌症发病率)的信息，反过来我们可以提供更准确的概率估计。

那是很多工作。

让我们看看如何使用几行 Python 代码来计算这个确切的场景。

### Python 代码计算

为了使这个例子具体化，我们可以用 Python 执行计算。

下面的示例在普通 Python 中执行相同的计算(没有库)，允许您使用参数并测试不同的场景。

```py
# calculate the probability of cancer patient and diagnostic test

# calculate P(A|B) given P(A), P(B|A), P(B|not A)
def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):
	# calculate P(not A)
	not_a = 1 - p_a
	# calculate P(B)
	p_b = p_b_given_a * p_a + p_b_given_not_a * not_a
	# calculate P(A|B)
	p_a_given_b = (p_b_given_a * p_a) / p_b
	return p_a_given_b

# P(A)
p_a = 0.0002
# P(B|A)
p_b_given_a = 0.85
# P(B|not A)
p_b_given_not_a = 0.05
# calculate P(A|B)
result = bayes_theorem(p_a, p_b_given_a, p_b_given_not_a)
# summarize
print('P(A|B) = %.3f%%' % (result * 100))
```

运行该示例计算患者患癌症的概率，给出的测试结果为阳性，与我们的手动计算相匹配。

```py
P(A|B) = 0.339%
```

这是一个有用的小脚本，您可能想要适应新的场景。

现在，使用来自二进制分类的术语来描述场景的贝叶斯定理的计算是很常见的。它为思考问题提供了一种非常直观的方式。在下一节中，我们将回顾这些术语，看看它们如何映射到定理中的概率上，以及它们如何与我们的场景相关联。

### 二进制分类术语

从[二进制(二级)分类](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)中的常用术语，即特异性和敏感性的概念来自哪里，来思考癌症测试示例可能会有所帮助。

就我个人而言，我发现这些术语有助于让一切变得有意义。

首先，我们来定义一个[混淆矩阵](https://machinelearningmastery.com/confusion-matrix-machine-learning/):

```py
                    | Positive Class      | Negative Class
Positive Prediction | True Positive (TP)  | False Positive (FP)
Negative Prediction | False Negative (FN) | True Negative (TN)
```

然后，我们可以从混淆矩阵中定义一些比率:

*   真阳性率(TPR) = TP / (TP + FN)
*   假阳性率(FPR) = FP / (FP + TN)
*   真负速率(TNR) =总氮/(总氮+磷)
*   假阴性率(FNR) = FN / (FN + TP)

这些术语被称为利率，但也可以解释为概率。

此外，注意到以下几点可能会有所帮助:

*   TPR + FNR = 1.0，或:
    *   fnr = 1.0–TPR
    *   TPR = 1.0–fnr
*   TNR + FPR = 1.0，或者:
    *   tnr = 1.0–FPR
    *   FPR = 1.0–tnr

回想一下，在上一节中，我们计算了假阳性率，给出了真阴性率的补数，即 FPR = 1.0–TNR。

其中一些费率有特殊的名称，例如:

*   灵敏度= TPR
*   特异性= TNR

我们可以根据贝叶斯定理将这些比率映射到熟悉的术语上:

*   **P(B|A)** :真阳性率(TPR)。
*   **P(非 B |非 A)** :真阴性率(TNR)。
*   **P(B |不是 A)** :假阳性率(FPR)。
*   **P(不是 B|A)** :假阴性率(FNR)。

我们还可以根据熟悉的贝叶斯定理来绘制条件(类别)和治疗(预测)的基本比率:

*   **P(A)** :正类概率(PC)。
*   **P(不是 A)** :负类概率(NC)。
*   **P(B)** :正预测的概率(PP)。
*   **P(不是 B)** :否定预测的概率(NP)。

现在，让我们考虑使用这些术语的贝叶斯定理:

*   P(A|B) = P(B|A) * P(A) / P(B)
*   P(A|B) = (TPR * PC) / PP

我们经常不能计算 P(B)，所以我们使用一个替代方案:

*   P(B)= P(B | A) * P(A)+P(B |不是 A)* P(不是 A)
*   P(B) = TPR * PC + FPR * NC

现在，让我们看看我们的癌症场景和癌症检测测试。

类别或状况将是“T0”癌症，治疗或预测将是“T2”测试。

首先，让我们回顾一下所有的费率:

*   真实阳性率:85%
*   假阳性率(FPR): 5%
*   真实阴性率(TNR): 95%
*   假阴性率:15%

让我们回顾一下我们对基本费率的了解:

*   阳性率:0.02%
*   负类(NC): 99.98%
*   正预测(PP): 5.016%
*   负面预测:94.984%

插一句，我们可以计算出阳性检测结果(阳性预测)的概率，即给定癌症的阳性检测结果的概率(真阳性率)乘以患癌症的基本率(阳性类别)，加上没有癌症的阳性检测结果的概率(假阳性率)加上没有癌症的概率(阴性类别)。

这些术语的计算如下:

*   P(B)= P(B | A) * P(A)+P(B |不是 A)* P(不是 A)
*   P(B) = TPR * PC + FPR * NC
*   P(B) = 85% * 0.02% + 5% * 99.98%
*   P(B) = 5.016%

然后，我们可以计算该场景的贝叶斯定理，即给定阳性检测结果的癌症概率(后验概率)是给定癌症的阳性检测结果概率(真实阳性率)乘以患癌概率(阳性分类率)，除以阳性检测结果概率(阳性预测)。

这些术语的计算如下:

*   P(A|B) = P(B|A) * P(A) / P(B)
*   P(A|B) = TPR * PC / PP
*   P(A|B) = 85% * 0.02% / 5.016%
*   P(A|B) = 0.339%

原来，在这种情况下，我们用贝叶斯定理计算的[后验概率](https://en.wikipedia.org/wiki/Posterior_probability)相当于[准确率](https://en.wikipedia.org/wiki/Precision_and_recall)，也称混淆矩阵的正预测值(PPV):

*   PPV = TP / (TP + FP)

或者，用我们的分类术语来说:

*   P(A|B) = PPV
*   PPV = TPR * PC / PP

**那么我们为什么要不厌其烦地计算后验概率呢？**

因为我们没有癌症和非癌症人群的混淆矩阵，这些人群都经过测试和未测试。相反，我们所拥有的只是一些关于我们的人口和测试的先验和概率。

这突出了我们在实践中可能选择使用计算的时候。

具体来说，当我们对所涉及的事件有信念，但我们不能通过计算现实世界中的例子来进行计算时。

## 假设建模的贝叶斯定理

贝叶斯定理是应用机器学习中的一个有用工具。

它提供了一种思考数据和模型之间关系的方式。

机器学习算法或模型是思考数据中结构化关系的特定方式。这样，模型可以被认为是关于数据中的关系的假设，例如输入( *X* )和输出( *y* 之间的关系。应用机器学习的实践是在给定的数据集上测试和分析不同的假设(模型)。

如果将模型视为假设的想法对您来说是新的，请参阅以下主题的教程:

*   [什么是机器学习中的假设？](https://machinelearningmastery.com/what-is-a-hypothesis-in-machine-learning/)

贝叶斯定理提供了描述数据( *D* )和假设(h)之间关系的概率模型；例如:

*   P(h|D) = P(D|h) * P(h) / P(D)

分解一下，它说给定假设成立或成立的概率给定一些观察到的数据可以计算为给定假设观察到数据的概率乘以假设成立的概率而不考虑数据，除以观察到数据的概率而不考虑假设。

> 贝叶斯定理提供了一种基于假设的先验概率、给定假设的各种观测数据的概率以及观测数据本身来计算假设概率的方法。

—第 156 页，[机器学习](https://amzn.to/2jWd51p)，1997。

在这个框架下，每一个计算都有一个特定的名称；例如:

*   P(h|D):假设的后验概率(我们要计算的东西)。
*   P(h):假设的先验概率。

这为思考和建模机器学习问题提供了一个有用的框架。

如果我们有一些关于假设的先验领域知识，这在先验概率中被捕获。如果我们没有，那么所有的假设可能都有相同的先验概率。

如果观察到数据 P(D)的概率增加，那么给定数据 P(h|D)假设成立的概率就会降低。相反，如果假设 P(h)的概率和观察给定假设的数据的概率增加，则假设保持给定数据 P(h|D)的概率增加。

在应用机器学习中，在数据集上测试不同模型的概念可以被认为是在给定观测数据的情况下，估计每个假设(h1，h2，h3，…在 H 中)为真的概率。

建模中用最大后验概率进行优化或寻找假设，简称为[最大后验](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation)或 MAP。

> 任何这种最大可能的假设被称为最大后验概率假设。我们可以通过使用贝叶斯定理计算每个候选假设的后验概率来确定 MAP 假设。

—第 157 页，[机器学习](https://amzn.to/2jWd51p)，1997。

在这个框架下，数据(D)的概率是恒定的，因为它用于评估每个假设。因此，可以将其从计算中移除，以给出简化的非标准化估计，如下所示:

*   H P(h|D)中的最大 H = P(D | H)* P(H)

如果我们没有任何关于被测试的假设的先验信息，它们可以被分配一个统一的概率，并且这个项也将是一个常数，并且可以从计算中移除，以给出以下内容:

*   H P(h|D)中的最大 H = P(D | H)

也就是说，目标是找到最能解释观测数据的假设。

诸如用于预测数值的线性回归和用于二进制分类的逻辑回归的拟合模型可以在 MAP 概率框架下构建和求解。这为更常见的最大似然估计(MLE)框架提供了一种替代方案。

## 分类的贝叶斯定理

分类是一个预测建模问题，包括给给定的输入数据样本分配一个标签。

分类预测建模的问题可以被框架化为计算给定数据样本的类标签的条件概率，例如:

*   P(类|数据)= (P(数据|类)* P(类))/ P(数据)

其中 P(类|数据)是给定所提供数据的类的概率。

可以对问题中的每个类执行该计算，并且可以选择被分配最大概率的类并将其分配给输入数据。

在实践中，计算用于分类的完全贝叶斯定理是非常具有挑战性的。

如果数据集适合代表更广泛的问题，那么类和数据的先验很容易从训练数据集中估计出来。

基于 P 类(数据|类)的观察的条件概率是不可行的，除非示例的数量非常大，例如大到足以有效地估计所有不同可能值组合的概率分布。这几乎是从来没有的情况，我们不会有足够的覆盖领域。

因此，贝叶斯定理的直接应用也变得棘手，尤其是当变量或特征的数量(n)增加时。

### 朴素贝叶斯分类器

将贝叶斯定理用于条件概率分类模型的解决方案是简化计算。

贝叶斯定理假设每个输入变量都依赖于所有其他变量。这是计算复杂的一个原因。我们可以去掉这个假设，将每个输入变量视为彼此独立的。

这将模型从依赖条件概率模型变为独立条件概率模型，并极大地简化了计算。

这意味着我们分别为每个输入变量计算 P(数据|类)，并将结果相乘，例如:

*   P(类| X1，X2，…，Xn)= P(X1 |类)* P(X2 |类)*…* P(Xn |类)* P(类)/ P(数据)

我们还可以降低观察数据的概率，因为它是所有计算的常数，例如:

*   P(类| X1，X2，…，Xn)= P(X1 |类)* P(X2 |类)*…* P(Xn |类)* P(类)

贝叶斯定理的这种简化是常见的，广泛用于分类预测建模问题，通常被称为[朴素贝叶斯](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/)。

单词“ *naive* ”是法语，通常在“I”上有一个分音符(umlaut)，为了简单起见，这一点通常被省略，“Bayes”被大写，因为它是以托马斯·贝叶斯牧师的名字命名的。

有关如何在 Python 中从头实现朴素贝叶斯的教程，请参见:

*   [如何在 Python 中从零开始开发朴素贝叶斯分类器](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/)
*   [Python 中从零开始的朴素贝叶斯分类器](https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)

### 贝叶斯最优分类器

贝叶斯最优分类器是一种概率模型，在给定训练数据集的情况下，它对一个新的例子做出最相似的预测。

该模型也被称为贝叶斯最优学习器、贝叶斯分类器、贝叶斯最优决策边界或贝叶斯最优判别函数。

*   **贝叶斯分类器**:对新实例进行最可能预测的概率模型。

具体来说，贝叶斯最优分类器回答了这个问题:

> 给定训练数据，新实例最有可能的分类是什么？

这不同于寻求最可能假设(模型)的 MAP 框架。相反，我们有兴趣做一个具体的预测。

下面的等式演示了在给定假设空间( *H* )的情况下，给定训练数据( *D* ，如何计算新实例( *vi* )的条件概率。

*   P(vj | D) =和{h 中的 H} P(vj | hi) * P(hi | D)

其中 *vj* 为待分类的新实例， *H* 为该实例分类的一组假设， *hi* 为给定假设， *P(vj | hi)* 为 *vi* 给定假设 *hi* 的后验概率， *P(hi | D)* 为给定数据*D*的假设 *hi* 的后验概率

选择具有最大概率的结果是贝叶斯最优分类的一个例子。

任何使用这个方程对例子进行分类的模型都是贝叶斯最优分类器，平均而言，没有其他模型能够胜过这种技术。

我们必须接受这一点。这是一件大事。

因为贝叶斯分类器是最优的，所以贝叶斯误差是可以产生的最小可能误差。

*   **贝叶斯误差**:进行预测时可能出现的最小误差。

这是一个理论模型，但它被认为是我们可能希望追求的理想。

朴素贝叶斯分类器是一个分类器的例子，它增加了一些简化的假设，并试图逼近贝叶斯最优分类器。

有关贝叶斯最优分类器的更多信息，请参见教程:

*   [贝叶斯最优分类器的简单介绍](https://machinelearningmastery.com/bayes-optimal-classifier/)

## 贝叶斯定理在机器学习中的更多应用

开发分类器模型可能是贝叶斯定理在机器学习中最常见的应用。

然而，还有许多其他的应用。两个重要的例子是最优化和因果模型。

### 贝叶斯优化

全局优化是一个具有挑战性的问题，寻找一个输入，导致最小或最大的成本给定的目标函数。

通常，目标函数的形式复杂且难以分析，并且通常是非凸的、非线性的、高维的、有噪声的，并且评估起来计算成本高。

贝叶斯优化提供了一种基于贝叶斯定理的有原则的技术，用于指导高效且有效的全局优化问题的搜索。它的工作原理是建立一个目标函数的概率模型，称为替代函数，然后在选择候选样本对真实目标函数进行评估之前，使用获取函数对其进行有效搜索。

贝叶斯优化通常用于应用机器学习，以在验证数据集上调整给定的表现良好的模型的超参数。

有关贝叶斯优化的更多信息，包括如何从零开始实现它，请参见教程:

*   [如何在 Python 中从头实现贝叶斯优化](https://machinelearningmastery.com/what-is-bayesian-optimization/)

### 贝叶斯信念网络

概率模型可以定义变量之间的关系，并用于计算概率。

完全条件模型可能需要大量的数据来涵盖所有可能的情况，而概率在实践中可能难以计算。简化假设，如所有随机变量的条件独立性，可能是有效的，例如在朴素贝叶斯的情况下，尽管这是一个彻底简化的步骤。

另一种方法是开发一个模型，在所有其他情况下保持随机变量之间已知的条件相关性和条件独立性。贝叶斯网络是一种概率图形模型，它明确地捕捉图形模型中已知的有向边的条件依赖。所有缺失的连接定义了模型中的条件独立性。

因此，贝叶斯网络提供了一个有用的工具来可视化一个领域的概率模型，回顾随机变量之间的所有关系，并根据现有证据推断场景的因果概率。

根据定义，网络并不完全是贝叶斯的，尽管假设随机变量(节点)的概率分布和随机变量(边)之间的关系都是主观指定的，该模型可以被认为捕获了关于复杂域的“信念”。

有关贝叶斯信念网络的更多信息，请参见教程:

*   [贝叶斯信念网络的温和介绍](https://machinelearningmastery.com/introduction-to-bayesian-belief-networks/)

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 相关教程

*   [对联合概率、边缘概率和条件概率的温和介绍](https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/)
*   [什么是机器学习中的假设？](https://machinelearningmastery.com/what-is-a-hypothesis-in-machine-learning/)
*   [如何在 Python 中从零开始开发朴素贝叶斯分类器](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/)
*   [Python 中从零开始的朴素贝叶斯分类器](https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)
*   [如何在 Python 中从头实现贝叶斯优化](https://machinelearningmastery.com/what-is-bayesian-optimization/)
*   [贝叶斯信念网络的温和介绍](https://machinelearningmastery.com/introduction-to-bayesian-belief-networks/)

### 书

*   [模式识别与机器学习](https://amzn.to/2JwHE7I)，2006。
*   [机器学习](https://amzn.to/2jWd51p)，1997。
*   [模式分类，第二版](https://amzn.to/2xVBI1n)，2001。
*   [机器学习:概率视角](https://amzn.to/2xKSTCP)，2012。

### 文章

*   [条件概率，维基百科](https://en.wikipedia.org/wiki/Conditional_probability)。
*   [贝叶斯定理，维基百科](https://en.wikipedia.org/wiki/Bayes%27_theorem)。
*   [最大后验估计，维基百科](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation)。
*   [假阳性和假阴性，维基百科](https://en.wikipedia.org/wiki/False_positives_and_false_negatives)。
*   [基础利率谬误，维基百科](https://en.wikipedia.org/wiki/Base_rate_fallacy)。
*   [敏感性和特异性，维基百科](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)。
*   [将困惑带出困惑矩阵](https://lucdemortier.github.io/articles/16/PerformanceMetrics)，2016 年。

## 摘要

在这篇文章中，你发现了计算条件概率的贝叶斯定理，以及它是如何在机器学习中使用的。

具体来说，您了解到:

*   什么是贝叶斯定理，如何在真实场景中进行计算。
*   贝叶斯定理计算中的术语意味着什么以及它们背后的直觉。
*   贝叶斯定理如何用于分类器、优化和因果模型的例子。

**你有什么问题吗？**
在下面的评论中提问，我会尽力回答。