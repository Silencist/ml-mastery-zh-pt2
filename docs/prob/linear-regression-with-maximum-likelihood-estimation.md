# 最大似然估计线性回归的简单介绍

> 原文：<https://machinelearningmastery.com/linear-regression-with-maximum-likelihood-estimation/>

最后更新于 2019 年 11 月 1 日

线性回归是预测数值的经典模型。

线性回归模型的参数可以用最小二乘法或最大似然估计法来估计。最大似然估计是一种概率框架，用于自动寻找最能描述观测数据的概率分布和参数。监督学习可以被框定为条件概率问题，并且最大似然估计可以用于拟合最能概括条件概率分布的模型的参数，即所谓的条件最大似然估计。

一个线性回归模型可以在这个框架下拟合，并且可以被证明是最小二乘法的一个完全相同的解。

在这篇文章中，你将发现具有最大似然估计的线性回归。

看完这篇文章，你会知道:

*   线性回归是预测数值的模型，最大似然估计是估计模型参数的概率框架。
*   线性回归模型的系数可以使用最大似然估计的负对数似然函数来估计。
*   负对数似然函数可用于推导线性回归的最小二乘解。

**用我的新书[机器学习概率](https://machinelearningmastery.com/probability-for-machine-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

*   **2019 年 11 月更新**:修正了 MLE 计算中的错别字，用 x 代替了 y(感谢诺曼)。

![A Gentle Introduction to Maximum Likelihood Estimation for Linear Regression](img/1cee34a7e3710b8c30e91bfc13b56e63.png)

线性回归的最大似然估计简介[弗兰克·米歇尔](https://www.flickr.com/photos/franckmichel/15093248820/)摄，版权所有。

## 概观

本教程分为四个部分；它们是:

1.  线性回归
2.  最大似然估计
3.  最大似然线性回归
4.  最小二乘和最大似然

## 线性回归

线性回归是来自统计学和机器学习的标准建模方法。

> 线性回归是统计学和(有监督的)机器学习的“工作马”。

—第 217 页，[机器学习:概率视角](https://amzn.to/2xKSTCP)，2012。

通常，它是将一个或多个数字输入映射到数字输出的模型。就预测建模而言，它适用于回归类型的问题:也就是说，对实值量的预测。

输入数据用 *n* 例表示为 *X* ，输出用 *y* 表示，每个输入一个输出。给定输入的模型预测表示为 *yhat* 。

*   yhat =型号(X)

该模型是根据称为系数(β)的参数定义的，其中每个输入有一个系数，还有一个提供截距或偏差的附加系数。

例如，具有 m 个变量 *x1，x2，…，xm* 的输入 *X* 的问题将具有系数*β1，β2，…，βm*和*β0*。给定的输入被预测为示例输入和系数的加权和。

*   yhat = beta 0+beta 1 * x1+beta 2 * x2++betam * XM

该模型也可以使用线性代数来描述，其中系数向量(*β*)和输入数据矩阵( *X* )以及输出向量( *y* )。

*   y = X *贝塔

这些例子来自更广泛的人群，因此，样本是不完整的。此外，预计在观测中会有测量误差或统计噪声。

模型的参数(*β*)必须根据从域中提取的观测样本来估计。

鉴于对模型的研究已有 100 多年，估计参数的方法有很多；然而，有两个框架是最常见的。它们是:

*   最小二乘优化。
*   最大似然估计。

两者都是涉及搜索不同模型参数的优化过程。

最小二乘优化是一种通过寻找一组参数来估计模型参数的方法，该组参数导致模型预测值( *yhat* )和实际输出值( *y* )之间的最小平方误差，即所谓的均方误差。

最大似然估计是一个频繁概率框架，它为模型寻找一组参数，使似然函数最大化。我们将仔细研究第二种方法。

在这两种框架下，可以使用不同的优化算法，例如像 [BFGS 算法](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)(或变体)这样的局部搜索方法，以及像[随机梯度下降](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)这样的一般优化方法。线性回归模型的特殊之处在于还存在解析解，这意味着系数可以直接使用线性代数来计算，这个主题不在本教程的讨论范围内。

有关更多信息，请参见:

*   [如何用线性代数求解线性回归](https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/)

## 最大似然估计

[最大似然估计](https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/)，简称 MLE，是估计模型参数的概率框架。

在最大似然估计中，我们希望在给定特定概率分布及其参数(*θ*)的情况下，最大化观察数据的条件概率( *X* )，正式表述为:

*   p(X；θ)

其中 *X* 实际上是从问题域 1 到 *n* 的所有观测值的联合概率分布。

*   P(x1，x2，x3，…，xn；θ)

这个结果条件概率被称为观察给定模型参数的数据的可能性，并使用符号 *L()* 来表示[可能性函数](https://en.wikipedia.org/wiki/Likelihood_function)。例如:

*   l(X；θ)

给定分布参数，联合概率分布可以重申为观察每个例子的条件概率的乘积。将许多小概率相乘可能不稳定；因此，通常将这个问题重述为自然对数条件概率的总和。

*   对数和(Xi；θ))

鉴于对数在似然函数中的常见用途，它被称为对数似然函数。在优化问题中，倾向于最小化成本函数而不是最大化成本函数也是常见的。因此，使用对数似然函数的负值，通常称为负对数似然函数。

*   最小化对数和(Xi；θ))

最大似然估计框架可以用作估计回归和分类预测建模的许多不同机器学习模型的参数的基础。这包括线性回归模型。

## 最大似然线性回归

我们可以将拟合机器学习模型的问题框架为概率密度估计问题。

具体来说，模型和模型参数的选择被称为建模假设 *h* ，问题涉及到寻找最能解释数据 *X* 的 *h* 。因此，我们可以找到最大化似然函数的建模假设。

*   最大化对数和(Xi；h))

监督学习可以被构造成一个条件概率问题，在给定输入的情况下预测输出的概率:

*   P(y | X)

因此，我们可以为监督机器学习定义条件最大似然估计如下:

*   最大化对数和(Xi；h))

现在我们可以用我们的线性回归模型来代替 *h* 。

我们可以做一些合理的假设，比如数据集中的观测值是独立的，从相同的概率分布(i.i.d .)中得出，目标变量( *y* )具有高斯分布的统计噪声，均值为零，所有例子的方差相同。

通过这些假设，我们可以将给定 *X* 的估计 *y* 的问题框定为根据给定 *X* 的高斯概率分布来估计 *y* 的平均值。

高斯函数的解析形式如下:

*   f(x)=(1/sqrt(2 * pi * sigma ^ 2))* exp(-1/(2 * sigma ^ 2)*(y-mu)^ 2)

其中*μ*是分布的平均值， *sigma^2* 是单位平方的方差。

我们可以使用这个函数作为我们的似然函数，其中*μ*被定义为来自具有给定系数集(*β*)的模型的预测，而*σ*是固定常数。

首先，我们可以将问题表述为数据集中每个示例的概率乘积的最大化:

*   最大化产品 I 到 n(1/sqrt(2 * pi * sigma^2))* exp(-1/(2 * sigma^2)*(yi–h(beta))^2 Xi))

其中 *xi* 是给定的例子，*贝塔*是指线性回归模型的系数。我们可以将其转换为对数似然模型，如下所示:

*   最大化 I 与 n 之和 log(1/sqrt(2 * pi * sigma^2))–(1/(2 * sigma^2)*(yi–h(beta))^2 Xi))

计算可以进一步简化，但我们现在就到此为止。

有趣的是，预测是一个分布的平均值。它表明，我们可以非常合理地在预测中添加一个界限，根据分布的标准偏差给出一个预测区间，这确实是一种常见的做法。

尽管模型在预测中假设高斯分布(即高斯噪声函数或误差函数)，但对模型的输入不存在这种期望( *X* )。

> [模型]仅考虑训练示例的目标值中的噪声，而不考虑描述实例本身的属性中的噪声。

—第 167 页，[机器学习](https://amzn.to/2jWd51p)，1997。

我们可以应用搜索过程来最大化这个对数似然函数，或者通过在开头添加负号来反转它，并最小化负对数似然函数(更常见)。

这为给定数据集的线性回归模型提供了解决方案。

这个框架也比较通用，可以用于曲线拟合，为拟合人工神经网络等其他回归模型提供基础。

## 最小二乘和最大似然

有趣的是，上一节给出的线性回归的最大似然解与最小二乘解相同。

推导之后，最小化最小二乘方程以使线性回归适合数据集，如下所示:

*   最小化 I 与 n 之和(yi–h )( beta))^2 Xi

其中，我们对每个目标变量( *yi* )和相关输入的模型预测 *h(xi，贝塔)*之间的平方误差进行求和。这通常被称为[普通最小二乘](https://en.wikipedia.org/wiki/Ordinary_least_squares)。更一般地说，如果该值是通过数据集中的示例数(平均)而不是求和来归一化的，那么该量被称为[均方误差](https://en.wikipedia.org/wiki/Mean_squared_error)。

*   MSE = 1/n * I 与 n 之和(yi–yhat)^2

从上一节中定义的似然函数开始，我们可以展示如何移除常数元素，以给出与求解线性回归的最小二乘法相同的方程。

注:此推导基于汤姆·米切尔的[机器学习](https://amzn.to/2jWd51p)第 6 章给出的例子。

*   最大化 I 与 n 之和 log(1/sqrt(2 * pi * sigma^2))–(1/(2 * sigma^2)*(yi–h(beta))^2 Xi))

移除常数的关键是关注评估不同模型时不变的内容，例如评估 *h(xi，贝塔)*时。

计算的第一项独立于模型，可以去掉，给出:

*   最大化 I 与 n 之和–(1/(2 * sigma^2)*(yi–h(Xi，Beta))^2)

然后，我们可以去掉负号来最小化正数量，而不是最大化负数量:

*   最小化 I 与 n 之和(1/(2 * sigma^2)*(yi–h(Xi，Beta))^2)

最后，我们可以丢弃剩余的第一项，它也独立于模型给出:

*   最小化 I 与 n 之和(yi–h )( beta))^2 Xi

我们可以看到这和最小二乘解是一样的。

事实上，在合理的假设下，最小化目标变量和模型输出之间的平方误差的算法也执行最大似然估计。

> ……在某些假设下，任何最小化输出假设预测和训练数据之间的平方误差的学习算法都将输出最大似然假设。

—第 164 页，[机器学习](https://amzn.to/2jWd51p)，1997。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 教程

*   [如何用线性代数求解线性回归](https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/)
*   [如何在 Python 中从零开始实现线性回归](https://machinelearningmastery.com/implement-linear-regression-stochastic-gradient-descent-scratch-python/)
*   [如何用 Python 从零开始实现简单线性回归](https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/)
*   [使用梯度下降进行机器学习的线性回归教程](https://machinelearningmastery.com/linear-regression-tutorial-using-gradient-descent-for-machine-learning/)
*   [机器学习简单线性回归教程](https://machinelearningmastery.com/simple-linear-regression-tutorial-for-machine-learning/)
*   [机器学习的线性回归](https://machinelearningmastery.com/linear-regression-for-machine-learning/)

### 书

*   第 15.1 节最小二乘作为最大似然估计量，[C 中的数值方法:科学计算的艺术](https://amzn.to/2YX0Obn)，第二版，1992。
*   第五章机器学习基础，[深度学习](https://amzn.to/2lnc3vL)，2016。
*   第 2.6.3 节函数逼近，[统计学习的要素](https://amzn.to/2YVqu8s)，2016。
*   第 6.4 节最大似然和最小二乘误差假设，[机器学习](https://amzn.to/2jWd51p)，1997。
*   第 3.1.1 节最大似然和最小二乘法，[模式识别和机器学习](https://amzn.to/2JwHE7I)，2006。
*   第 7.3 节最大似然估计(最小二乘法)，[机器学习:概率观点](https://amzn.to/2xKSTCP)，2012。

### 文章

*   [最大似然估计，维基百科](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)。
*   [似然函数，维基百科](https://en.wikipedia.org/wiki/Likelihood_function)。
*   [线性回归，维基百科](https://en.wikipedia.org/wiki/Linear_regression)。

## 摘要

在这篇文章中，你发现了具有最大似然估计的线性回归。

具体来说，您了解到:

*   线性回归是预测数值的模型，最大似然估计是估计模型参数的概率框架。
*   线性回归模型的系数可以使用最大似然估计的负对数似然函数来估计。
*   负对数似然函数可用于推导线性回归的最小二乘解。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。