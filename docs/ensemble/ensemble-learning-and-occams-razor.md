# 集成学习算法复杂度和奥卡姆剃刀

> 原文：<https://machinelearningmastery.com/ensemble-learning-and-occams-razor/>

最后更新于 2021 年 4 月 27 日

**奥卡姆剃刀**提出，在机器学习中，我们应该更喜欢系数更少的更简单的模型，而不是像集成这样的复杂模型。

从表面上看，剃刀是一种启发，它表明更复杂的假设会做出更多的假设，而这些假设又会使假设过于狭窄，无法很好地概括。在机器学习中，它表明像**集成这样的复杂模型会过度训练训练数据集**，并且在新数据上表现不佳。

实际上，在预测技能是最重要的考虑因素的项目中，集合几乎是普遍选择的模型类型。此外，经验结果显示，随着集成学习模型的复杂度递增，泛化误差**持续减小。这些发现与奥卡姆剃刀原则不一致。**

在本教程中，您将发现如何将奥卡姆剃刀与集成机器学习相协调。

完成本教程后，您将知道:

*   奥卡姆剃刀是一种启发式方法，建议选择更简单的机器学习模型，因为它们有望更好地推广。
*   启发式可以分为两个剃刀，一个是真的，仍然是一个有用的工具，另一个是假的，应该放弃。
*   像 boosting 这样的集成学习算法提供了第二个 razor 如何失败的具体案例，增加的复杂性可以降低泛化误差。

**用我的新书[Python 集成学习算法](https://machinelearningmastery.com/ensemble-learning-algorithms-with-python/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![Ensemble Learning Algorithm Complexity and Occam's Razor](img/d1e0b2a6ad1b0df3dcd164bea5163a04.png)

集成学习算法复杂性和奥卡姆剃刀
图片由[迪伦 _ 奥东内尔](https://www.flickr.com/photos/65141172@N00/20348027351/)提供，保留部分权利。

## 教程概述

本教程分为三个部分；它们是:

1.  奥卡姆模型选择剃刀
2.  奥卡姆机器学习的两把剃刀
3.  奥卡姆剃刀与集成学习

## 奥卡姆模型选择剃刀

[模型选择](https://machinelearningmastery.com/a-gentle-introduction-to-model-selection-for-machine-learning/)是从预测建模项目的许多候选机器学习模型中选择一个的过程。

根据模型的预期表现选择模型通常很简单，例如选择具有最高准确率或最低预测误差的模型。

另一个重要的考虑是选择更简单的模型而不是复杂的模型。

更简单的模型通常被定义为做出更少假设或具有更少元素的模型，最常见的特征是更少的系数(例如规则、层、权重等)。).选择更简单型号的理由与奥卡姆剃刀有关。

> 想法是最好的科学理论是解释所有事实的最小理论。

—第 197 页，[数据挖掘:实用机器学习工具与技术](https://amzn.to/2tlRP9V)，2016。

[奥卡姆剃刀](https://en.wikipedia.org/wiki/Occam%27s_razor)是一种解决问题的方法，通常被用来表示如果其他条件都相同，我们应该更喜欢更简单的解决方案。

*   **奥卡姆剃刀**:如果其他都相等，那么最简单的解就是正确的。

它是以奥卡姆的威廉命名的，旨在对抗更复杂的哲学，而预测力却没有相应的增加。

> 奥卡姆著名剃刀的威廉说“努尔哈赤是必然的多元论”，大致翻译过来，意思是“实体不应该在必然之外增加”。

——[奥卡姆的《两把剃刀:锋利的和钝的》](https://www.aaai.org/Library/KDD/1998/kdd98-006.php)，1998 年。

这不是一条规则，更多的是一种解决问题的启发式方法，在科学中通常被引用来偏爱更简单的[假设](https://machinelearningmastery.com/what-is-a-hypothesis-in-machine-learning/)，它比更复杂的假设做得更多。

> 科学中有一个长期的传统，即在其他条件相同的情况下，简单的理论比复杂的理论更可取。这就是以中世纪哲学家奥卡姆(或奥卡姆)的威廉命名的奥卡姆剃刀。

—第 197 页，[数据挖掘:实用机器学习工具与技术](https://amzn.to/2tlRP9V)，2016。

带有更多假设的复杂假设的问题在于它们可能过于具体。

它们可能包括手头上或容易获得的具体案例的细节，反过来可能不会推广到新案例。也就是说，假设越多，其应用范围就越窄。相反，更少的假设意味着更一般的假设，对更多的情况有更大的预测能力。

*   **简单假设**:假设少，反过来适用性广。
*   **复杂假设**:假设多，反过来适用性窄。

这对机器学习有影响，因为我们特别试图从具体的观察中归纳出新的看不见的情况，称为[归纳推理](https://en.wikipedia.org/wiki/Inductive_reasoning)。

如果奥卡姆剃刀认为更复杂的模型不能很好地概括，那么在应用机器学习中，它建议我们应该选择更简单的模型，因为它们对新数据的预测误差更低。

**如果这是真的，那么我们如何证明使用集成机器学习算法是正确的？**

根据定义，集成机器学习算法比单个机器学习模型更复杂，因为它们由许多单独的机器学习模型组成。

奥卡姆剃刀认为，集成学习算法的复杂性增加意味着它们不会像更简单的模型一样适用于同一数据集。

然而，当对新数据的预测技能是最重要的关注点时，例如机器学习竞赛，集成机器学习算法是主要的解决方案。集成已经被研究了很长时间，并且在一次又一次的研究中被证明不会过度填充训练数据集。

> 经验上观察到，某些集成技术通常不会过拟合模型，即使集成包含数千个分类器。

—第 40 页，[使用集成方法的模式分类](https://amzn.to/2zxc0F7)，2010。

如何调和这种不一致？

## 奥卡姆机器学习的两把剃刀

长期以来，理论上更简单的模型更好地概括与实践中更复杂的模型如集成更好地概括之间的冲突，作为一个不方便的经验发现，大多被忽视。

20 世纪 90 年代末，佩德罗·多明戈斯(Pedro Domingos)专门研究了这个问题，并发表在 1996 年 T2 获奖论文《奥卡姆剃刀在知识发现中的作用》(T7)中

在工作中，多明戈斯将这个问题定义为奥卡姆剃刀在应用机器学习中的两个具体的普遍断言的含义，他称之为“奥卡姆剃刀在机器学习中的两个含义”，它们是(摘自论文):

*   **第一剃刀**:给定两个具有相同泛化误差的模型，应该首选更简单的模型，因为简单本身就是可取的。
*   **第二剃刀**:给定两个训练集误差相同的模型，应该优先选择更简单的模型，因为它可能具有更低的泛化误差。

多明戈斯随后从机器学习的理论和实证研究中列举了支持和反对每一种剃刀的大量例子。

**第一剃刀**建议，如果两个模型在训练中没有看到的数据上有相同的预期表现，我们应该更喜欢更简单的模型。多明戈斯强调，这种剃刀适用于机器学习项目，并提供了很好的启发。

第二个剃刀**建议，如果两个模型在训练数据集上具有相同的表现，那么应该选择更简单的模型，因为当用于对新数据进行预测时，它有望更好地推广。**

 **从表面上看，这似乎是明智的。

这是在机器学习项目中不采用集成算法背后的论点，因为与其他模型相比，集成算法非常复杂，预计不会一概而论。

原来**这把剃刀无法得到机器学习文献证据的支持**。

> 所有这些证据都指向这样一个结论:第二把剃刀不仅在总体上不正确；在 KDD 申请的域名类型中，这也是典型的错误。

——[奥卡姆的《两把剃刀:锋利的和钝的》](https://www.aaai.org/Library/KDD/1998/kdd98-006.php)，1998 年。

## 奥卡姆剃刀与集成学习

一旦你仔细考虑了一会儿，这个发现听起来就很直观。

例如，在实践中，我们不会仅根据机器学习模型在训练数据集上的表现来选择机器学习模型。我们凭直觉，或者可能在大量的经验之后，默认训练集上的表现估计是对保持数据集上的表现的糟糕估计。

我们有这样的期望，因为模型可以过度训练数据集。

然而，不太直观的是，过拟合训练数据集可以在保持测试集上获得更好的表现。这在系统研究的实践中已经多次观察到。

一种常见的情况是，在模型的每次学习迭代中，在训练数据集和保持测试数据集上绘制模型的表现，例如支持增量学习的模型的训练时期或迭代。

如果训练数据集上的学习被设置为持续大量的训练迭代和观察到的曲线，则通常可以看到训练数据集上的表现将下降到零误差。这是意料之中的，因为我们可能会认为，在给定足够的资源和时间进行训练的情况下，模型会过度训练训练数据集。然而，测试集上的表现将继续提高，即使训练集上的表现保持固定在零误差。

> ……偶尔，在训练误差达到零后很长时间，泛化误差会继续改善。

—第 40 页，[数据挖掘中的集成方法](https://amzn.to/2QCEC5q)，2010。

这种行为可以通过像 boosting 和 bagging 这样的集成学习算法来观察，在这些算法中，随着额外的模型成员被添加到集成中，保持数据集的表现将继续提高。

> 一个非常令人惊讶的发现是，在组合分类器对训练数据的分类误差已经下降到零很长时间之后，执行更多的提升迭代可以减少新数据上的误差。

—第 489 页，[数据挖掘:实用机器学习工具与技术](https://amzn.to/2tlRP9V)，2016。

也就是说，模型的复杂度是递增的，这系统地减少了对看不见的数据的误差，例如泛化误差。额外的训练不能提高训练数据集的表现；没有可能的改进。

> 在不减少训练误差的情况下执行更多的提升迭代并不能更好地解释训练数据，而且它肯定会增加组合分类器的复杂性。

—第 490 页，[数据挖掘:实用机器学习工具与技术](https://amzn.to/2tlRP9V)，2016。

这一发现直接与第二把剃刀相矛盾，并支持多明戈斯关于放弃第二把剃刀的论点。

> 第一个基本上没有争议，而第二个，从字面上看，是错误的。

——[奥卡姆的《两把剃刀:锋利的和钝的》](https://www.aaai.org/Library/KDD/1998/kdd98-006.php)，1998 年。

这个问题已经被研究过，并且通常可以通过集成算法来解释，集成算法学习对训练数据集的预测更有信心，而训练数据集会延续到保持数据。

> 这个矛盾可以通过考虑分类器对其预测的信心来解决。

—第 490 页，[数据挖掘:实用机器学习工具与技术](https://amzn.to/2tlRP9V)，2016。

第一把剃刀仍然是应用机器学习中的一个重要启发。

这把剃刀的关键方面是“T0”的谓词，所有其他的都是相等的也就是说，如果比较两个模型，必须使用它们在保持数据集上的泛化误差进行比较，或者使用 [k 倍交叉验证](https://machinelearningmastery.com/k-fold-cross-validation/)进行估计。如果在这些情况下，它们的表现相同，那么剃须刀就可以生效，我们可以选择更简单的解决方案。

这不是选择车型的唯一方法。

我们可能会选择更简单的模型，因为它更容易解释，如果模型可解释性是比预测技能更重要的项目需求，这仍然有效。

当模型参数的数量被认为是复杂性的度量时，集成学习算法无疑是一种更复杂的模型。因此，机器学习中的一个开放问题涉及到复杂性的替代度量。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 相关教程

*   [机器学习模型选择的温和介绍](https://machinelearningmastery.com/a-gentle-introduction-to-model-selection-for-machine-learning/)

### 报纸

*   [奥卡姆的《两把剃刀:锋利的和钝的》](https://www.aaai.org/Library/KDD/1998/kdd98-006.php)，1998。
*   [奥卡姆剃刀在知识发现中的作用](https://link.springer.com/article/10.1023/A:1009868929893)，1999。

### 书

*   [数据挖掘中的集成方法:通过组合预测来提高准确性](https://amzn.to/2QCEC5q)，2010。
*   [使用集成方法的模式分类](https://amzn.to/2zxc0F7)，2010。
*   [数据挖掘:实用机器学习工具与技术](https://amzn.to/2tlRP9V)，2016。

### 文章

*   [Occam's razor，维基百科](https://en.wikipedia.org/wiki/Occam%27s_razor)。
*   奥卡姆的威廉，维基百科。

## 摘要

在本教程中，您发现了如何将奥卡姆剃刀与集成机器学习相协调。

具体来说，您了解到:

*   奥卡姆剃刀是一种启发式方法，建议选择更简单的机器学习模型，因为它们有望更好地推广。
*   启发式可以分为两种剃刀，一种是真的，仍然是有用的工具，另一种是假的，应该放弃。
*   像 boosting 这样的集成学习算法提供了第二个 razor 如何失败的具体案例，增加的复杂性可以降低泛化误差。

**你有什么问题吗？**
在下面的评论中提问，我会尽力回答。**