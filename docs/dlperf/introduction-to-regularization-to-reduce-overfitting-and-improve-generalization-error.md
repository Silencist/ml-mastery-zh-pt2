# 如何避免深度学习神经网络中的过拟合

> 原文：<https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/>

最后更新于 2019 年 8 月 6 日

训练一个能够很好地推广到新数据的深度神经网络是一个具有挑战性的问题。

容量太小的模型无法学习问题，而容量太大的模型可以学习得太好，过度训练训练数据集。这两种情况都导致模型不能很好地推广。

减少泛化误差的现代方法是使用更大的模型，这可能需要在训练期间使用正则化来保持模型的权重小。这些技术不仅可以减少过拟合，还可以加快模型的优化速度，提高整体表现。

在这篇文章中，你将发现训练神经网络时过拟合的问题，以及如何用正则化方法解决这个问题。

看完这篇文章，你会知道:

*   通过增加网络容量可以很容易地解决适配不足的问题，但是过度适配需要使用专门的技术。
*   像权重衰减这样的正则化方法为控制大型神经网络模型的过拟合提供了一种简单的方法。
*   一个现代的正则化建议是使用提前停止和权重约束。

**用我的新书[更好的深度学习](https://machinelearningmastery.com/better-deep-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![A Gentle Introduction to Regularization to Reduce Overfitting and Improve Generalization Error](img/4916f30b78bf258df2ec7c23779fb86a.png)

减少过拟合和改善泛化误差的正则化的温和介绍[jaimelie . beale](https://www.flickr.com/photos/jaimilee/38797061420/)摄，保留部分权利。

## 概观

本教程分为四个部分；它们是:

1.  模型泛化和过拟合问题
2.  通过约束模型复杂性减少过拟合
3.  正则化方法
4.  正规化建议

## 模型泛化和过拟合问题

神经网络的目标是拥有一个最终模型，该模型在我们用来训练它的数据(例如训练数据集)和模型将用来进行预测的新数据上表现良好。

> 机器学习的核心挑战是，我们必须在新的、以前看不到的输入上表现良好——而不仅仅是那些训练我们模型的输入。在以前没有观察到的输入上表现良好的能力被称为概括。

—第 110 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

我们要求模型从已知的例子中学习，并在将来从那些已知的例子中归纳出新的例子。我们使用类似训练/测试分割或 k-fold 交叉验证的方法来评估模型推广到新数据的能力。

学习和推广新的案例是很难的。

学习太少，模型在训练数据集和新数据上将表现不佳。这个模型会解决这个问题。过多的学习和模型将在训练数据集上表现良好，而在新数据上表现不佳，模型将过拟合问题。在这两种情况下，模型都没有一般化。

*   **下钻模型**。未能充分了解问题的模型，在训练数据集上表现不佳，在保持样本上表现不佳。
*   **过冲模型**。学习训练数据集太好的模型，在训练数据集上表现良好，但在等待样本上表现不佳。
*   **良好拟合模型**。适合学习训练数据集并很好地推广到旧数据集的模型。

可以在偏差-方差权衡的背景下考虑模型拟合。

欠信息模型具有高偏差和低方差。不管训练数据中的具体样本如何，它都无法学习问题。过拟合模型具有低偏差和高方差。该模型学习训练数据太好了，并且随着训练数据集中新的未见过的例子或者甚至统计噪声被添加到例子中，表现变化很大。

> 为了很好地概括，系统需要足够强大以逼近目标函数。如果过于简单，甚至无法拟合训练数据，那么对新数据的泛化能力也很可能很差。[……]然而，一个过于复杂的系统可能能够以许多不同的方式近似数据，从而产生类似的误差，并且不太可能选择最能概括的那一个…

—第 241 页，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2Dxo4XU)，1999。

我们可以通过增加模型的容量来解决装配不足的问题。能力是指模型适应各种功能的能力；更大的容量意味着一个模型可以适合更多类型的函数，用于将输入映射到输出。增加模型的容量很容易通过改变模型的结构来实现，例如添加更多的层和/或向层添加更多的节点。

因为欠比特模型很容易解决，所以过比特模型更常见。

通过在训练数据集和保持验证数据集上评估模型，在训练期间监控模型的表现，可以很容易地诊断过度训练模型。在训练期间绘制模型表现的线图，称为学习曲线，将显示熟悉的模式。

例如，模型在训练和验证数据集上的损失(我们寻求最小化)的线图将显示训练数据集的下降和可能平稳的线，以及验证数据集的先下降，然后在某个点开始再次上升的线。

> 随着训练的进行，当网络适应训练数据的特性时，泛化误差可以减小到最小，然后再次增加。

—第 250 页，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2Dxo4XU)，1999。

学习曲线图讲述了模型学习问题的故事，直到它开始过拟合，并且它推广到看不见的验证数据集的能力开始变差。

## 通过约束模型复杂性减少过拟合

有两种方法可以接近 overfit 模型:

1.  通过在更多的例子上训练网络来减少过拟合。
2.  通过改变网络的复杂性来减少过拟合。

深度神经网络的一个好处是，当它们被输入越来越大的数据集时，它们的表现会不断提高。一个拥有几乎无限数量例子的模型最终会在网络的学习能力方面趋于平稳。

模型可以过度训练训练数据集，因为它有足够的能力这样做。降低模型的容量会降低模型过拟合训练数据集的可能性，达到不再过拟合的程度。

一个神经网络模型的容量，它的复杂性，是由它的节点和层的结构和它的权重的参数定义的。因此，我们可以通过以下两种方式之一来降低神经网络的复杂性，以减少过拟合:

1.  通过改变网络结构(权重数)来改变网络复杂性。
2.  通过更改网络参数(权重值)来更改网络复杂性。

> 在神经网络的情况下，复杂性可以通过改变网络中自适应参数的数量来改变。这叫做结构稳定。[……]控制模型复杂性的第二种主要方法是通过使用正则化，这涉及到在误差函数中添加惩罚项。

—第 332 页，[用于模式识别的神经网络](https://amzn.to/2I9gNMP)，1995。

例如，可以例如通过网格搜索来调整该结构，直到找到合适数量的节点和/或层来减少或消除该问题的过拟合。或者，可以通过移除节点对模型进行过拟合和修剪，直到它在验证数据集上获得合适的表现。

更常见的是通过确保模型的参数(权重)保持较小来约束模型的复杂性。小的参数意味着不太复杂，反过来，更稳定的模型，对输入数据的统计波动不太敏感。

> 较大的权重往往会导致[激活]功能的急剧转变，因此输入的小变化会导致输出的大变化。

—第 269 页，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2Dxo4XU)，1999。

更常见的是关注约束神经网络中权重大小的方法，因为单个网络结构可以被定义为约束不足，例如具有比问题所需大得多的容量，并且在训练期间可以使用正则化来确保模型不会过拟合。在这种情况下，表现甚至可以更好，因为额外的能力可以集中在更好地学习问题中的可推广概念上。

寻求通过保持网络权重小来减少过拟合(减少泛化误差)的技术被称为正则化方法。更具体地说，正则化指的是一类添加额外信息以将不适定问题转化为更稳定的适定问题的方法。

> 如果给定信息中的小变化导致解决方案中的大变化，则称问题是不适定的。这种数据的不稳定性使得解决方案不可靠，因为微小的测量误差或参数的不确定性可能会被极大地放大，并导致截然不同的响应。[……]正则化背后的思想是使用补充信息以稳定的形式重述不适定问题。

—第 266 页，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2Dxo4XU)，1999。

正则化方法被如此广泛地用于减少过拟合，以至于术语“*正则化*”可以用于改善神经网络模型的泛化误差的任何方法。

> 正则化是我们对学习算法所做的任何修改，目的是减少其泛化误差，而不是训练误差。正则化是机器学习领域关注的焦点之一，其重要性仅次于优化。

—第 120 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

## 神经网络的正则化方法

最简单也可能是最常见的正则化方法是根据模型中权重的大小，在损失函数中加入一个惩罚。

1.  [**【权重正则化(权重衰减)**](https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/) :在训练过程中根据权重的大小对模型进行惩罚。

这将鼓励模型将输入映射到训练数据集的输出，使得模型的权重保持较小。这种方法被称为权重正则化或权重衰减，并且几十年来被证明对于更简单的线性模型和神经网络都非常有效。

> 收集更多数据的一个简单的替代方法是通过调整权重衰减系数等超参数来减小模型的大小或改进正则化…

—第 427 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

下面列出了五种最常见的附加正则化方法。

1.  [**【活动规则化】**](https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras/) :在训练过程中根据激活的大小惩罚模型。
2.  [**【权重约束】**](https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/) :将权重的大小约束在一个范围内或低于一个限制。
3.  [](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)**:在训练过程中可能会移除输入。**
***   [**噪声**](https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/) :训练时给输入增加统计噪声。*   [**【提前停止】**](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/) :监控验证集上的模型表现，当表现下降时停止训练。**

 **这些方法中的大部分已经被证明(或证明)近似了在损失函数中加入惩罚的效果。

每种方法处理问题的方式都不同，在泛化表现、可配置性和/或计算复杂性方面都有优势。

## 正规化建议

本节概述了一些使用正则化方法进行深度学习神经网络的建议。

您应该始终考虑使用正则化，除非您有非常大的数据集，例如大数据规模。

> 除非你的训练集包含数千万或更多的例子，否则你应该从一开始就包含一些温和的正则化形式。

—第 426 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

一个很好的总体建议是设计一个约束不足的神经网络结构，并使用正则化来减少过拟合的可能性。

> ……控制模型的复杂性不是一件简单的事情，需要找到大小合适、参数数量合适的模型。相反，……在实际的深度学习场景中，我们几乎总是会发现——最佳拟合模型(在最小化泛化误差的意义上)是已经适当正则化的大型模型。

—第 229 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

除了在训练中保持小重量的方法之外，几乎应该普遍使用提前停止。

> 早停应该几乎普遍使用。

—第 426 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

一些更具体的建议包括:

*   **经典**:使用提前停止和权重衰减(L2 权重正则化)。
*   **替代**:使用提前停止和带有重量限制的附加噪音。
*   **现代**:除了重量限制外，使用提前停止和脱扣。

这些建议将适合多层感知器和卷积神经网络。

对递归神经网络的一些建议包括:

*   **经典**:使用增加了权重噪声的提前停止和最大范数等权重约束。
*   **现代**:使用提前停止，通过时间感知的退圈和重量限制反向传播。

在正规化方面没有万灵药，强烈鼓励系统的实验。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 书

*   第七章深度学习的正则化，[深度学习](https://amzn.to/2NJW3gE)，2016。
*   第 5.5 节。神经网络中的正则化，[模式识别和机器学习](https://amzn.to/2Q2rEeP)，2006。
*   第 16 章，提高泛化能力的启发式方法，[神经锻造:前馈人工神经网络中的监督学习](https://amzn.to/2Dxo4XU)，1999。
*   第 9 章学习和推广，[用于模式识别的神经网络](https://amzn.to/2I9gNMP)，1995。

### 文章

*   [什么是过拟合，如何避免？神经网络常见问题](ftp://ftp.sas.com/pub/neural/FAQ3.html#A_over)。
*   [正则化(数学)，维基百科](https://en.wikipedia.org/wiki/Regularization_(mathematics))。

## 摘要

在这篇文章中，你发现了训练神经网络时过拟合的问题，以及如何用正则化方法解决这个问题。

具体来说，您了解到:

*   通过增加网络容量可以很容易地解决适配不足的问题，但是过度适配需要使用专门的技术。
*   像权重衰减这样的正则化方法为控制大型神经网络模型的过拟合提供了一种简单的方法。
*   一个现代的正则化建议是使用提前停止和权重约束。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。**