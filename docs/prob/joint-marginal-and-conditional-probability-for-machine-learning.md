# 对联合概率、边缘概率和条件概率的温和介绍

> 原文：<https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/>

最后更新于 2020 年 5 月 6 日

概率量化了随机变量结果的不确定性。

理解和计算单个变量的概率相对容易。然而，在机器学习中，我们经常有许多随机变量以复杂和未知的方式相互作用。

有一些特定的技术可以用来量化多个随机变量的概率，例如联合概率、边缘概率和条件概率。这些技术为概率理解预测模型与数据的拟合提供了基础。

在这篇文章中，你会发现一个温和的介绍联合，边际，和多个随机变量的条件概率。

看完这篇文章，你会知道:

*   联合概率是两个事件同时发生的概率。
*   边缘概率是一个事件的概率，与另一个变量的结果无关。
*   条件概率是一个事件在第二个事件出现时发生的概率。

**用我的新书[机器学习概率](https://machinelearningmastery.com/probability-for-machine-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

*   **2019 年 10 月更新**:修正小错别字，谢谢安娜。
*   **2019 年 11 月更新**:描述了联合概率的对称计算。

![A Gentle Introduction to Joint, Marginal, and Conditional Probability](img/9ebd2dcc502a4b8a4f32c0d26c586d61.png)

联合概率、边缘概率和条件概率简介[摄于](https://www.flickr.com/photos/alwbutler/6929964622/)摄于 [Masterbutler，版权所有。](https://www.flickr.com/photos/alwbutler/6929964622/)

## 概观

本教程分为三个部分；它们是:

1.  一个随机变量的概率
2.  多个随机变量的概率
3.  独立和排他的概率

## 一个随机变量的概率

概率量化了事件发生的可能性。

具体来说，它量化了一个随机变量出现特定结果的可能性，例如掷硬币、掷骰子或从一副牌中抽一张扑克牌。

> 概率是对某件事情发生的可能性的度量。

—第 57 页，[概率:对于热情的初学者](https://amzn.to/2jULJsu)，2016。

对于一个随机变量 *x* ， *P(x)* 是一个给 *x* 的所有值赋予一个概率的函数。

*   x = P(x)的概率密度

随机变量 x 的特定事件 *A* 的概率表示为 *P(x=A)* ，或简称为 *P(A)。*

*   事件 A 的概率= P(A)

概率的计算方法是期望结果的数量除以总的可能结果，在这种情况下，所有结果的可能性相等。

*   概率=(期望结果数)/(可能结果总数)

如果我们考虑一个离散的随机变量，比如骰子的滚动，这是很直观的。例如，模具轧制 a 5 的概率计算为轧制 a 5 (1)的一个结果除以离散结果(6)或 1/6 或约 0.1666 或约 16.666 的总数。

所有结果的概率总和必须等于 1。如果没有，我们就没有有效的概率。

*   所有结果的概率总和= 1.0。

不可能结果的概率为零。例如，不可能用标准的六面模轧制 7。

*   不可能结果的概率= 0.0

某个结果的概率是一。例如，当轧制六面模具时，肯定会出现 1 到 6 之间的值。

*   确定结果的概率= 1.0

事件不发生的概率，称为补数。

这可以通过 1 减去事件的概率，或者*1–P(A)*来计算。例如，不滚动 5 的概率为 1–P(5)或 1–0.166 或约 0.833 或约 83.333%。

*   非事件 A = 1 的概率–P(A)

现在我们已经熟悉了一个随机变量的概率，让我们考虑多个随机变量的概率。

## 多个随机变量的概率

在机器学习中，我们很可能使用许多随机变量。

例如，给定一个数据表，如在 excel 中，每行代表一个单独的观察或事件，每列代表一个单独的随机变量。

变量可以是离散的，这意味着它们取一组有限的值，也可以是连续的，这意味着它们取一个实值或数值。

因此，我们对两个或更多随机变量的概率感兴趣。

这很复杂，因为随机变量可以通过多种方式相互作用，进而影响它们的概率。

这可以通过将讨论简化为仅仅两个随机变量( *X，Y* )来简化，尽管原理可以推广到多个变量。

此外，为了讨论两个事件的概率，每个变量一个事件( *X=A，Y=B* ，尽管我们也可以很容易地讨论每个变量的事件组。

因此，我们引入多个随机变量的概率作为事件 *A* 和事件 *B* 的概率，简写为*X = A**Y = B*。

我们假设这两个变量在某种程度上是相关或相依的。

因此，我们可能要考虑三种主要的概率类型；它们是:

*   **联合概率**:事件概率 *A* 和 *B* 。
*   **边缘概率**:事件 X 的概率= *A* 给定变量 *Y* 。
*   **条件概率**:事件 *A* 给定事件 *B* 的概率。

这些类型的概率构成了许多问题预测建模的基础，例如分类和回归。例如:

*   一行数据的概率是每个输入变量的联合概率。
*   一个输入变量的特定值的概率是其他输入变量的值之间的边缘概率。
*   预测模型本身是给定输入示例的输出的条件概率的估计。

联合概率、边缘概率和条件概率是机器学习的基础。

让我们依次仔细看看每一个。

### 两个变量的联合概率

我们可能对两个同时发生的事件的概率感兴趣，例如两个不同随机变量的结果。

两个(或多个)事件的概率称为[联合概率](https://en.wikipedia.org/wiki/Joint_probability_distribution)。两个或多个随机变量的联合概率被称为联合概率分布。

例如，事件 *A* 和事件 *B* 的联合概率正式写成:

*   警队(甲及乙)

“*和*或连词用颠倒的大写字母“ *U* ”运算符“ *^* ”或有时用逗号“，”来表示。

*   P(A ^ B)
*   警(甲、乙)

事件 *A* 和 *B* 的联合概率计算为事件 *A* 给定事件 *B* 乘以事件 *B* 的概率。

这可以正式表述如下:

*   P(A 和 B) = P(给定的 B) * P(B)

联合概率的计算有时被称为概率的基本规则或概率的乘积规则或概率的 T2 链规则。

这里， *P(A 给定 B)* 是事件 A 给定事件 B 已经发生的概率，称为条件概率，如下所述。

联合概率是对称的，意味着 *P(A 和 B)* 与 *P(B 和 A)* 相同。使用条件概率的计算也是对称的，例如:

*   P(A 和 B) = P(A 给定 B) * P(B) = P(B 给定 A) * P(A)

### 边缘概率

我们可能对一个随机变量的事件概率感兴趣，而不考虑另一个随机变量的结果。

例如 *Y* 所有结果的 *X=A* 的概率。

一个事件在另一个随机变量的所有(或子集)结果存在的情况下的概率称为[边缘概率](https://en.wikipedia.org/wiki/Marginal_distribution)或边际分布。存在附加随机变量时，一个随机变量的边缘概率称为边缘概率分布。

之所以称之为边缘概率，是因为如果两个变量的所有结果和概率一起排列在一个表格中( *X* 为列， *Y* 为行)，那么一个变量的边缘概率( *X* )将是表格边缘上另一个变量(Y 行)的概率之和。

边缘概率没有特别的符号；它只是第二个变量的所有事件的所有概率与第一个变量的给定固定事件的所有概率的和或并。

*   所有 Y 的和 P(X=A，Y=yi)

这是概率论中另一个重要的基础规则，被称为“*求和规则*”

边缘概率不同于条件概率(下述)，因为它考虑第二个变量的所有事件的联合，而不是单个事件的概率。

### 条件概率

考虑到另一个事件的发生，我们可能对一个事件的概率感兴趣。

一个事件发生的概率被称为[条件概率](https://en.wikipedia.org/wiki/Conditional_probability)。一个到一个或多个随机变量的条件概率被称为条件概率分布。

例如，事件 *A* 给定事件 *B* 的条件概率正式写成:

*   给定的

给定的“*”用管道“|”运算符表示；例如:*

 **   警(甲|乙)

事件 *A* 给定事件 *B* 的条件概率计算如下:

*   P(给定的 B) = P(A 和 B) / P(B)

该计算假设事件 *B* 的概率不为零，例如并非不可能。

事件 *A* 给定事件 *B* 的概念并不意味着事件 *B* 已经发生(例如是确定的)；相反，它是给定试验中事件 *A* 发生在事件 *B* 之后或出现时的概率。

## 独立和排他的概率

当考虑多个随机变量时，它们可能不相互作用。

我们可能知道或假设两个变量不是相互依赖的，而是独立的。

或者，变量可以相互作用，但它们的事件可能不会同时发生，称为排他性。

在这一节中，我们将仔细研究在这些情况下多个随机变量的概率。

### 独立性ˌ自立性

如果一个变量不依赖于另一个变量，这被称为[独立性](https://en.wikipedia.org/wiki/Independence_(probability_theory))或统计独立性。

这对计算两个变量的概率有影响。

例如，我们可能对独立事件 *A* 和 *B* 的联合概率感兴趣，这与 *A* 的概率和*B*的概率相同

概率采用乘法进行组合，因此独立事件的联合概率计算为事件 *A* 乘以事件 *B* 的概率。

这可以正式表述如下:

*   **联合概率** : P(A 和 B) = P(A) * P(B)

正如我们可能凭直觉知道的那样，独立随机变量事件的边缘概率就是事件的概率。

我们熟悉的是单个随机变量的概率概念:

*   **边缘概率** : P(A)

我们把独立概率的边缘概率简称为概率。

同样的，当变量独立时 *A* 给定 *B* 的条件概率只是 *A* 的概率，因为 *B* 的概率没有影响。例如:

*   **条件概率** : P(A 给定 B) = P(A)

我们可能熟悉统计独立于采样的概念。这假设一个样本不受先前样本的影响，并且不影响未来样本。

许多机器学习算法假设来自一个域的样本彼此独立，并且来自相同的概率分布，称为[独立同分布](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)，简称 i.i.d。

### 排他性

如果一个事件的发生排除了其他事件的发生，则称这些事件为[互斥](https://en.wikipedia.org/wiki/Mutual_exclusivity)。

事件的概率被认为是不相交的，这意味着它们不能相互作用，是严格独立的。

如果事件 *A* 的概率与事件 *B* 互斥，那么事件 *A* 和事件 *B* 的联合概率为零。

*   P(A 和 B) = 0.0

相反，结果的概率可以描述为事件 *A* 或事件 *B* ，正式表述如下:

*   P(A 或 B) = P(A) + P(B)

“或”也称为并集，表示为大写的“ *U* ”字母；例如:

*   P(A 或 B) = P(A U B)

如果事件不是相互排斥的，我们可能会对任何一个事件的结果感兴趣。

非互斥事件的概率计算为事件 *A* 的概率和事件 *B* 的概率减去两个事件同时发生的概率。

这可以正式表述如下:

*   P(A 或 B)= P(A)+P(B)–P(A 和 B)

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 书

*   [概率:对于热情的初学者](https://amzn.to/2jULJsu)，2016 年。
*   [模式识别与机器学习](https://amzn.to/2JwHE7I)，2006。
*   [机器学习:概率视角](https://amzn.to/2xKSTCP)，2012。

### 文章

*   [概率，维基百科](https://en.wikipedia.org/wiki/Probability)。
*   [概率统计中的符号，维基百科](https://en.wikipedia.org/wiki/Notation_in_probability_and_statistics)。
*   [独立性(概率论)，维基百科](https://en.wikipedia.org/wiki/Independence_(probability_theory))。
*   [独立同分布随机变量，维基百科](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)。
*   [相互排他性，维基百科](https://en.wikipedia.org/wiki/Mutual_exclusivity)。
*   [边际分布，维基百科](https://en.wikipedia.org/wiki/Marginal_distribution)。
*   [联合概率分布，维基百科](https://en.wikipedia.org/wiki/Joint_probability_distribution)。
*   [条件概率，维基百科](https://en.wikipedia.org/wiki/Conditional_probability)。

## 摘要

在这篇文章中，你发现了多个随机变量的联合概率、边缘概率和条件概率的温和介绍。

具体来说，您了解到:

*   联合概率是两个事件同时发生的概率。
*   边缘概率是一个事件的概率，与另一个变量的结果无关。
*   条件概率是一个事件在第二个事件出现时发生的概率。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。*