# 机器学习中不确定性的温和介绍

> 原文：<https://machinelearningmastery.com/uncertainty-in-machine-learning/>

最后更新于 2019 年 9 月 25 日

应用机器学习需要管理不确定性。

机器学习项目中有许多不确定性的来源，包括特定数据值的差异、从域中收集的数据样本，以及从这些数据开发的任何模型的不完美性。

管理预测建模的机器学习中固有的不确定性可以通过来自概率的工具和技术来实现，概率是一个专门设计来处理不确定性的领域。

在这篇文章中，你将发现机器学习中不确定性的挑战。

看完这篇文章，你会知道:

*   不确定性是机器学习初学者，尤其是开发人员最大的困难来源。
*   数据中的噪声、领域的不完全覆盖和模型的不完善是机器学习中不确定性的三个主要来源。
*   概率为量化、处理和利用应用机器学习中的不确定性提供了基础和工具。

**用我的新书[机器学习概率](https://machinelearningmastery.com/probability-for-machine-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![A Gentle Introduction to Uncertainty in Machine Learning](img/d412a7c036662ed7cb07985c7cfba759.png)

机器学习中的不确定性简介
图片由 [Anastasiy Safari](https://www.flickr.com/photos/anastasiy/37131287005/) 提供，保留部分权利。

## 概观

本教程分为五个部分；它们是:

*   机器学习中的不确定性
*   观测中的噪声
*   领域的不完全覆盖
*   问题的不完美模型
*   如何管理不确定性

## 机器学习中的不确定性

应用机器学习需要适应不确定性。

[不确定性](https://en.wikipedia.org/wiki/Uncertainty)是指在信息不完善或不完全的情况下工作。

不确定性是机器学习领域的基础，然而它是给初学者造成最大困难的方面之一，尤其是那些来自开发人员背景的人。

对于软件工程师和开发人员来说，计算机是决定性的。你写一个程序，计算机按照你说的做。算法是基于空间或时间复杂性进行分析的，可以选择优化对项目最重要的算法，如执行速度或内存限制。

使用机器学习的预测建模包括拟合模型以将输入的例子映射到输出，例如在回归问题的情况下是一个数字，或者在分类问题的情况下是一个类标签。

自然，初学者会提出合理的问题，例如:

*   我应该使用的最佳功能是什么？
*   我的数据集的最佳算法是什么？

这些问题的答案是未知的，甚至可能是不可知的，至少是确切的。

> 计算机科学的许多分支主要处理完全确定和确定的实体。[……]鉴于许多计算机科学家和软件工程师在相对干净和特定的环境中工作，机器学习大量使用概率论可能会令人惊讶。

—第 54 页，[深度学习](https://amzn.to/2lnc3vL)，2016。

这是造成初学者困难的主要原因。

答案未知的原因是因为不确定性，解决方案是系统地评估不同的解决方案，直到为特定的预测问题发现一组好的或足够好的特征和/或算法。

机器学习中的不确定性主要有三个来源，在接下来的章节中，我们将依次来看三个可能的来源。

## 观测中的噪声

来自域的观察不清晰；相反，它们包含噪音。

来自域的观察通常被称为“*实例*或“*样本*，并且是一行数据。它是测量的或收集的。它是描述对象或主体的数据。它是模型的输入和预期的输出。

一个例子可能是在训练数据的情况下测量的一个[鸢尾花和花的种类](https://en.wikipedia.org/wiki/Iris_flower_data_set)的一组测量值。

```py
Sepal length: 	5.1 cm
Sepal width: 	3.5 cm
Petal length: 	1.4 cm
Petal width: 	0.2 cm
Species: 		Iris setosa
```

在需要预测的新数据的情况下，这只是没有花的种类的测量。

```py
Sepal length: 	5.1 cm
Sepal width: 	3.5 cm
Petal length: 	1.4 cm
Petal width: 	0.2 cm
Species: 		?
```

噪声是指观察中的可变性。

可变性可以是自然的，例如比正常的花更大或更小。这也可能是一个错误，例如测量时的失误或写下时的错别字。

这种可变性不仅影响输入或测量，还影响输出；例如，观察可能有不正确的类标签。

这意味着，尽管我们对该领域有观察，我们必须预期一些可变性或随机性。

真实的世界，反过来，真实的数据，是混乱的或不完美的。作为实践者，我们必须对数据保持怀疑，并开发系统来预期甚至利用这种不确定性。

这就是为什么花了这么多时间来审查数据统计和创建可视化，以帮助识别那些异常或不寻常的情况:所谓的数据清理。

## 领域的不完全覆盖

来自用于训练模型的领域的观察是样本，根据定义是不完整的。

在统计学中，随机样本是指从没有系统偏差的领域中选择的一组观察值。总会有一些偏见。

例如，我们可以选择测量一个花园中随机选择的花的大小。这些花是随机挑选的，但范围仅限于一个花园。范围可以扩大到一个城市、一个国家、一个大陆等等的花园。

样本中需要适当水平的方差和偏差，以便样本能够代表数据或模型将用于的任务或项目。

我们的目标是收集或获得一个合适的代表性随机观察样本，以训练和评估机器学习模型。通常，我们对采样过程几乎没有控制。相反，我们访问数据库或 CSV 文件，我们拥有的数据是我们必须处理的数据。

在所有情况下，我们永远不会有所有的观察结果。如果我们这样做了，就不需要预测模型了。

这意味着总会有一些未被观察到的案例。将会有我们没有覆盖的部分问题领域。无论我们如何鼓励我们的模型进行推广，我们都只能希望我们能够覆盖训练数据集中的案例和没有覆盖的显著案例。

这就是为什么我们将数据集分割成训练集和测试集，或者使用像[这样的重采样方法进行 k 倍交叉验证](https://machinelearningmastery.com/k-fold-cross-validation/)。我们这样做是为了处理数据集代表性的不确定性，并估计建模过程对该过程中未使用的数据的表现。

## 问题的不完美模型

机器学习模型总会有一些误差。

这常常被概括为“[所有车型都是错的](https://en.wikipedia.org/wiki/All_models_are_wrong)”，或者更完全的用[乔治·博克斯](https://en.wikipedia.org/wiki/George_E._P._Box)的一句格言来说:

> 所有模型都是错误的，但有些是有用的

这不仅仅适用于模型、工件，而是用于准备它的整个过程，包括数据的选择和准备、训练超参数的选择以及模型预测的解释。

模型误差可能意味着不完美的预测，例如预测回归问题中与预期完全不同的量，或者预测与预期不匹配的类标签。

考虑到我们刚刚讨论的数据的不确定性，这种类型的预测误差是可以预期的，包括观测中的噪声和区域的不完全覆盖。

另一种错误是遗漏错误。为了推广到新的情况，我们省略了细节或抽象它们。这是通过选择更简单但对数据细节更稳健的模型来实现的，而不是选择对训练数据高度专门化的复杂模型。因此，我们可能并且经常选择一个已知会在训练数据集中出错的模型，期望该模型能够更好地推广到新的情况，并具有更好的整体表现。

> 在许多情况下，使用简单但不确定的规则比使用复杂但确定的规则更实际，即使真正的规则是确定性的，并且我们的建模系统具有适应复杂规则的保真度。

—第 55 页，[深度学习](https://amzn.to/2lnc3vL)，2016。

然而，预测是必需的。

鉴于我们知道模型会出错，我们通过寻找一个足够好的模型来处理这种不确定性。这通常被解释为选择一个相对于简单方法或其他已建立的学习模型而言比较熟练的模型，例如良好的相对表现。

## 如何管理不确定性

应用机器学习中的不确定性是用概率来管理的。

概率是设计用来处理、操纵和驾驭不确定性的数学领域。

> 模式识别领域的一个关键概念是不确定性。它既通过测量中的噪声产生，也通过数据集的有限大小产生。概率理论为不确定性的量化和处理提供了一个一致的框架，并构成了模式识别的核心基础之一。

—第 12 页，[模式识别与机器学习](https://amzn.to/2JwHE7I)，2006。

事实上，概率论是更广泛的人工智能领域的核心。

> 代理可以使用概率和决策理论的方法来处理不确定性，但是首先他们必须从经验中学习他们对世界的概率理论。

—第 802 页，[人工智能:现代方法](https://amzn.to/2Y7yCpO)，第 3 版，2009 年。

来自概率的方法和工具提供了思考机器学习解决的预测建模问题的随机或[随机](https://en.wikipedia.org/wiki/Stochastic)性质的基础和方式；例如:

*   **在噪声观测**方面，概率和统计帮助我们理解和量化来自域的观测中变量的期望值、可变性。
*   **就域**的不完全覆盖而言，概率有助于理解和量化域内观测值的预期分布和密度。
*   **就模型误差**而言，概率有助于理解和量化我们的预测模型在应用于新数据时的预期能力和表现差异。

但这只是一个开始，因为概率为许多机器学习模型的迭代训练提供了基础，称为[最大似然估计](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)，落后于线性回归、逻辑回归、人工神经网络等模型。

概率还为开发特定算法(如朴素贝叶斯)以及机器学习的整个子研究领域(如贝叶斯信念网络等图形模型)提供了基础。

> 概率方法构成了大量数据挖掘和机器学习技术的基础。

—第 336 页，[数据挖掘:实用机器学习工具和技术](https://amzn.to/2lnW5S7)。2016 年第 4 版。

我们在应用机器学习中使用的过程是精心选择的，以解决我们已经讨论过的不确定性的来源，但是理解为什么选择这些过程需要对概率和概率论有基本的理解。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 书

*   [第三章:概率论，深度学习](https://amzn.to/2lnc3vL)，2016。
*   [第二章:概率，机器学习:概率视角](https://amzn.to/2xKSTCP)，2012。
*   [第二章:概率分布、模式识别和机器学习](https://amzn.to/2JwHE7I)，2006。

### 文章

*   [不确定性，维基百科](https://en.wikipedia.org/wiki/Uncertainty)。
*   [随机，维基百科](https://en.wikipedia.org/wiki/Stochastic)。
*   [所有型号都错了，维基百科](https://en.wikipedia.org/wiki/All_models_are_wrong)。

## 摘要

在这篇文章中，你发现了机器学习中不确定性的挑战。

具体来说，您了解到:

*   不确定性是机器学习初学者，尤其是开发人员最大的困难来源。
*   数据中的噪声、领域的不完全覆盖和模型的不完善是机器学习中不确定性的三个主要来源。
*   概率为量化、处理和利用应用机器学习中的不确定性提供了基础和工具。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。