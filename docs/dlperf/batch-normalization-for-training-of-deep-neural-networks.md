# 深度神经网络批量归一化简介

> 原文：<https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/>

最后更新于 2019 年 12 月 4 日

训练具有几十层的深度神经网络是具有挑战性的，因为它们可能对学习算法的初始随机权重和配置敏感。

造成这种困难的一个可能的原因是，当权重被更新时，输入到网络深层的分布在每个小批量之后可能会改变。这会导致学习算法永远追逐运动目标。网络中各层输入分布的这种变化被称为技术名称“*内部协变量移位*”

批处理标准化是一种训练深度神经网络的技术，它为每个小批处理标准化了一个层的输入。这具有稳定学习过程和显著减少训练深层网络所需的训练时期数量的效果。

在这篇文章中，您将发现用于加速深度学习神经网络训练的批处理规范化方法。

看完这篇文章，你会知道:

*   深度神经网络的训练具有挑战性，尤其是因为来自先前层的输入在权重更新后会发生变化。
*   批处理标准化是一种标准化网络输入的技术，应用于前一层的激活或直接输入。
*   批处理规范化加速了训练，在某些情况下通过将时代减半或更好，并提供了一些正则化，减少了泛化误差。

**用我的新书[更好的深度学习](https://machinelearningmastery.com/better-deep-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![How to Calibrate Probabilities for Imbalanced Classification](img/6ed5f60197c1c233f9c23c7400311deb.png)

深度神经网络批处理规范化的温和介绍
图片由[丹尼斯·贾维斯](https://flickr.com/photos/archer10/33621284946/)提供，保留部分权利。

## 概观

本教程分为五个部分；它们是:

1.  深度网络训练问题
2.  标准化层输入
3.  如何标准化层输入
4.  使用批处理规范化的示例
5.  使用批处理规范化的提示

## 深度网络训练问题

训练深度神经网络，例如具有几十个隐藏层的网络，是具有挑战性的。

这一挑战的一个方面是，使用假设当前层之前的层中的权重是固定的误差估计，从输出到输入逐层向后更新模型。

> 非常深的模型涉及几个函数或层的组合。在假设其他层不变的情况下，梯度告诉如何更新每个参数。实际上，我们同时更新所有的层。

—第 317 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

因为在更新过程中所有层都发生了变化，所以更新过程永远在追逐移动的目标。

例如，给定先前层输出具有给定分布的值的预期，更新层的权重。在前一层的权重更新后，这种分布可能会改变。

> 训练深度神经网络是复杂的，因为每一层的输入分布在训练过程中会随着前一层的参数变化而变化。这通过要求较低的学习率和仔细的参数初始化来减缓训练，并且使得训练具有饱和非线性的模型变得众所周知的困难。

——[批量归一化:通过减少内部协变量移位加速深度网络训练](https://arxiv.org/abs/1502.03167)，2015。

引入批量归一化的论文作者将训练期间输入分布的变化称为“*内部协变量移动*”

> 在训练过程中，我们将深层网络内部节点分布的变化称为内部协变量偏移。

——[批量归一化:通过减少内部协变量移位加速深度网络训练](https://arxiv.org/abs/1502.03167)，2015。

## 标准化层输入

批处理规范化，简称 batchnorm，是一种帮助协调模型中多个层的更新的技术。

> 批处理规范化提供了一种优雅的方式来重新参数化几乎任何深度网络。重新参数化大大减少了跨多个层协调更新的问题。

—第 318 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

它这样做是为了缩放层的输出，特别是通过标准化每个小批量的每个输入变量的激活，例如激活前一层的节点。回想一下，标准化指的是重新调整数据，使其平均值为零，标准偏差为 1，例如标准高斯。

> 批量标准化对模型进行重新参数化，以使某些单元始终按照定义进行标准化

—第 319 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

当应用于计算机视觉中的图像时，这个过程也被称为“T0”白化。

> 通过白化每一层的输入，我们将朝着实现输入的固定分布迈出一步，这将消除内部协变量移动的不良影响。

——[批量归一化:通过减少内部协变量移位加速深度网络训练](https://arxiv.org/abs/1502.03167)，2015。

对前一层的激活进行标准化意味着后续层在权重更新期间对输入的传播和分布所做的假设不会改变，至少不会显著改变。这具有稳定和加速深度神经网络的训练过程的效果。

> 批量标准化仅用于标准化每个单元的均值和方差，以稳定学习，但允许单元之间的关系和单个单元的非线性统计发生变化。

—第 320 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

规范化层的输入对模型的训练有影响，大大减少了所需的时期数量。它还可以具有正则化效果，减少泛化误差，就像使用激活正则化一样。

> 批量归一化会对优化表现产生显著影响，尤其是对于卷积网络和具有乙状线非线性的网络。

—第 425 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

虽然减少“*内部协变量移动*”是开发该方法的动机，但有一些建议认为，相反，批处理规范化是有效的，因为它平滑并反过来简化了在训练网络时正在解决的优化功能。

> ……batchorm 从根本上影响了网络训练:它使相应优化问题的场景明显更加平滑。这尤其确保梯度更具预测性，从而允许使用更大范围的学习率和更快的网络收敛。

— [批处理规范化如何帮助优化？(不，不是关于内部协变量移位)](https://arxiv.org/abs/1805.11604)，2018。

## 如何标准化层输入

在训练过程中，可以通过计算每一个小批量层的每个输入变量的平均值和标准偏差，并使用这些统计数据来执行标准化，从而实现批量标准化。

或者，平均值和标准偏差的运行平均值可以跨小批量保持，但可能导致不稳定的训练。

> 很自然地会问，我们是否可以简单地使用移动平均线[…]在训练期间执行标准化[…]。然而，这已经被观察到导致模型爆炸。

——[批量重正化:减少批量标准化模型中的小批量依赖](https://arxiv.org/abs/1702.03275)，2017。

训练后，层输入的平均值和标准偏差可以设置为在训练数据集上观察到的平均值。

对于小的[小批量](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)或不包含来自训练数据集的示例的代表性分布的小批量，训练和推理(在训练后使用模型)之间的标准化输入的差异会导致表现的显著差异。这可以通过修改称为批量重正化(简称 BatchRenorm)的方法来解决，该方法使得变量平均值和标准偏差的估计在小批量中更加稳定。

> 批处理重正化通过每维校正来扩展批处理，以确保激活在训练和推理网络之间匹配。

——[批量重正化:减少批量标准化模型中的小批量依赖](https://arxiv.org/abs/1702.03275)，2017。

输入的这种标准化可以应用于第一隐藏层的输入变量，或者更深层的隐藏层的激活。

在实践中，通常允许层学习两个新参数，即新的平均值和标准偏差，分别为贝塔和伽玛，这允许标准化层输入的自动缩放和移位。这些参数由模型学习，作为训练过程的一部分。

> 请注意，简单地标准化层的每个输入可能会改变层可以表示的内容。[……]这些参数与原始模型参数一起学习，并恢复网络的表示能力。

——[批量归一化:通过减少内部协变量移位加速深度网络训练](https://arxiv.org/abs/1502.03167)，2015。

重要的是，反向传播算法被更新以对变换的输入进行操作，并且误差也被用于更新由模型学习的新的标度和移位参数。

标准化应用于层的输入，即输入变量或来自前一层的激活函数的输出。给定激活函数的选择，层的输入分布可能是非高斯的。在这种情况下，在前一层中的激活函数之前标准化总和激活可能是有益的。

> 我们在非线性之前添加 BN 变换……………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………………

——[批量归一化:通过减少内部协变量移位加速深度网络训练](https://arxiv.org/abs/1502.03167)，2015。

## 使用批处理规范化的示例

本节提供了一些利用批处理规范化的里程碑论文和流行模型的例子。

在 2015 年的论文中，该论文引入了名为“[批量归一化:通过减少内部协变量移位](https://arxiv.org/abs/1502.03167)来加速深度网络训练”的技术，来自谷歌的作者谢尔盖·约夫(Sergey Ioffe)和克里斯蒂安·塞格迪(Christian Szegedy)展示了基于初始阶段的卷积神经网络在照片分类方面相对于基线方法的显著加速。

> 通过仅使用批处理规范化[…]，我们在不到一半的训练步骤中匹配了 Inception 的准确性。

何等人在 2015 年发表的论文《用于图像识别的深度残差学习》中，在称为 ResNet 的深度模型中的卷积层之后使用了批处理归一化，并在标准照片分类任务 ImageNet 数据集上获得了最先进的结果。

> 我们在每次卷积之后和激活之前都采用了批量归一化

来自谷歌的 Christian Szegedy 等人在 2016 年发表的论文《重新思考计算机视觉的初始架构》》中，在他们更新的称为 GoogleNet Inception-v3 的初始模型中使用了批处理规范化，在 ImageNet 数据集上实现了当时最先进的结果。

> BN-辅助是指辅助分类器的全连接层也是批量归一化的版本，而不仅仅是卷积。

来自百度的 Dario Amodei 在他们 2016 年的论文《深度语音 2:英语和普通话的端到端语音识别》中使用了批量归一化递归神经网络的变体作为语音识别的端到端深度模型。

> ……我们发现，当应用于大数据集上的非常深的 rnn 网络时，我们使用的 BatchNorm 变体除了加速训练之外，还大大改善了最终的泛化误差

## 使用批处理规范化的提示

本节提供了在自己的神经网络中使用批处理规范化的提示和建议。

### 使用不同的网络类型

批处理标准化是一种通用技术，可用于标准化层的输入。

它可以用于大多数网络类型，如多层感知器、卷积神经网络和递归神经网络。

### 可能在激活前使用

可以在前一层的激活函数之前或之后，对该层的输入使用批量标准化。

如果对于像双曲正切和逻辑函数这样的 s 形函数，在激活函数之后**可能更合适。**

对于可能导致非高斯分布的激活，如大多数网络类型的现代默认校正线性激活函数，在激活函数之前设置**可能是合适的。**

> 批处理标准化的目标是在整个训练中实现激活值的稳定分布，在我们的实验中，我们在非线性之前应用它，因为在非线性处，匹配第一和第二矩更有可能导致稳定分布

——[批量归一化:通过减少内部协变量移位加速深度网络训练](https://arxiv.org/abs/1502.03167)，2015。

也许用你的网络测试这两种方法。

### 使用大学习率

使用批处理规范化使网络在训练过程中更加稳定。

这可能需要使用比正常学习率大得多的速率，这又可能进一步加快学习过程。

> 在批处理标准化模型中，我们已经能够从更高的学习率中实现训练加速，而没有不良副作用

——[批量归一化:通过减少内部协变量移位加速深度网络训练](https://arxiv.org/abs/1502.03167)，2015。

更快的训练也意味着用于学习率的衰减速率可能会增加。

### 对权重初始化不太敏感

深度神经网络对用于在训练前初始化权重的技术非常敏感。

批量归一化给训练带来的稳定性使得训练深度网络对权重初始化方法的选择不太敏感。

### 数据准备的替代方案

批量标准化可用于标准化具有不同标度的原始输入变量。

如果为每个输入特征计算的平均值和标准偏差是在小批量上计算的，而不是在整个训练数据集上计算的，那么批量大小必须充分代表每个变量的范围。

它可能不适合数据分布高度非高斯的变量，在这种情况下，最好将数据缩放作为预处理步骤。

### 不要用于丢弃

批量归一化提供了一些正则化效果，减少了泛化误差，也许不再需要使用[去核来正则化](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)。

> 从修改后的 BN-initiation 中删除 from 加快了训练速度，而不会增加过拟合。

——[批量归一化:通过减少内部协变量移位加速深度网络训练](https://arxiv.org/abs/1502.03167)，2015。

此外，在同一网络中使用批处理规范化和丢弃可能不是一个好主意。

原因是，考虑到在丢弃过程中节点的随机丢弃，用于标准化前一层激活的统计数据可能变得有噪声。

> 批处理标准化有时还会减少泛化误差，并允许省略缺失，因为用于标准化每个变量的统计估计中存在噪声。

—第 425 页，[深度学习](https://amzn.to/2NJW3gE)，2016。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

### 书

*   第 8.7.1 节–批量标准化，[深度学习](https://amzn.to/2NJW3gE)，2016。
*   第 7.3.1 节。高级架构模式，[Python 深度学习，](https://amzn.to/2Ck4ImT) 2017。

### 报纸

*   [批量归一化:通过减少内部协变量移位加速深度网络训练](https://arxiv.org/abs/1502.03167)，2015。
*   [批量重正化:减少批量标准化模型中的小批量依赖](https://arxiv.org/abs/1702.03275)，2017。
*   [批处理规范化如何帮助优化？(不，不是关于内部协变量移位)](https://arxiv.org/abs/1805.11604)，2018。

### 文章

*   [批量归一化，维基百科](https://en.wikipedia.org/wiki/Batch_normalization)。
*   [批量定额为什么起作用？，deeplearning.ai](https://www.youtube.com/watch?v=nUUqwaxLnWs) ，视频
*   [批量归一化](https://www.youtube.com/watch?v=Xogn6veSyxA)，OpenAI，2016。
*   【ReLU 之前还是之后批量归一化？，Reddit 。

## 摘要

在这篇文章中，您发现了用于加速深度学习神经网络训练的批处理规范化方法。

具体来说，您了解到:

*   深度神经网络的训练具有挑战性，尤其是因为来自先前层的输入在权重更新后会发生变化。
*   批处理标准化是一种标准化网络输入的技术，应用于前一层的激活或直接输入。
*   批处理规范化加速了训练，在某些情况下通过将时代减半或更好，并提供了一些正则化，减少了泛化误差。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。