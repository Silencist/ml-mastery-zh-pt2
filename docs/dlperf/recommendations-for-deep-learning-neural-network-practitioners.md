# 深度学习神经网络从业者推荐

> 原文：<https://machinelearningmastery.com/recommendations-for-deep-learning-neural-network-practitioners/>

最后更新于 2019 年 8 月 6 日

鉴于开源库的广泛采用，深度学习神经网络的定义和训练相对简单。

然而，神经网络的配置和训练仍然具有挑战性。

在他 2012 年发表的题为《*基于梯度的深度架构训练实用建议》*》的论文中，作为预印本和 2012 年流行书籍《*神经网络:交易技巧》*的一章，“深度学习领域之父”之一的 Yoshua Bengio 提供了配置和调整神经网络模型的实用建议。

在这篇文章中，你将浏览这篇长而有趣的论文，并为现代深度学习实践者挑选出最相关的技巧和诀窍。

看完这篇文章，你会知道:

*   深度学习复兴的早期基础包括预处理和自动编码器。
*   神经网络超参数范围的初始配置建议。
*   如何有效地调整神经网络超参数以及更有效地调整模型的策略。

**用我的新书[更好的深度学习](https://machinelearningmastery.com/better-deep-learning/)启动你的项目**，包括*分步教程*和所有示例的 *Python 源代码*文件。

我们开始吧。

![Practical Recommendations for Deep Learning Neural Network Practitioners](img/8440b4686336a1577b0e7c902687e167.png)

为深度学习神经网络从业者提供的实用建议
图片由 [Susanne Nilsson](https://www.flickr.com/photos/infomastern/24261083134/) 提供，保留部分权利。

## 概观

本教程分为五个部分；它们是:

1.  从业者必读
2.  论文概述
3.  深度学习的开始
4.  通过梯度下降学习
5.  超参数建议

## 对从业者的建议

2012 年，流行实用书籍《神经网络:交易技巧》第二版出版。

[第一版](https://amzn.to/2qOitnd)于 1999 年出版，包含 17 章(每一章都由不同的学者和专家撰写)，内容是如何最大限度地利用神经网络模型。更新后的第二版增加了 13 章，其中包括由 Yoshua Bengio 撰写的一个重要章节(第 19 章)，标题为“*基于梯度的深度架构培训实用建议*”

第二版出版的时间是对神经网络重新产生兴趣的重要时间，也是已经成为“深度学习”的开始 Yoshua Bengio 的章节很重要，因为它为开发神经网络模型提供了建议，包括当时非常现代的深度学习方法的细节。

虽然这一章可以作为第二版的一部分阅读，但本吉奥还在 arXiv 网站上发布了这一章的预印本，可在此访问:

*   [深度架构基于梯度的训练实用建议](https://arxiv.org/abs/1206.5533)，预印本，2012。

这一章也很重要，因为它为四年后成为事实上的深度学习教科书提供了一个宝贵的基础，书名很简单，叫做《深度学习》，本吉奥是该书的合著者。

这一章(我将从现在开始称之为论文)是所有神经网络从业者的必读。

在这篇文章中，我们将逐步浏览论文的每一部分，并指出一些最突出的建议。

## 论文概述

论文的目标是为从业者提供开发神经网络模型的实用建议。

有许多类型的神经网络模型和许多类型的从业者，因此目标是广泛的，并且建议不特定于给定类型的神经网络或预测建模问题。这很好，因为我们可以在我们的项目中自由地应用这些建议，但也令人沮丧，因为没有给出文献或案例研究中的具体例子。

这些建议的重点是模型超参数的配置，特别是与随机梯度下降学习算法相关的超参数。

> 本章旨在作为一个实用指南，为一些最常用的超参数提供建议，特别是在基于反向传播梯度和基于梯度的优化的学习算法的背景下。

这些建议是在深度学习领域出现的背景下提出的，在这个领域，现代方法和快速的图形处理器硬件促进了网络的发展，其深度和能力都超过了以前。Bengio 将这种复兴追溯到 2006 年(在撰写本文之前的六年)和开发[贪婪逐层预训练方法](https://machinelearningmastery.com/greedy-layer-wise-pretraining-tutorial/)，后来(在撰写本文之后)被大量使用 ReLU、Dropout、BatchNorm 和其他有助于开发深度模型的方法所取代。

2006 年的深度学习突破集中在使用无监督学习，通过在特征层次的每一层提供局部训练信号来帮助学习内部表示。

本文分为六个主要部分，第三部分提供了关于配置超参数的建议的主要阅读重点。论文的完整目录如下。

*   摘要
*   1 导言
    *   1.1 深度学习和贪婪逐层预处理
    *   1.2 去噪和压缩自动编码器
    *   1.3 在线学习和泛化误差优化
*   2 个梯度
    *   2.1 梯度下降和学习率
    *   2.2 梯度计算和自动微分
*   3 个超级参数
    *   3.1 神经网络超参数
        *   3.1.1 近似优化的超参数
    *   3.2 模型的超参数和训练准则
    *   3.3 手动搜索和网格搜索
        *   3.3.1 探索超参数的一般指南
        *   3.3.2 坐标下降和多分辨率搜索
        *   3.3.3 自动和半自动网格搜索
        *   3.3.4 超参数的逐层优化
    *   3.4 超参数的随机采样
*   4 调试和分析
    *   4.1 坡度检查和受控过拟合
    *   4.2 可视化和统计
*   5 其他建议
    *   5.1 多核机器、BLAS 和图形处理器
    *   5.2 稀疏高维输入
    *   5.3 符号变量、嵌入、多任务学习和多关系学习
*   6 个开放式问题
    *   6.1 培训更深层架构的额外难度
    *   6.2 自适应学习率和二阶方法
    *   6.3 结论

我们将不涉及每一部分，而是将重点放在论文的开头，特别是关于超参数和模型调整的建议。

## 深度学习的开始

引言部分花了一些时间来介绍深度学习的开始，如果把它看作是该领域的历史快照，那将会非常有趣。

当时，深度学习复兴是由神经网络模型的发展推动的，该模型的层数比以前基于贪婪逐层预处理和通过自动编码器进行表示学习等技术使用的层数多得多。

> 训练深度神经网络最常用的方法之一是基于贪婪逐层预训练。

这种方法不仅很重要，因为它允许开发更深层次的模型，而且无监督的形式允许使用无标记的例子，例如半监督学习，这也是一个突破。

> 特征学习和深度学习的另一个重要动机是它们可以用未标记的例子来完成…

因此，重用(字面上的重用)是一个主要的主题。

> 重用的概念解释了分布式表示的力量，也是深度学习背后理论优势的核心。

尽管理论上可以证明一个具有足够容量的单层或两层神经网络可以逼近任何函数，但他温和地提醒人们，深层网络为逼近更复杂的函数提供了一条计算捷径。这是一个重要的提醒，有助于推动深度模型的发展。

> 理论结果清楚地确定了函数族，在这些函数族中，深度表示可能比深度不够的函数更有效率。

时间花在了两个主要的深度学习突破上:贪婪逐层预训练(有监督和无监督)和自动编码器(去噪和对比)。

第三个突破，成果管理制被留在了本书的另一章中讨论，该章由该方法的开发者辛顿撰写。

*   受限玻尔兹曼机(RBM)。
*   贪婪逐层预处理(无监督和有监督)。
*   自动编码器(去噪和对比)。

尽管具有里程碑意义，但在深度学习的发展过程中，这些技术没有一种是首选的，也没有一种被广泛使用，或许除了自动编码器之外，没有一种技术像以前一样得到了大力研究。

## 通过梯度下降学习

第二节提供了梯度和梯度学习算法的基础，梯度和梯度学习算法是用于将神经网络权重拟合到训练数据集的主要优化技术。

这包括批量梯度下降和随机梯度下降之间的重要区别，以及通过小批量梯度下降的近似，今天都简称为随机梯度下降。

*   **批量梯度下降**。使用训练数据集中的所有示例来估计梯度。
*   **小批量梯度下降**。使用训练数据集中的样本子集来估计梯度。
*   **随机(在线)梯度下降**。使用训练数据集中的每个单一模式来估计梯度。

小批量变量被提供作为一种方法来实现由随机梯度下降提供的收敛速度和由批量梯度下降提供的误差梯度的改进估计。

批量越大，收敛速度越慢。

> 另一方面，随着 B[批处理大小]的增加，每次计算完成的更新数量减少，这减慢了收敛速度(就误差与执行的乘加操作数量而言)，因为在相同的计算时间内可以完成的更新更少。

由于在梯度估计中引入了统计噪声，较小的批量提供了规则化效果。

> ……较小的 B 值[批次大小]可能受益于参数空间中的更多探索和一种正则化形式，这两者都是由于梯度估计器中注入的“噪声”，这可以解释有时用较小的 B 观察到的更好的测试结果

这一次也是自动微分在神经网络模型开发中的引入和广泛采用。

> 梯度可以手动计算，也可以通过自动微分计算。

本吉奥对此特别感兴趣，因为他参与了开发现在已不存在的“安托”Python 数学库“T1”和“T2”派尔恩 2 深度学习库“T3”，或许分别由“T4”TensorFlow“T5”和“T6”Keras“T7”取得了成功。

手动实现神经网络的微分很容易出错，并且错误很难调试并导致次优表现。

> 当用手动微分实现梯度下降算法时，结果往往是冗长、脆弱的代码，缺乏模块化——所有这些对软件工程来说都是坏事。

[自动微分](https://en.wikipedia.org/wiki/Automatic_differentiation)被描绘成一种更稳健的方法，将神经网络开发为数学运算的图形，每个运算都知道如何微分，这可以象征性地定义。

一种更好的方法是用对象来表示流程图，这些对象模块化了如何从输入计算输出以及如何计算梯度下降所需的偏导数。

基于图的方法定义模型的灵活性和计算误差导数时误差可能性的降低意味着这种方法已经成为现代开源神经网络库的标准，至少在底层数学库中是如此。

## 超参数建议

本文的重点是在随机梯度下降下控制模型收敛和推广的超参数的配置。

### 使用验证数据集

本节从使用来自训练集和测试集的单独验证数据集来调整模型超参数的重要性开始。

> 对于任何影响学习器有效能力的超参数，基于样本外数据(在训练集之外)选择其值更有意义，例如验证集表现、在线错误或交叉验证错误。

以及在模型表现评估中不包括验证数据集的重要性。

> 一旦一些样本外数据被用于选择超参数值，它就不能再用于获得泛化表现的无偏估计量，因此人们通常使用测试集(或者在小数据集的情况下使用双重交叉验证)来估计纯学习算法的泛化误差(超参数选择隐藏在内部)。

交叉验证通常不用于神经网络模型，因为它们可能需要几天、几周甚至几个月的时间来训练。然而，在可以使用交叉验证的较小数据集上，建议使用双重交叉验证技术，其中在每个交叉验证文件夹内执行超参数调整。

> 双重交叉验证递归地应用交叉验证的思想，使用外部循环交叉验证来评估泛化误差，然后在每个外部循环拆分的训练子集中应用内部循环交叉验证(即，再次将其拆分为训练和验证折叠)，以便为该拆分选择超参数。

### 学习超参数

然后介绍一套学习超参数，并附带一些建议。

套件中的超参数有:

*   **初始学习率**。权重更新的比例；0.01 是一个好的开始。
*   **学习状态计划表**。随着时间的推移，学习率下降；1/T 是一个好的开始。
*   **小批量**。用于估计梯度的样本数量；32 岁是一个好的开始。
*   **训练迭代**。权重的更新次数；设置大并使用提前停止。
*   **动量**。使用以前重量更新的历史记录；设置为大(例如 0.9)。
*   **层特定超参数**。可能，但很少做到。

学习率是最重要的调节参数。尽管 0.01 的值是推荐的起点，但对于特定的数据集和模型，需要拨入该值。

> 这通常是最重要的一个超参数，人们应该始终确保它已经被调优[……]默认值 0.01 通常适用于标准的多层神经网络，但是仅仅依赖这个默认值是愚蠢的。

他甚至说，如果只有一个参数可以调整，那就是学习率。

> 如果只有时间来优化一个超参数，并且使用随机梯度下降，那么这就是值得调整的超参数。

批处理大小是作为对学习速度的控制来呈现的，而不是关于调整测试集表现(泛化误差)。

> 理论上，这个超参数应该影响训练时间，而不是太多的测试表现，因此可以在选择了其他超参数(除了学习率)之后，通过比较训练曲线(训练和验证误差对训练时间量)，与其他超参数分开优化。

## 模型超参数

然后引入模型超参数，再加上一些建议。

它们是:

*   **节点数**。控制模型的容量；使用更大的正则化模型。
*   **权重正则化**。处罚权重大的车型；一般来说，试试 L2 或 L1 的稀疏性。
*   **活动正规化**。对大型激活的模型进行处罚；试试 L1 的稀疏表示。
*   **激活功能**。用作隐藏层中节点的输出；使用乙状结肠函数(逻辑和唐)或整流器(现在的标准)。
*   **重量初始化**。优化过程的起点；受前一层的激活功能和尺寸的影响。
*   **随机种子**。优化过程的随机性质；多次运行的平均模型。
*   **预处理**。建模前准备数据；至少标准化和消除相关性。

配置一个层中的节点数量很有挑战性，可能也是初学者问得最多的问题之一。他建议，在每个隐藏层中使用相同数量的节点可能是一个很好的起点。

> 在一项大型比较研究中，我们发现对所有层使用相同的大小通常比使用减小的大小(金字塔状)或增加的大小(倒置的金字塔)效果更好或相同，但当然这可能取决于数据。

他还建议对第一个隐藏层使用过完备配置。

> 对于我们所处理的大多数任务，我们发现过完备(大于输入向量)的第一隐藏层比欠完备层效果更好。

给定对分层训练和自动编码器的关注，表示的稀疏性(隐藏层的输出)是当时的焦点。因此建议使用[活动正则化](https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras/)，这在较大的编码器-解码器模型中可能仍然有用。

> 稀疏表示可能是有利的，因为它们促进了解开表示的潜在因素的表示。

当时，线性整流器激活功能刚刚开始使用，还没有被广泛采用。今天，使用整流器(ReLU)是标准，因为使用它的模型很容易胜过使用逻辑或双曲正切非线性的模型。

### 调整超参数

默认配置在大多数问题上对大多数神经网络都很有效。

然而，需要对超参数进行调整，以便在给定数据集上充分利用给定模型。

调整超参数可能具有挑战性，这既是因为需要计算资源，也是因为可能很容易过度填充验证数据集，从而导致误导性的结果。

> 人们不得不认为超参数选择是一种困难的学习形式:既有优化问题(寻找产生低验证误差的超参数配置)又有泛化问题:优化验证表现后，预期的泛化存在不确定性，在比较许多超参数配置时，有可能过度估计验证误差并获得表现的乐观偏差估计。

为模型调整一个超参数并绘制结果通常会产生一个 U 形曲线，显示表现差、表现好以及表现差的模式(例如，将损失或误差降至最低)。目标是找到“ *U* 的底部。”

问题是，许多超参数相互作用，并且“*U”*的底部可能会有噪声。

> 尽管对于第一近似，我们期望一种 U 形曲线(当仅考虑单个超参数时，其他参数是固定的)，但该曲线也可能有噪声变化，部分原因是使用了有限的数据集。

为了帮助这一搜索，他提供了三个有价值的提示，在调整模型超参数时通常要考虑这些提示:

*   **边界上的最佳值**。如果在搜索的区间边缘找到一个好的值，可以考虑扩大搜索范围。
*   **考虑的数值范围**。考虑在对数标度上搜索，至少在开始时(例如 0.1、0.01、0.001 等)。).
*   **计算考虑因素**。考虑放弃结果的保真度以加速搜索。

提出了三种系统的超参数搜索策略:

*   **坐标下降**。每次拨入一个超参数。
*   **多分辨率搜索**。反复放大搜索间隔。
*   **网格搜索**。定义一个 n 维的值网格，并依次测试每个值。

这些策略可以单独使用，甚至可以组合使用。

网格搜索可能是最常被理解和广泛使用的模型超参数调整方法。它是详尽的，但可并行化的，这是一个可以利用廉价云计算基础设施的优势。

> 与许多其他优化策略(如坐标下降)相比，网格搜索的优势在于完全可并行化。

通常，通过迭代网格搜索，结合多分辨率和网格搜索，重复该过程。

> 通常，单个网格搜索是不够的，从业者倾向于继续进行一系列网格搜索，每次都根据先前获得的结果调整所考虑的值的范围。

他还建议保持一个人在循环中，以密切关注 bug，并使用模式识别来识别趋势和改变搜索空间的形状。

> 人类可以非常擅长执行超参数搜索，有人类在循环中也有优势，它可以帮助检测学习算法的 bug 或不想要的或意想不到的行为。

然而，重要的是尽可能地自动化，以确保该过程对于未来的新问题和模型是可重复的。

网格搜索是详尽而缓慢的。

> 寻找好的超参数配置的网格搜索方法的一个严重问题是，它与所考虑的超参数的数量成指数关系。

他建议使用随机采样策略，这已被证明是有效的。每个超参数的区间可以统一搜索。这种分布可能会因包含先验而有所偏差，例如选择合理的违约。

> 随机采样的思想是用随机(通常是均匀的)采样代替规则网格。通过从先前分布(通常在感兴趣的区间内的对数域中是均匀的)中独立地采样每个超参数来选择每个测试的超参数配置。

文章最后给出了一些更一般性的建议，包括调试学习过程的技术、加速 GPU 硬件的训练以及剩余的未决问题。

## 进一步阅读

如果您想更深入地了解这个主题，本节将提供更多资源。

*   [神经网络:交易技巧:交易技巧](https://amzn.to/2qOitnd)，第一版，1999 年。
*   [神经网络:交易的诀窍:交易的诀窍](https://amzn.to/2DzmS5F)，第二版，2012 年。
*   [深度架构基于梯度的训练实用建议](https://arxiv.org/abs/1206.5533)，预印本，2012。
*   [深度学习](https://amzn.to/2qWg61L)，2016 年。
*   [自动分化，维基百科](https://en.wikipedia.org/wiki/Automatic_differentiation)。

## 摘要

在这篇文章中，您发现了 Yoshua Bengio 在 2012 年发表的题为“基于梯度的深度体系结构训练的实用建议”*的论文中的突出建议、技巧和诀窍*

 *你读过这篇论文吗？你有什么想法？
在下面的评论里告诉我。

你有什么问题吗？
在下面的评论中提问，我会尽力回答。*